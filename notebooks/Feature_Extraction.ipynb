{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c98c967-2beb-4c39-81bc-1e41040f3f85",
   "metadata": {},
   "source": [
    "# Cough Detection Feature Extraction\n",
    "\n",
    "This notebook serves as a foundation on what and why each features are extracted from multimodal biosignals (audio + IMU sensors) for machine learning-based cough detection.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "- **Window size**: 0.4 seconds (6400 audio samples @ 16kHz, 40 IMU samples @ 100Hz)\n",
    "- **Data augmentation**: Random temporal shifts (aug_factor=2) to increase dataset diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, signal\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if os.path.exists(\"/kaggle/usr/lib/\"):\n",
    "    # Load from Kaggle as utility scripts\n",
    "    from edge_ai_cough_count_helpers import * # pyright: ignore[reportMissingImports]\n",
    "    from edge_ai_cough_count_dataset_gen import * # pyright: ignore[reportMissingImports]\n",
    "    from edge_ai_cough_count_features import extract_audio_features, extract_imu_features # pyright: ignore[reportMissingImports]\n",
    "else:\n",
    "    # Add src directory to path\n",
    "    sys.path.append(os.path.abspath('../src'))\n",
    "    from helpers import *\n",
    "    from dataset_gen import *\n",
    "    from features import extract_audio_features, extract_imu_features\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ebe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Constants from paper\n",
    "WINDOW_LEN = 0.4  # 0.4 second windows\n",
    "AUG_FACTOR = 2    # Data augmentation factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413s0jutr5o",
   "metadata": {},
   "source": [
    "## Why Feature Extraction?\n",
    "\n",
    "You might wonder: **Why not just feed the raw audio and IMU signals directly into a machine learning model?**\n",
    "\n",
    "Here's why feature extraction is essential:\n",
    "\n",
    "1. **Size reduction**: A 0.4-second window contains 6,400 audio samples. Instead of processing all 6,400 values, we extract just 65 meaningful features that capture the \"essence\" of the sound.\n",
    "\n",
    "2. **Noise reduction**: Raw signals contain a lot of irrelevant information and noise. Features focus on what matters - patterns that distinguish coughs from other sounds.\n",
    "\n",
    "3. **Better learning**: Machine learning models learn faster and more accurately from compact, meaningful features than from thousands of raw signal values.\n",
    "\n",
    "**Analogy**: Imagine describing a person. Instead of showing a full high-resolution photo (raw signal), you describe key features: height, eye color, age, build (extracted features). Both convey information, but features are more compact and focused on what matters.\n",
    "\n",
    "In this notebook, we'll extract **105 features** that capture the unique characteristics of cough sounds and motions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gbh3xz74vaj",
   "metadata": {},
   "source": [
    "## Feature Categories Overview\n",
    "\n",
    "Our 105 features are organized into two main groups:\n",
    "\n",
    "| Category | Count | What It Captures | Example Use Case |\n",
    "|----------|-------|------------------|------------------|\n",
    "| **Audio Features** | **65** | Sound characteristics from microphone | |\n",
    "| ‚Üí MFCC | 52 | Sound \"texture\" or \"timbre\" | Distinguishing cough from speech |\n",
    "| ‚Üí Spectral | 10 | Frequency distribution | High-pitched vs low-pitched sounds |\n",
    "| ‚Üí Time-domain | 3 | Loudness and sharpness | Sudden bursts vs gradual sounds |\n",
    "| **IMU Features** | **40** | Body motion from sensors | |\n",
    "| ‚Üí Accelerometer | 20 | Linear motion (chest/body movement) | Detecting physical cough motion |\n",
    "| ‚Üí Gyroscope | 20 | Rotational motion (head/neck) | Detecting head movement during cough |\n",
    "| **Total** | **105** | Complete multimodal signature | Robust cough detection |\n",
    "\n",
    "### Why Use Both Audio AND Motion Sensors?\n",
    "\n",
    "**Multimodal sensing is more robust than either modality alone:**\n",
    "\n",
    "- **When audio struggles**: Noisy environments (music, traffic, other people talking)\n",
    "  - IMU still captures the physical motion of coughing\n",
    "  \n",
    "- **When IMU struggles**: Sitting still, gentle cough, other body movements\n",
    "  - Audio captures the distinctive cough sound\n",
    "  \n",
    "- **Together**: The combination is much more accurate than either sensor alone\n",
    "\n",
    "In the walkthrough below, you'll see exactly how each feature category works and why it helps detect coughs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2415e9",
   "metadata": {},
   "source": [
    "## Understanding Sampling and Sample Rate\n",
    "\n",
    "Before we can extract features from audio and IMU signals, we need to understand how these **continuous real-world signals** become **discrete digital data** that computers can process.\n",
    "\n",
    "### What is Sampling?\n",
    "\n",
    "**Sampling** is the process of converting a continuous analog signal (like sound waves or motion) into a discrete digital signal by taking measurements at regular intervals.\n",
    "\n",
    "**Analogy**: Imagine you're watching a car drive by:\n",
    "- **Continuous (analog)**: The car moves smoothly through space\n",
    "- **Discrete (digital)**: You take snapshots every 0.1 seconds to track its position\n",
    "\n",
    "The snapshots are your **samples**, and how often you take them is your **sampling rate**.\n",
    "\n",
    "### What is Sample Rate (Sampling Frequency)?\n",
    "\n",
    "**Sample rate** (also called **sampling frequency**) is the number of samples taken per second, measured in **Hertz (Hz)**.\n",
    "\n",
    "**Examples:**\n",
    "- **16,000 Hz** means 16,000 samples per second\n",
    "- **100 Hz** means 100 samples per second\n",
    "\n",
    "**Higher sample rate** = more snapshots per second = more detailed representation of the signal\n",
    "\n",
    "### Our Sensors and Their Sample Rates\n",
    "\n",
    "In this cough detection project, we use two types of sensors with different sample rates:\n",
    "\n",
    "| Sensor | Sample Rate | Samples per Second | What It Captures |\n",
    "|--------|-------------|-------------------|------------------|\n",
    "| **Microphone** | 16,000 Hz | 16,000 | Sound waves (cough, speech, ambient noise) |\n",
    "| **IMU (Accelerometer + Gyroscope)** | 100 Hz | 100 | Body motion (chest movement, head rotation) |\n",
    "\n",
    "**Why different rates?**\n",
    "- **Audio** changes very rapidly (sound frequencies range from 20 Hz to 20,000 Hz), so we need a high sample rate to capture it accurately\n",
    "- **Body motion** changes much more slowly than sound waves, so 100 samples/second is sufficient\n",
    "\n",
    "### From Real World to Digital Samples\n",
    "\n",
    "Here's what happens in practice:\n",
    "\n",
    "**When you cough:**\n",
    "\n",
    "1. **Microphone** measures air pressure 16,000 times per second ‚Üí Creates an array of 16,000 numbers per second\n",
    "2. **Accelerometer** measures chest acceleration 100 times per second ‚Üí Creates an array of 100 numbers per second\n",
    "3. **Gyroscope** measures body rotation 100 times per second ‚Üí Creates an array of 100 numbers per second\n",
    "\n",
    "**A 1-second recording produces:**\n",
    "- Audio: 16,000 values (numbers representing sound amplitude)\n",
    "- IMU: 100 values for each of the 6 channels (3 accelerometer axes + 3 gyroscope axes) = 600 values total\n",
    "\n",
    "These arrays of numbers are what we'll work with for feature extraction!\n",
    "\n",
    "### Why Does This Matter?\n",
    "\n",
    "Understanding sample rate is crucial because:\n",
    "1. It tells us **how much data** we're working with\n",
    "2. It determines **time-to-sample conversion** (e.g., \"0.5 seconds = 8,000 audio samples\")\n",
    "3. It affects **window sizes** for machine learning (explained in the next section)\n",
    "\n",
    "Let's verify our sensor sample rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d3470",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Audio sample rate: {FS_AUDIO} Hz\")\n",
    "print(f\"IMU sample rate: {FS_IMU} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bqlxbwbf6ng",
   "metadata": {},
   "source": [
    "## Understanding Windowing - Fixed-Length Segments\n",
    "\n",
    "### Why Do We Need Windows?\n",
    "\n",
    "Machine learning models require **fixed-size inputs**. But our recordings vary in length:\n",
    "- Some recordings are 10 seconds long\n",
    "- Others are 30 seconds or longer\n",
    "- A single cough might last 0.3 seconds, another might last 0.6 seconds\n",
    "\n",
    "**The solution**: Extract fixed-length **windows** (segments) from the recordings.\n",
    "\n",
    "### What is a Window?\n",
    "\n",
    "A **window** is a fixed-length segment of the signal, like cutting a movie into clips of exactly 10 seconds each.\n",
    "\n",
    "From the original paper:\n",
    "- **Window length**: 0.4 seconds\n",
    "- **Why 0.4 seconds?** This is long enough to capture a complete cough (typically 0.2-0.5s), but short enough to be efficient for ML\n",
    "\n",
    "### Window Length ‚Üî Sample Count Relationship\n",
    "\n",
    "The number of samples in a window depends on the **sampling frequency**:\n",
    "\n",
    "**Formula**: `number_of_samples = window_length_seconds √ó sampling_frequency`\n",
    "\n",
    "**For audio (16,000 Hz):**\n",
    "- 0.4 seconds √ó 16,000 samples/second = **6,400 samples**\n",
    "\n",
    "**For IMU (100 Hz):**\n",
    "- 0.4 seconds √ó 100 samples/second = **40 samples**\n",
    "\n",
    "**Analogy**: If you record video at 30 frames per second, a 2-second clip contains 2 √ó 30 = 60 frames. Same concept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sample_window(\n",
    "        sample: np.ndarray,\n",
    "        frequency: int, \n",
    "        window_duration: float,\n",
    "        start_point_index = None,\n",
    "        start_point_time = None,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Extract features from the given NumPy array, the window duration, and optionally, the starting point.\n",
    "    The starting point can either be an index of the sample, or a the time that sample should start,\n",
    "    which in this case will calculate the sample index automatically.\n",
    "\n",
    "    ## Example:\n",
    "    - Given the sample rate: 16000 Hz\n",
    "    - If window starts at 0.5s ‚Üí 0.5 * 16000 = 8000th sample\n",
    "    - The window length to extract is 0.4s ‚Üí 0.4 √ó 16000 = 6400 samples\n",
    "    - Thus, the sample region to extract is feature[8000: 8000 + 6400]\n",
    "    \"\"\"\n",
    "    window_length = int(window_duration * frequency)\n",
    "    if start_point_index is None:\n",
    "        if start_point_time is not None:\n",
    "            # Calculate the sample start point from the given time\n",
    "            start_point_index = int(start_point_time * frequency)\n",
    "        else:\n",
    "            raise ValueError(\"You must provide either start_point_index or start_point_time.\")\n",
    "    else:\n",
    "        # Clamp to [0, sample_length - window_length]\n",
    "        start_point_index = min(max(0, start_point_index), len(sample) - window_length)\n",
    "    \n",
    "    # Slice the sample using NumPy slicing\n",
    "    return sample[start_point_index: start_point_index + window_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77d112",
   "metadata": {},
   "source": [
    "### Two Types of Windows We'll Extract\n",
    "\n",
    "In this paper, we need to extract both cough and non-cough audios. Thus, we will have two types of windows.\n",
    "\n",
    "1. **Cough windows**\n",
    "   - We know the exact start/end time from `ground_truth.json`, so we can use it as a reference.\n",
    "   - We will extract their windows by centering around annotated cough events\n",
    "   - The window includes the cough, possibly shifted randomly for data augmentation\n",
    "\n",
    "2. **Non-cough windows**:\n",
    "   - There's no ground truth. We didn't cough!\n",
    "   - We will randomly sampled segments from non-cough sounds (laugh, throat-clearing, breathing recordings)\n",
    "   - Used as negative examples (label = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_window(audio, imu, start_point_time=None, start_point_index=None):\n",
    "    \"\"\"Extract audio and IMU window from the given window start point\"\"\"\n",
    "    audio_window = extract_sample_window(audio, FS_AUDIO, WINDOW_LEN, start_point_index, start_point_time)\n",
    "\n",
    "    # For IMU features, we need to stack all 6 IMU channels into a 2D array\n",
    "    # Channels: [Accel_x, Accel_y, Accel_z, Gyro_Y, Gyro_P, Gyro_R]\n",
    "    imu_window = np.column_stack([\n",
    "        extract_sample_window(feature, FS_IMU, WINDOW_LEN, start_point_index, start_point_time)\n",
    "        for feature in [imu.x, imu.y, imu.z, imu.Y, imu.P, imu.R]\n",
    "    ])\n",
    "    return audio_window, imu_window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240eb24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Understanding Features Through Examples\n",
    "\n",
    "We'll take **two real examples** from our dataset and walk through the feature extraction process step-by-step:\n",
    "\n",
    "- **Example A**: A real **cough** from the dataset\n",
    "- **Example B**: A **throat-clearing** sound (non-cough)\n",
    "\n",
    "By comparing these side-by-side, you'll understand:\n",
    "1. What each of the 105 features captures\n",
    "2. Why these features help distinguish coughs from other sounds\n",
    "3. How audio and IMU signals complement each other\n",
    "\n",
    "This hands-on walkthrough will make the abstract concept of \"features\" concrete and intuitive!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b31fcaa",
   "metadata": {},
   "source": [
    "### Step 0: Load dataset\n",
    "\n",
    "First, start by selecting one subject from the dataset as an example, and perform window extraction on that subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ed029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate dataset folder\n",
    "kaggle_dataset_dir = '/kaggle/input/edge-ai-cough-count'\n",
    "base_dir = kaggle_dataset_dir if os.path.exists(kaggle_dataset_dir) else \"..\"\n",
    "data_folder = base_dir + '/public_dataset/'\n",
    "\n",
    "# Check if exists, otherwise try alternative path\n",
    "if not os.path.exists(data_folder):\n",
    "    raise FileNotFoundError(\n",
    "        \"Cannot find public_dataset/. Please download from: \"\n",
    "        \"https://zenodo.org/record/7562332\"\n",
    "    )\n",
    "\n",
    "# Get list of subject IDs\n",
    "subject_ids = [d for d in os.listdir(data_folder) \n",
    "               if os.path.isdir(os.path.join(data_folder, d))]\n",
    "subject_ids = sorted(subject_ids)\n",
    "\n",
    "print(f\"‚úì Found {len(subject_ids)} subjects: {subject_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mug2ht9jwup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Load Example A: Cough Window\n",
    "# ===================================================================\n",
    "# Using subject \"14287\", trial 1, sitting, no noise, cough sound\n",
    "example_subj = \"14287\"\n",
    "\n",
    "# Load full audio signals (both microphones: air-facing and body-facing)\n",
    "cough_audio_air, cough_audio_skin = load_audio(\n",
    "    data_folder, example_subj, Trial.ONE, Movement.SIT, Noise.NONE, Sound.COUGH\n",
    ")\n",
    "\n",
    "# Load full IMU signals (all 6 channels: accel x,y,z + gyro Y,P,R)\n",
    "cough_imu = load_imu(\n",
    "    data_folder, example_subj, Trial.ONE, Movement.SIT, Noise.NONE, Sound.COUGH\n",
    ")\n",
    "\n",
    "# Load ground truth annotations (start/end times of each cough in seconds)\n",
    "cough_starts, cough_ends = load_annotation(\n",
    "    data_folder, example_subj, Trial.ONE, Movement.SIT, Noise.NONE, Sound.COUGH\n",
    ")\n",
    "\n",
    "cough_audio_window, cough_imu_window = extract_subject_window(\n",
    "    audio=cough_audio_air,\n",
    "    imu=cough_imu,\n",
    "    # For window start point, we have ground truth on when the cough happens\n",
    "    # Let's select the time when we first cough\n",
    "    start_point_time=cough_starts[0]\n",
    ")\n",
    "\n",
    "# ===================================================================\n",
    "# Load Example B: Throat-Clearing Window (Non-Cough)\n",
    "# ===================================================================\n",
    "# Load full throat-clearing signals\n",
    "throat_audio_air, throat_audio_skin = load_audio(\n",
    "    data_folder, example_subj, Trial.ONE, Movement.SIT, Noise.NONE, Sound.THROAT\n",
    ")\n",
    "throat_imu = load_imu(\n",
    "    data_folder, example_subj, Trial.ONE, Movement.SIT, Noise.NONE, Sound.THROAT\n",
    ")\n",
    "\n",
    "throat_audio_window, throat_imu_window = extract_subject_window(\n",
    "    audio=throat_audio_air,\n",
    "    imu=throat_imu,\n",
    "    # We don't have a starting point in mind, so pick randomly from the sample length\n",
    "    start_point_index=np.random.randint(0, len(throat_audio_air))\n",
    ")\n",
    "\n",
    "print(\"‚úì Loaded example windows:\")\n",
    "print(f\"  Cough: {len(cough_audio_window)} audio samples, {len(cough_imu_window)} IMU samples\")\n",
    "print(f\"  Throat-clearing: {len(throat_audio_window)} audio samples, {len(throat_imu_window)} IMU samples\")\n",
    "print(f\"\\n  Expected: 6400 audio samples (0.4s √ó 16kHz), 40 IMU samples (0.4s √ó 100Hz)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb8ff7",
   "metadata": {},
   "source": [
    "### Step 1: Select and Visualize Example Windows\n",
    "\n",
    "Let's look at the raw signals. Notice the differences in waveform patterns between cough and throat-clearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4j7tku4bxe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize raw signals side-by-side\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# Cough audio\n",
    "time_audio = np.arange(len(cough_audio_window)) / FS_AUDIO\n",
    "axes[0, 0].plot(time_audio, cough_audio_window, color='steelblue', linewidth=0.8)\n",
    "axes[0, 0].set_title('Example A: Cough - Audio Signal', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Amplitude', fontsize=11)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "# Add annotation for sharp burst\n",
    "peak_idx = np.argmax(np.abs(cough_audio_window))\n",
    "peak_val = cough_audio_window[peak_idx]\n",
    "y_range = axes[0, 0].get_ylim()[1] - axes[0, 0].get_ylim()[0]\n",
    "text_offset = y_range * 0.20 if peak_val > 0 else -y_range * 0.20\n",
    "axes[0, 0].annotate('Sharp burst ‚Üí', \n",
    "                    xy=(peak_idx/FS_AUDIO, peak_val),\n",
    "                    xytext=(peak_idx/FS_AUDIO - 0.08, peak_val + text_offset),\n",
    "                    arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                    fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "# Cough IMU (accelerometer Z)\n",
    "time_imu = np.arange(len(cough_imu_window)) / FS_IMU\n",
    "axes[0, 1].plot(time_imu, -cough_imu_window[:, 2], color='darkgreen', linewidth=1.5)\n",
    "axes[0, 1].set_title('Example A: Cough - IMU Accel Z', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Acceleration', fontsize=11)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "# Add annotation for IMU spike\n",
    "imu_peak_idx = np.argmax(-cough_imu_window[:, 2])\n",
    "imu_peak_val = -cough_imu_window[imu_peak_idx, 2]\n",
    "y_range_imu = axes[0, 1].get_ylim()[1] - axes[0, 1].get_ylim()[0]\n",
    "text_offset_imu = y_range_imu * 0.20 if imu_peak_val > 0 else -y_range_imu * 0.20\n",
    "axes[0, 1].annotate('‚Üê IMU spike', \n",
    "                    xy=(imu_peak_idx/FS_IMU, imu_peak_val),\n",
    "                    xytext=(imu_peak_idx/FS_IMU + 0.05, imu_peak_val + text_offset_imu),\n",
    "                    arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                    fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "# Throat-clearing audio\n",
    "axes[1, 0].plot(time_audio, throat_audio_window, color='orange', linewidth=0.8)\n",
    "axes[1, 0].set_title('Example B: Throat-Clearing - Audio Signal', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Amplitude', fontsize=11)\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Throat-clearing IMU\n",
    "axes[1, 1].plot(time_imu, -throat_imu_window[:, 2], color='purple', linewidth=1.5)\n",
    "axes[1, 1].set_title('Example B: Throat-Clearing - IMU Accel Z', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Acceleration', fontsize=11)\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699e94c2",
   "metadata": {},
   "source": [
    "#### üìä Observations:\n",
    "- Cough audio: Sharp, sudden burst with high amplitude\n",
    "- Cough IMU: Clear spike corresponding to chest/body movement\n",
    "- Throat-clearing: More gradual, sustained sound pattern\n",
    "- Features will capture these differences quantitatively!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v34vph3l48d",
   "metadata": {},
   "source": [
    "### Step 2: Understanding Audio - The Spectrogram\n",
    "\n",
    "Audio signals have both **time** and **frequency** information. A **spectrogram** shows how frequencies change over time - like a musical score that displays which notes (frequencies) are played when.\n",
    "\n",
    "**Think of it like this:**\n",
    "- **Low frequencies** (bottom of spectrogram): Deep, bass sounds\n",
    "- **High frequencies** (top of spectrogram): Sharp, treble sounds like \"s\" or \"sh\"\n",
    "- **Color intensity**: How loud that frequency is at that moment\n",
    "\n",
    "Let's visualize the spectrograms for our cough vs throat-clearing examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3yabj7fw14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spectrograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cough spectrogram\n",
    "D_cough = librosa.amplitude_to_db(np.abs(librosa.stft(cough_audio_window)), ref=np.max)\n",
    "img1 = librosa.display.specshow(D_cough, sr=FS_AUDIO, x_axis='time', y_axis='hz', \n",
    "                                 ax=axes[0], cmap='viridis')\n",
    "axes[0].set_title('Example A: Cough - Spectrogram', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency (Hz)', fontsize=11)\n",
    "axes[0].set_ylim(0, 8000)\n",
    "fig.colorbar(img1, ax=axes[0], format='%+2.0f dB')\n",
    "\n",
    "# Throat-clearing spectrogram\n",
    "D_throat = librosa.amplitude_to_db(np.abs(librosa.stft(throat_audio_window)), ref=np.max)\n",
    "img2 = librosa.display.specshow(D_throat, sr=FS_AUDIO, x_axis='time', y_axis='hz',\n",
    "                                 ax=axes[1], cmap='viridis')\n",
    "axes[1].set_title('Example B: Throat-Clearing - Spectrogram', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[1].set_ylabel('Frequency (Hz)', fontsize=11)\n",
    "axes[1].set_ylim(0, 8000)\n",
    "fig.colorbar(img2, ax=axes[1], format='%+2.0f dB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8c096",
   "metadata": {},
   "source": [
    "#### üìä What we observe:\n",
    "- Cough: Sharp vertical bands (sudden burst) with strong high-frequency content (bright yellow at top)\n",
    "- Throat-clearing: More horizontal spread (sustained) with energy concentrated in lower frequencies\n",
    "- Coughs are like cymbal crashes - lots of high frequencies\n",
    "- Throat-clearing is more like clearing vocal cords - lower, more gradual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rj5c2xgenie",
   "metadata": {},
   "source": [
    "### Step 3: MFCC Features - Capturing Audio \"Texture\"\n",
    "\n",
    "**MFCC** stands for **Mel-Frequency Cepstral Coefficients** - a fancy name for features that capture the \"shape\" or \"texture\" of sound.\n",
    "\n",
    "**Beginner-friendly explanation:**\n",
    "- MFCCs are inspired by how human ears perceive sound\n",
    "- The \"Mel\" scale mimics how we hear: we're better at distinguishing low frequencies than high ones\n",
    "- We extract **13 MFCC values** at each time point, then compute statistics (mean, std, min, max) across the window\n",
    "- This gives us **13 √ó 4 = 52 MFCC features** per audio window\n",
    "\n",
    "**Why MFCCs matter:**\n",
    "- MFCC coefficient 1-3 capture the overall spectral shape (coughs are sharper and more percussive)\n",
    "- Higher coefficients capture fine details of the sound texture\n",
    "- They're widely used in speech recognition and audio classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fpd5brb6hnh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MFCCs for both examples\n",
    "mfcc_cough = librosa.feature.mfcc(y=cough_audio_window, sr=FS_AUDIO, n_mfcc=13)\n",
    "mfcc_throat = librosa.feature.mfcc(y=throat_audio_window, sr=FS_AUDIO, n_mfcc=13)\n",
    "\n",
    "# Compute statistics for comparison\n",
    "mfcc_cough_mean = np.mean(mfcc_cough, axis=1)\n",
    "mfcc_throat_mean = np.mean(mfcc_throat, axis=1)\n",
    "mfcc_cough_std = np.std(mfcc_cough, axis=1)\n",
    "mfcc_throat_std = np.std(mfcc_throat, axis=1)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# MFCC heatmap for cough\n",
    "img1 = axes[0, 0].imshow(mfcc_cough, aspect='auto', origin='lower', cmap='coolwarm')\n",
    "axes[0, 0].set_title('Example A: Cough - MFCC Heatmap', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time Frame', fontsize=11)\n",
    "axes[0, 0].set_ylabel('MFCC Coefficient', fontsize=11)\n",
    "axes[0, 0].set_yticks(range(13))\n",
    "fig.colorbar(img1, ax=axes[0, 0])\n",
    "\n",
    "# MFCC heatmap for throat-clearing\n",
    "img2 = axes[0, 1].imshow(mfcc_throat, aspect='auto', origin='lower', cmap='coolwarm')\n",
    "axes[0, 1].set_title('Example B: Throat-Clearing - MFCC Heatmap', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Time Frame', fontsize=11)\n",
    "axes[0, 1].set_ylabel('MFCC Coefficient', fontsize=11)\n",
    "axes[0, 1].set_yticks(range(13))\n",
    "fig.colorbar(img2, ax=axes[0, 1])\n",
    "\n",
    "# Bar chart of MFCC means\n",
    "x = np.arange(13)\n",
    "width = 0.35\n",
    "axes[1, 0].bar(x - width/2, mfcc_cough_mean, width, label='Cough', color='steelblue', alpha=0.8)\n",
    "axes[1, 0].bar(x + width/2, mfcc_throat_mean, width, label='Throat-clearing', color='orange', alpha=0.8)\n",
    "axes[1, 0].set_title('MFCC Mean Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('MFCC Coefficient', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Mean Value', fontsize=11)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Bar chart of MFCC standard deviations\n",
    "axes[1, 1].bar(x - width/2, mfcc_cough_std, width, label='Cough', color='steelblue', alpha=0.8)\n",
    "axes[1, 1].bar(x + width/2, mfcc_throat_std, width, label='Throat-clearing', color='orange', alpha=0.8)\n",
    "axes[1, 1].set_title('MFCC Standard Deviation Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('MFCC Coefficient', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Std Value', fontsize=11)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a623a1",
   "metadata": {},
   "source": [
    "#### üìä Key insights:\n",
    "- Notice how coefficients 0-3 differ significantly between cough and throat-clearing\n",
    "- These capture the overall spectral shape - coughs are sharper and more percussive\n",
    "- The standard deviation (variability over time) also differs\n",
    "- Together, these 52 features (13 coefficients √ó 4 statistics) provide a powerful signature!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ia8z661l",
   "metadata": {},
   "source": [
    "### Step 4: Spectral Features - Frequency Characteristics\n",
    "\n",
    "Spectral features describe **WHERE the energy is concentrated** in the frequency spectrum. Think of them as describing the \"color\" of sound.\n",
    "\n",
    "**Beginner-friendly definitions:**\n",
    "- **Spectral Centroid**: The \"center of mass\" of frequencies - like finding the average frequency. A high centroid means more high-frequency content (like \"s\" sounds).\n",
    "- **Spectral Rolloff**: The frequency below which 85% of the energy is concentrated. Higher rolloff = more high-frequency energy.\n",
    "- **Spectral Bandwidth**: The \"spread\" of frequencies - how wide the frequency range is.\n",
    "- **Spectral Flatness**: How \"noisy\" vs \"tonal\" the sound is (white noise = 1, pure tone = 0).\n",
    "\n",
    "Let's visualize these for our examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egxz8e0o0ct",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectral features\n",
    "\n",
    "# Compute PSD (Power Spectral Density)\n",
    "freqs_cough, psd_cough = signal.welch(cough_audio_window, fs=FS_AUDIO, nperseg=512)\n",
    "freqs_throat, psd_throat = signal.welch(throat_audio_window, fs=FS_AUDIO, nperseg=512)\n",
    "\n",
    "# Compute spectral centroid\n",
    "centroid_cough = np.mean(librosa.feature.spectral_centroid(y=cough_audio_window, sr=FS_AUDIO))\n",
    "centroid_throat = np.mean(librosa.feature.spectral_centroid(y=throat_audio_window, sr=FS_AUDIO))\n",
    "\n",
    "# Compute spectral rolloff\n",
    "rolloff_cough = np.mean(librosa.feature.spectral_rolloff(y=cough_audio_window, sr=FS_AUDIO))\n",
    "rolloff_throat = np.mean(librosa.feature.spectral_rolloff(y=throat_audio_window, sr=FS_AUDIO))\n",
    "\n",
    "# Compute spectral bandwidth\n",
    "bandwidth_cough = np.mean(librosa.feature.spectral_bandwidth(y=cough_audio_window, sr=FS_AUDIO))\n",
    "bandwidth_throat = np.mean(librosa.feature.spectral_bandwidth(y=throat_audio_window, sr=FS_AUDIO))\n",
    "\n",
    "# Visualize PSD with spectral features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cough PSD\n",
    "axes[0].plot(freqs_cough, psd_cough, color='steelblue', linewidth=2, label='PSD')\n",
    "axes[0].axvline(centroid_cough, color='red', linestyle='--', linewidth=2, label=f'Centroid: {centroid_cough:.0f} Hz')\n",
    "axes[0].axvline(rolloff_cough, color='blue', linestyle='--', linewidth=2, label=f'Rolloff: {rolloff_cough:.0f} Hz')\n",
    "axes[0].axvspan(centroid_cough - bandwidth_cough/2, centroid_cough + bandwidth_cough/2, \n",
    "                alpha=0.2, color='green', label=f'Bandwidth: {bandwidth_cough:.0f} Hz')\n",
    "axes[0].set_title('Example A: Cough - Spectral Features', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Frequency (Hz)', fontsize=11)\n",
    "axes[0].set_ylabel('Power Spectral Density', fontsize=11)\n",
    "axes[0].set_xlim(0, 8000)\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Throat-clearing PSD\n",
    "axes[1].plot(freqs_throat, psd_throat, color='orange', linewidth=2, label='PSD')\n",
    "axes[1].axvline(centroid_throat, color='red', linestyle='--', linewidth=2, label=f'Centroid: {centroid_throat:.0f} Hz')\n",
    "axes[1].axvline(rolloff_throat, color='blue', linestyle='--', linewidth=2, label=f'Rolloff: {rolloff_throat:.0f} Hz')\n",
    "axes[1].axvspan(centroid_throat - bandwidth_throat/2, centroid_throat + bandwidth_throat/2,\n",
    "                alpha=0.2, color='green', label=f'Bandwidth: {bandwidth_throat:.0f} Hz')\n",
    "axes[1].set_title('Example B: Throat-Clearing - Spectral Features', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Frequency (Hz)', fontsize=11)\n",
    "axes[1].set_ylabel('Power Spectral Density', fontsize=11)\n",
    "axes[1].set_xlim(0, 8000)\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create comparison table\n",
    "print(\"\\nüìä Spectral Feature Comparison:\")\n",
    "print(f\"{'Feature':<20} {'Cough':>15} {'Throat-Clearing':>20}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Spectral Centroid':<20} {centroid_cough:>12.1f} Hz {centroid_throat:>17.1f} Hz\")\n",
    "print(f\"{'Spectral Rolloff':<20} {rolloff_cough:>12.1f} Hz {rolloff_throat:>17.1f} Hz\")\n",
    "print(f\"{'Spectral Bandwidth':<20} {bandwidth_cough:>12.1f} Hz {bandwidth_throat:>17.1f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b7d85a",
   "metadata": {},
   "source": [
    "#### üí° Insight:\n",
    "- Coughs typically have HIGHER spectral centroid and rolloff (more high-frequency 'hiss')\n",
    "- Throat-clearing has more energy in LOWER frequencies (vocal cord vibrations)\n",
    "- These differences help the ML model distinguish coughs from other sounds!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7py9fm3ubf",
   "metadata": {},
   "source": [
    "### Step 5: Time-Domain Features - Temporal Characteristics\n",
    "\n",
    "Time-domain features describe properties of the waveform **over time** (rather than in frequency).\n",
    "\n",
    "**Beginner-friendly definitions:**\n",
    "- **Zero-Crossing Rate (ZCR)**: How often the signal crosses zero (related to frequency). High ZCR = lots of oscillations.\n",
    "- **RMS Energy**: Root Mean Square energy - a measure of \"loudness\" or overall power.\n",
    "- **Crest Factor**: Ratio of peak amplitude to RMS. Measures how \"peaky\" or \"spiky\" the signal is.\n",
    "  - High crest factor = sharp transients (like a cough)\n",
    "  - Low crest factor = smooth, sustained sound (like a vowel)\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è Important Note on DC Offset and Preprocessing:**\n",
    "\n",
    "According to the research paper methodology:\n",
    "- **IMU signals**: DC offset (mean) is explicitly removed from each 0.4s window before feature extraction\n",
    "- **Audio signals**: No DC offset removal is mentioned - features are computed on raw audio\n",
    "\n",
    "**For the visualization cell below ONLY**, we remove DC offset from audio to improve visual clarity when comparing peak/RMS levels. This makes it easier to see the AC component (the actual oscillations we care about).\n",
    "\n",
    "**The actual feature extraction functions compute features on raw audio signals** (with DC offset intact), staying faithful to the paper. The visualizations below are for educational purposes only.\n",
    "\n",
    "Let's visualize these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toh75tdx49h",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute time-domain features\n",
    "# Zero-crossing rate\n",
    "zcr_cough = librosa.feature.zero_crossing_rate(cough_audio_window)[0].mean()\n",
    "zcr_throat = librosa.feature.zero_crossing_rate(throat_audio_window)[0].mean()\n",
    "\n",
    "# RMS energy\n",
    "rms_cough = np.sqrt(np.mean(cough_audio_window**2))\n",
    "rms_throat = np.sqrt(np.mean(throat_audio_window**2))\n",
    "\n",
    "# Crest factor\n",
    "crest_cough = np.max(np.abs(cough_audio_window)) / (rms_cough + 1e-10)\n",
    "crest_throat = np.max(np.abs(throat_audio_window)) / (rms_throat + 1e-10)\n",
    "\n",
    "# ===================================================================\n",
    "# For VISUALIZATION ONLY: Remove DC offset for clarity\n",
    "# ===================================================================\n",
    "cough_centered = cough_audio_window - np.mean(cough_audio_window)\n",
    "throat_centered = throat_audio_window - np.mean(throat_audio_window)\n",
    "\n",
    "# Compute visualization-only metrics on DC-centered signals\n",
    "zcr_cough_vis = np.sum(np.diff(np.sign(cough_centered)) != 0) / len(cough_centered)\n",
    "zcr_throat_vis = np.sum(np.diff(np.sign(throat_centered)) != 0) / len(throat_centered)\n",
    "\n",
    "rms_cough_vis = np.sqrt(np.mean(cough_centered**2))\n",
    "rms_throat_vis = np.sqrt(np.mean(throat_centered**2))\n",
    "\n",
    "peak_cough_vis = np.max(np.abs(cough_centered))\n",
    "peak_throat_vis = np.max(np.abs(throat_centered))\n",
    "\n",
    "crest_cough_vis = peak_cough_vis / (rms_cough_vis + 1e-10)\n",
    "crest_throat_vis = peak_throat_vis / (rms_throat_vis + 1e-10)\n",
    "\n",
    "# ===================================================================\n",
    "# Visualize\n",
    "# ===================================================================\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "\n",
    "# Row 1: Waveforms with zero crossings (DC-centered)\n",
    "time_audio = np.arange(len(cough_audio_window)) / FS_AUDIO\n",
    "\n",
    "axes[0, 0].plot(time_audio, cough_centered, color='steelblue', linewidth=0.8, label='Waveform (DC removed)')\n",
    "axes[0, 0].axhline(0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "zero_crossings_cough = np.where(np.diff(np.sign(cough_centered)))[0]\n",
    "axes[0, 0].scatter(zero_crossings_cough[:100]/FS_AUDIO, \n",
    "                   np.zeros(min(100, len(zero_crossings_cough))), \n",
    "                   color='red', s=10, alpha=0.5, label='Zero crossings (first 100)')\n",
    "axes[0, 0].set_title(f'Example A: Cough - Zero Crossings (ZCR={zcr_cough_vis:.4f})', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time (seconds)', fontsize=10)\n",
    "axes[0, 0].set_ylabel('Amplitude (DC removed)', fontsize=10)\n",
    "axes[0, 0].legend(fontsize=9)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(time_audio, throat_centered, color='orange', linewidth=0.8, label='Waveform (DC removed)')\n",
    "axes[0, 1].axhline(0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "zero_crossings_throat = np.where(np.diff(np.sign(throat_centered)))[0]\n",
    "axes[0, 1].scatter(zero_crossings_throat[:100]/FS_AUDIO,\n",
    "                   np.zeros(min(100, len(zero_crossings_throat))),\n",
    "                   color='red', s=10, alpha=0.5, label='Zero crossings (first 100)')\n",
    "axes[0, 1].set_title(f'Example B: Throat-Clearing - Zero Crossings (ZCR={zcr_throat_vis:.4f})',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Time (seconds)', fontsize=10)\n",
    "axes[0, 1].set_ylabel('Amplitude (DC removed)', fontsize=10)\n",
    "axes[0, 1].legend(fontsize=9)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Row 2: RMS Energy comparison (visualization values)\n",
    "axes[1, 0].bar(['Cough', 'Throat-Clearing'], [rms_cough_vis, rms_throat_vis], \n",
    "               color=['steelblue', 'orange'], alpha=0.7)\n",
    "axes[1, 0].set_title('RMS Energy Comparison (DC removed)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('RMS Energy', fontsize=10)\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "for i, (label, val) in enumerate([('Cough', rms_cough_vis), ('Throat', rms_throat_vis)]):\n",
    "    axes[1, 0].text(i, val, f'{val:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Row 2: Crest Factor comparison (visualization values)\n",
    "axes[1, 1].bar(['Cough', 'Throat-Clearing'], [crest_cough_vis, crest_throat_vis],\n",
    "               color=['steelblue', 'orange'], alpha=0.7)\n",
    "axes[1, 1].set_title('Crest Factor Comparison (DC removed)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Crest Factor (Peak/RMS)', fontsize=10)\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "for i, (label, val) in enumerate([('Cough', crest_cough_vis), ('Throat', crest_throat_vis)]):\n",
    "    axes[1, 1].text(i, val, f'{val:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Row 3: Waveforms with peak and RMS lines (DC-centered)\n",
    "axes[2, 0].plot(time_audio, cough_centered, color='steelblue', linewidth=0.8, label='Waveform (DC removed)')\n",
    "axes[2, 0].axhline(rms_cough_vis, color='green', linestyle='--', linewidth=2, label=f'RMS: {rms_cough_vis:.4f}')\n",
    "axes[2, 0].axhline(-rms_cough_vis, color='green', linestyle='--', linewidth=2)\n",
    "axes[2, 0].axhline(peak_cough_vis, color='red', linestyle='--', linewidth=2, label=f'Peak: {peak_cough_vis:.4f}')\n",
    "axes[2, 0].axhline(-peak_cough_vis, color='red', linestyle='--', linewidth=2)\n",
    "axes[2, 0].set_title(f'Example A: Cough - Peak vs RMS (Crest={crest_cough_vis:.2f})',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[2, 0].set_xlabel('Time (seconds)', fontsize=10)\n",
    "axes[2, 0].set_ylabel('Amplitude (DC removed)', fontsize=10)\n",
    "axes[2, 0].legend(fontsize=9)\n",
    "axes[2, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[2, 1].plot(time_audio, throat_centered, color='orange', linewidth=0.8, label='Waveform (DC removed)')\n",
    "axes[2, 1].axhline(rms_throat_vis, color='green', linestyle='--', linewidth=2, label=f'RMS: {rms_throat_vis:.4f}')\n",
    "axes[2, 1].axhline(-rms_throat_vis, color='green', linestyle='--', linewidth=2)\n",
    "axes[2, 1].axhline(peak_throat_vis, color='red', linestyle='--', linewidth=2, label=f'Peak: {peak_throat_vis:.4f}')\n",
    "axes[2, 1].axhline(-peak_throat_vis, color='red', linestyle='--', linewidth=2)\n",
    "axes[2, 1].set_title(f'Example B: Throat-Clearing - Peak vs RMS (Crest={crest_throat_vis:.2f})',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[2, 1].set_xlabel('Time (seconds)', fontsize=10)\n",
    "axes[2, 1].set_ylabel('Amplitude (DC removed)', fontsize=10)\n",
    "axes[2, 1].legend(fontsize=9)\n",
    "axes[2, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# Print comparison table using ACTUAL features (raw signal)\n",
    "# ===================================================================\n",
    "print(\"\\nüìä Time-Domain Feature Comparison (ACTUAL features used by ML model):\")\n",
    "print(f\"{'Feature':<20} {'Cough':>15} {'Throat-Clearing':>20}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Zero-Crossing Rate':<20} {zcr_cough:>15.4f} {zcr_throat:>20.4f}\")\n",
    "print(f\"{'RMS Energy':<20} {rms_cough:>15.4f} {rms_throat:>20.4f}\")\n",
    "print(f\"{'Crest Factor':<20} {crest_cough:>15.2f} {crest_throat:>20.2f}\")\n",
    "print(f\"{'DC Offset (mean)':<20} {np.mean(cough_audio_window):>15.4f} {np.mean(throat_audio_window):>20.4f}\")\n",
    "\n",
    "print(\"\\nüìä Visualization-Only Metrics (DC-centered for clarity):\")\n",
    "print(f\"{'Feature':<20} {'Cough':>15} {'Throat-Clearing':>20}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Zero-Crossing Rate':<20} {zcr_cough_vis:>15.4f} {zcr_throat_vis:>20.4f}\")\n",
    "print(f\"{'RMS Energy':<20} {rms_cough_vis:>15.4f} {rms_throat_vis:>20.4f}\")\n",
    "print(f\"{'Crest Factor':<20} {crest_cough_vis:>15.2f} {crest_throat_vis:>20.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85851a39",
   "metadata": {},
   "source": [
    "#### üí° Insight:\n",
    "- Coughs have HIGH crest factor (sharp peaks) - characteristic of impulsive sounds\n",
    "- Throat-clearing has LOWER crest factor - more gradual and sustained\n",
    "- Higher ZCR in coughs reflects the noisy, broadband nature of the sound\n",
    "\n",
    "#### ‚ö†Ô∏è Note:\n",
    "- ACTUAL ML features are computed on raw audio (with DC offset)\n",
    "- Visualizations use DC-centered audio for clarity only\n",
    "- This follows the paper methodology: no DC removal for audio, but DC removal for IMU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ov15ujw525",
   "metadata": {},
   "source": [
    "### Step 6: IMU Features - Motion Signals\n",
    "\n",
    "Now let's look at the **Inertial Measurement Unit (IMU)** - the motion sensors.\n",
    "\n",
    "**Beginner-friendly explanation:**\n",
    "- **IMU**: A sensor that measures motion using two components:\n",
    "  - **Accelerometer**: Measures linear acceleration (like feeling pushed in a car). Captures chest/body movement during cough.\n",
    "  - **Gyroscope**: Measures rotation (like turning your head). Captures head/neck movement during cough.\n",
    "\n",
    "- **Each sensor has 3 axes**: X, Y, Z (measuring motion in 3D space)\n",
    "- **L2 norm**: The total magnitude of motion, calculated as ‚àö(x¬≤ + y¬≤ + z¬≤)\n",
    "  - Think of it like calculating distance: if you walk 3 meters north and 4 meters east, you're ‚àö(3¬≤ + 4¬≤) = 5 meters from your starting point\n",
    "\n",
    "**Total**: 3 accel + 1 accel_L2 + 3 gyro + 1 gyro_L2 = **8 signals** √ó 5 features each = **40 IMU features**\n",
    "\n",
    "Let's visualize all 8 signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6k1w5msuon5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L2 norms\n",
    "# First, center the signals (subtract mean) as per the paper\n",
    "cough_imu_centered = cough_imu_window - np.mean(cough_imu_window, axis=0, keepdims=True)\n",
    "throat_imu_centered = throat_imu_window - np.mean(throat_imu_window, axis=0, keepdims=True)\n",
    "\n",
    "# Compute L2 norms\n",
    "cough_accel_l2 = np.linalg.norm(cough_imu_centered[:, 0:3], axis=1)\n",
    "cough_gyro_l2 = np.linalg.norm(cough_imu_centered[:, 3:6], axis=1)\n",
    "throat_accel_l2 = np.linalg.norm(throat_imu_centered[:, 0:3], axis=1)\n",
    "throat_gyro_l2 = np.linalg.norm(throat_imu_centered[:, 3:6], axis=1)\n",
    "\n",
    "# Build per-feature signal arrays (same order for both classes)\n",
    "time_imu = np.arange(40) / FS_IMU\n",
    "signal_names = ['Accel X', 'Accel Y', 'Accel Z', 'Accel L2', 'Gyro Y', 'Gyro P', 'Gyro R', 'Gyro L2']\n",
    "colors = ['steelblue', 'darkgreen', 'purple', 'red', 'orange', 'brown', 'pink', 'darkred']\n",
    "\n",
    "cough_signals = [\n",
    "    cough_imu_centered[:, 0],  # Accel X\n",
    "    cough_imu_centered[:, 1],  # Accel Y\n",
    "    cough_imu_centered[:, 2],  # Accel Z\n",
    "    cough_accel_l2,            # Accel L2\n",
    "    cough_imu_centered[:, 3],  # Gyro Y\n",
    "    cough_imu_centered[:, 4],  # Gyro P\n",
    "    cough_imu_centered[:, 5],  # Gyro R\n",
    "    cough_gyro_l2              # Gyro L2\n",
    "]\n",
    "\n",
    "throat_signals = [\n",
    "    throat_imu_centered[:, 0],  # Accel X\n",
    "    throat_imu_centered[:, 1],  # Accel Y\n",
    "    throat_imu_centered[:, 2],  # Accel Z\n",
    "    throat_accel_l2,            # Accel L2\n",
    "    throat_imu_centered[:, 3],  # Gyro Y\n",
    "    throat_imu_centered[:, 4],  # Gyro P\n",
    "    throat_imu_centered[:, 5],  # Gyro R\n",
    "    throat_gyro_l2              # Gyro L2\n",
    "]\n",
    "\n",
    "# --- One figure: 8 rows (features) √ó 2 cols (Cough vs Throat) ---\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=8, ncols=2,\n",
    "    figsize=(16, 18),\n",
    "    sharex=True\n",
    ")\n",
    "\n",
    "for i, (name, color) in enumerate(zip(signal_names, colors)):\n",
    "    # Left column: Cough\n",
    "    axL = axes[i, 0]\n",
    "    axL.plot(time_imu, cough_signals[i], color=color, linewidth=2)\n",
    "    axL.set_title(f'Cough - {name}', fontsize=11, fontweight='bold')\n",
    "    axL.set_ylabel('Magnitude', fontsize=9)\n",
    "    axL.grid(alpha=0.3)\n",
    "    axL.axhline(0, color='black', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "    # Right column: Throat-clearing\n",
    "    axR = axes[i, 1]\n",
    "    axR.plot(time_imu, throat_signals[i], color=color, linewidth=2)\n",
    "    axR.set_title(f'Throat-Clearing - {name}', fontsize=11, fontweight='bold')\n",
    "    axR.grid(alpha=0.3)\n",
    "    axR.axhline(0, color='black', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# Only bottom row gets x-labels to reduce clutter\n",
    "axes[-1, 0].set_xlabel('Time (seconds)', fontsize=9)\n",
    "axes[-1, 1].set_xlabel('Time (seconds)', fontsize=9)\n",
    "\n",
    "plt.suptitle('IMU Features - Motion Signals (Cough vs Throat-Clearing)', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d2448",
   "metadata": {},
   "source": [
    "#### üìä Observations:\n",
    "- Cough creates distinctive motion patterns:\n",
    "    - Sudden chest movement (accelerometer)\n",
    "    - Head/neck movement (gyroscope)\n",
    "- L2 norms capture the total magnitude regardless of direction\n",
    "- These motion signatures complement audio features for robust detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hq099vawkhr",
   "metadata": {},
   "source": [
    "### Step 7: IMU Statistical Features\n",
    "\n",
    "For each of the 8 IMU signals, we compute **5 statistical features**:\n",
    "\n",
    "**Beginner-friendly definitions:**\n",
    "- **Line Length**: Sum of absolute differences between consecutive samples. Measures how \"wiggly\" or active the signal is.\n",
    "- **Zero-Crossing Rate**: How often the signal changes sign (crosses zero). Similar to audio ZCR.\n",
    "- **Kurtosis**: A measure of \"peakiness\". High kurtosis = sharp spikes; low kurtosis = smooth signal.\n",
    "- **Crest Factor**: Peak-to-RMS ratio (same as audio). Measures impulsiveness.\n",
    "- **RMS**: Root Mean Square - overall magnitude of the signal.\n",
    "\n",
    "Let's extract all 40 features and compare cough vs throat-clearing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u7ic4ey7xjl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract full 40 IMU features using our feature extraction function\n",
    "imu_features_cough = extract_imu_features(cough_imu_window)\n",
    "imu_features_throat = extract_imu_features(throat_imu_window)\n",
    "\n",
    "# Create feature names for visualization\n",
    "feature_types = ['Line Length', 'ZCR', 'Kurtosis', 'Crest', 'RMS']\n",
    "signal_names_short = ['Acc_X', 'Acc_Y', 'Acc_Z', 'Acc_L2', 'Gyro_Y', 'Gyro_P', 'Gyro_R', 'Gyro_L2']\n",
    "feature_names = []\n",
    "for sig in signal_names_short:\n",
    "    for feat in feature_types:\n",
    "        feature_names.append(f'{sig}_{feat}')\n",
    "\n",
    "# Visualize all 40 features as a bar chart\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "x = np.arange(40)\n",
    "width = 0.35\n",
    "\n",
    "# Use raw feature values (no normalization for 2-sample comparison)\n",
    "ax.bar(x - width/2, imu_features_cough, width, label='Cough', color='steelblue', alpha=0.7)\n",
    "ax.bar(x + width/2, imu_features_throat, width, label='Throat-Clearing', color='orange', alpha=0.7)\n",
    "\n",
    "ax.set_title('All 40 IMU Features Comparison (Raw Values)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Feature Index', fontsize=12)\n",
    "ax.set_ylabel('Feature Value', fontsize=12)\n",
    "ax.set_xticks(x[::5])  # Show every 5th tick\n",
    "ax.set_xticklabels(range(0, 40, 5))\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "ax.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Add vertical lines to separate signal groups\n",
    "for i in range(1, 8):\n",
    "    ax.axvline(i * 5 - 0.5, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# Add signal labels\n",
    "signal_positions = [2.5, 7.5, 12.5, 17.5, 22.5, 27.5, 32.5, 37.5]\n",
    "for pos, sig_name in zip(signal_positions, signal_names_short):\n",
    "    ax.text(pos, ax.get_ylim()[1] * 0.9, sig_name, ha='center', fontsize=8,\n",
    "            rotation=0, fontweight='bold', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find top 10 most discriminative features (largest absolute difference)\n",
    "differences = np.abs(imu_features_cough - imu_features_throat)\n",
    "top_indices = np.argsort(differences)[-10:][::-1]\n",
    "\n",
    "print(\"\\nüìä Top 10 Most Discriminative IMU Features:\")\n",
    "print(f\"{'Rank':<6} {'Feature Name':<25} {'Cough':>12} {'Throat':>12} {'Diff':>12}\")\n",
    "print(\"-\" * 75)\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    print(f\"{rank:<6} {feature_names[idx]:<25} {imu_features_cough[idx]:>12.4f} {imu_features_throat[idx]:>12.4f} {differences[idx]:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8b91a1",
   "metadata": {},
   "source": [
    "#### üí° Key Insights:\n",
    "\n",
    "- **Gyroscope Line Length dominates** - all top 5 features measure rotational motion intensity\n",
    "- Coughs produce **3x more gyroscope activity** than throat-clearing (head/neck rotation)\n",
    "- Line Length (signal 'wiggliness') is far more discriminative than statistical measures\n",
    "- Gyro_R (Roll) shows the largest difference - captures head tilting during cough\n",
    "- IMU captures biomechanics: coughs = sudden head/neck movement, throat-clearing = more stationary\n",
    "- These 40 features complement audio features for robust multimodal detection!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mz0g5w6snto",
   "metadata": {},
   "source": [
    "### Step 8: Complete Feature Vector - Putting It All Together\n",
    "\n",
    "Now let's extract the **complete 105-feature vector** (65 audio + 40 IMU) for both examples and visualize the final result.\n",
    "\n",
    "This is what gets fed into the machine learning model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g33931zo6nq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract complete feature vectors\n",
    "audio_features_cough = extract_audio_features(cough_audio_window, fs=FS_AUDIO)\n",
    "audio_features_throat = extract_audio_features(throat_audio_window, fs=FS_AUDIO)\n",
    "\n",
    "# Combine audio + IMU\n",
    "complete_features_cough = np.concatenate([audio_features_cough, imu_features_cough])\n",
    "complete_features_throat = np.concatenate([audio_features_throat, imu_features_throat])\n",
    "\n",
    "print(f\"‚úì Complete feature vectors extracted:\")\n",
    "print(f\"  Cough: {len(complete_features_cough)} features\")\n",
    "print(f\"  Throat-clearing: {len(complete_features_throat)} features\")\n",
    "print(f\"\\n  Breakdown: 65 audio + 40 IMU = 105 total\\n\")\n",
    "\n",
    "# Visualize as grouped bar chart\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 14))\n",
    "\n",
    "# Use log scale for better visualization of features with different magnitudes\n",
    "# Apply log(1 + abs(x)) * sign(x) transformation to preserve sign\n",
    "def log_transform(x):\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "\n",
    "cough_log = log_transform(complete_features_cough)\n",
    "throat_log = log_transform(complete_features_throat)\n",
    "\n",
    "# Panel 1: All 105 features\n",
    "x = np.arange(105)\n",
    "width = 0.35\n",
    "axes[0].bar(x - width/2, cough_log, width, label='Cough', color='steelblue', alpha=0.7)\n",
    "axes[0].bar(x + width/2, throat_log, width, label='Throat-Clearing', color='orange', alpha=0.7)\n",
    "axes[0].set_title('Complete 105-Feature Vector (Log-Scaled)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature Index', fontsize=12)\n",
    "axes[0].set_ylabel('Log-Scaled Feature Value', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "axes[0].axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "# Add divider between audio and IMU\n",
    "axes[0].axvline(64.5, color='red', linestyle='--', linewidth=2, label='Audio | IMU')\n",
    "axes[0].text(32, axes[0].get_ylim()[1] * 0.85, 'Audio Features (65)', \n",
    "             ha='center', fontsize=11, fontweight='bold',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "axes[0].text(82, axes[0].get_ylim()[1] * 0.85, 'IMU Features (40)',\n",
    "             ha='center', fontsize=11, fontweight='bold',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "# Panel 2: Audio features only (first 65)\n",
    "x_audio = np.arange(65)\n",
    "axes[1].bar(x_audio - width/2, cough_log[:65], width, label='Cough', color='steelblue', alpha=0.7)\n",
    "axes[1].bar(x_audio + width/2, throat_log[:65], width, label='Throat-Clearing', color='orange', alpha=0.7)\n",
    "axes[1].set_title('Audio Features Only (65 features, Log-Scaled)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Feature Index', fontsize=11)\n",
    "axes[1].set_ylabel('Log-Scaled Feature Value', fontsize=11)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "axes[1].axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "# Add labels for MFCC, Spectral, Time-domain regions\n",
    "axes[1].text(26, axes[1].get_ylim()[1] * 0.85, 'MFCC (52)', ha='center', fontsize=9,\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.4))\n",
    "axes[1].text(57, axes[1].get_ylim()[1] * 0.85, 'Spectral (10)', ha='center', fontsize=9,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.4))\n",
    "axes[1].text(63, axes[1].get_ylim()[1] * 0.85, 'Time (3)', ha='center', fontsize=9,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.4))\n",
    "\n",
    "# Panel 3: IMU features only (last 40)\n",
    "x_imu = np.arange(40)\n",
    "axes[2].bar(x_imu - width/2, cough_log[65:], width, label='Cough', color='steelblue', alpha=0.7)\n",
    "axes[2].bar(x_imu + width/2, throat_log[65:], width, label='Throat-Clearing', color='orange', alpha=0.7)\n",
    "axes[2].set_title('IMU Features Only (40 features, Log-Scaled)', fontsize=13, fontweight='bold')\n",
    "axes[2].set_xlabel('Feature Index', fontsize=11)\n",
    "axes[2].set_ylabel('Log-Scaled Feature Value', fontsize=11)\n",
    "axes[2].legend(fontsize=10)\n",
    "axes[2].grid(alpha=0.3, axis='y')\n",
    "axes[2].axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute overall statistics using raw values\n",
    "differences_all = np.abs(complete_features_cough - complete_features_throat)\n",
    "top_15_indices = np.argsort(differences_all)[-15:][::-1]\n",
    "\n",
    "# Map feature indices to types\n",
    "feature_type_map = {\n",
    "    (0, 52): \"MFCC\",\n",
    "    52: \"Spectral Centroid\",\n",
    "    53: \"Spectral Rolloff\", \n",
    "    54: \"Spectral Bandwidth\",\n",
    "    55: \"Spectral Flatness\",\n",
    "    56: \"Spectral Contrast\",\n",
    "    57: \"Total Power\",\n",
    "    58: \"Dominant Frequency\",\n",
    "    59: \"Spectral Spread\",\n",
    "    60: \"Spectral Skewness\",\n",
    "    61: \"Spectral Kurtosis\",\n",
    "    62: \"Zero-Crossing Rate\",\n",
    "    63: \"RMS Energy\",\n",
    "    64: \"Crest Factor\",\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üìä TOP 15 MOST DISCRIMINATIVE FEATURES (out of 105)\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Rank':<6} {'Type':<10} {'Feature Name':<30} {'Cough':>15} {'Throat':>15}\")\n",
    "print(\"-\" * 90)\n",
    "for rank, idx in enumerate(top_15_indices, 1):\n",
    "    if idx < 65:\n",
    "        feat_type = \"Audio\"\n",
    "        if idx < 52:\n",
    "            feat_name = f\"MFCC #{idx}\"\n",
    "        else:\n",
    "            feat_name = feature_type_map.get(idx, f\"Audio #{idx}\")\n",
    "    else:\n",
    "        feat_type = \"IMU\"\n",
    "        imu_idx = idx - 65\n",
    "        sig_idx = imu_idx // 5\n",
    "        feat_idx = imu_idx % 5\n",
    "        feat_name = f\"{signal_names_short[sig_idx]}_{feature_types[feat_idx]}\"\n",
    "    \n",
    "    print(f\"{rank:<6} {feat_type:<10} {feat_name:<30} {complete_features_cough[idx]:>15.2f} \"\n",
    "          f\"{complete_features_throat[idx]:>15.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da9e320",
   "metadata": {},
   "source": [
    "#### üéØ CRITICAL INSIGHTS FROM THIS EXAMPLE\n",
    "\n",
    "1. **Audio features dominate discrimination** - all top 15 are audio, NO IMU in top 15!\n",
    "\n",
    "2. **Spectral features >> MFCC features**:\n",
    " - Spectral Kurtosis (rank 1): Measures frequency distribution 'peakiness'\n",
    " - Spectral Skewness (rank 2): Measures frequency distribution asymmetry\n",
    " - Spectral Rolloff (rank 3): 6x higher for cough (2547 vs 414 Hz)\n",
    " - Spectral Centroid (rank 4): 3x higher for cough (988 vs 312 Hz)\n",
    "\n",
    "3. **Why spectral features win in THIS example**:\n",
    " - This is a quiet environment (sitting, no background noise)\n",
    " - Coughs have sharp, broadband spectral bursts (high kurtosis/skewness)\n",
    " - Throat-clearing has concentrated low-frequency energy\n",
    " - The spectral difference is dramatic and easily separable\n",
    "\n",
    "4. **Context matters - this is ONE example**:\n",
    " - In noisy environments (music, traffic), IMU features become MORE critical\n",
    " - Multimodal (audio + IMU) provides robustness across ALL conditions\n",
    " - This notebook shows feature extraction; actual model uses ALL 105 features\n",
    "\n",
    "5. **Next step**: Train XGBoost on the full dataset ‚Üí Model_Training_XGBoost.ipynb\n",
    " - You'll see how feature importance changes across 15 subjects and all conditions\n",
    " - The model learns when to rely on audio vs IMU vs both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdtu2frzi1t",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Summary and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this notebook, we transformed raw biosignal data into machine learning-ready features:\n",
    "\n",
    "**Raw Data ‚Üí Features:**\n",
    "- **Input**: 0.4-second windows (6,400 audio samples + 40 IMU samples per window)\n",
    "- **Output**: 105 compact features per window\n",
    "- **Compression**: From ~6,440 values ‚Üí 105 values (61√ó reduction!)\n",
    "\n",
    "### Feature Breakdown\n",
    "\n",
    "| Category | Count | Purpose |\n",
    "|----------|-------|---------|\n",
    "| **MFCC** | 52 | Capture sound texture/timbre |\n",
    "| **Spectral** | 10 | Describe frequency distribution |\n",
    "| **Time-domain** | 3 | Measure loudness and sharpness |\n",
    "| **IMU** | 40 | Capture body motion patterns |\n",
    "| **TOTAL** | **105** | Complete multimodal signature |\n",
    "\n",
    "### Key Insights from Our Examples\n",
    "\n",
    "Through our step-by-step walkthrough, we learned:\n",
    "\n",
    "1. **Coughs have distinct characteristics:**\n",
    "   - Sharp, impulsive audio bursts (high crest factor)\n",
    "   - Strong high-frequency content (high spectral centroid/rolloff)\n",
    "   - Distinctive motion patterns (accelerometer + gyroscope spikes)\n",
    "\n",
    "2. **Multimodal sensing is powerful:**\n",
    "   - Audio alone: Good, but struggles in noisy environments\n",
    "   - IMU alone: Captures motion, but misses acoustic details\n",
    "   - Combined: Robust detection across all conditions\n",
    "\n",
    "3. **Features enable machine learning:**\n",
    "   - Each window becomes a point in 105-dimensional space\n",
    "   - Coughs cluster in one region, non-coughs in another\n",
    "   - ML models learn to find the optimal decision boundary\n",
    "---\n",
    "\n",
    "**You now understand the complete feature extraction pipeline!** üéâ\n",
    "\n",
    "The journey from raw biosignals ‚Üí features ‚Üí trained model is a fundamental workflow in biomedical signal processing and machine learning. The skills you've learned here apply to many other domains: speech recognition, activity recognition, ECG analysis, and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge-ai-cough-count",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
