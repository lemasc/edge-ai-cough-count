{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c98c967-2beb-4c39-81bc-1e41040f3f85",
   "metadata": {},
   "source": [
    "# Cough Detection Feature Extraction\n",
    "\n",
    "This notebook demonstrates how to extract handcrafted features from multimodal biosignals (audio + IMU sensors) for machine learning-based cough detection.\n",
    "\n",
    "## Objective\n",
    "\n",
    "Extract dataset features for three modality configurations:\n",
    "1. **IMU-only**: 40 handcrafted features from accelerometer and gyroscope\n",
    "2. **Audio-only**: 65 features from outer microphone (MFCC + spectral + time-domain)\n",
    "3. **Multimodal**: Combined 105 features (Audio + IMU)\n",
    "\n",
    "These features will be used to train classical ML models (like XGBoost) to distinguish coughs from other sounds and motions.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "- **Window size**: 0.4 seconds (6400 audio samples @ 16kHz, 40 IMU samples @ 100Hz)\n",
    "- **Data augmentation**: Random temporal shifts (aug_factor=2) to increase dataset diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387e71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, signal\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from helpers import *\n",
    "from dataset_gen import *\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ebe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Constants from paper\n",
    "WINDOW_LEN = 0.4  # 0.4 second windows\n",
    "AUG_FACTOR = 2    # Data augmentation factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413s0jutr5o",
   "metadata": {},
   "source": [
    "## Why Feature Extraction?\n",
    "\n",
    "You might wonder: **Why not just feed the raw audio and IMU signals directly into a machine learning model?**\n",
    "\n",
    "Here's why feature extraction is essential:\n",
    "\n",
    "1. **Size reduction**: A 0.4-second window contains 6,400 audio samples. Instead of processing all 6,400 values, we extract just 65 meaningful features that capture the \"essence\" of the sound.\n",
    "\n",
    "2. **Noise reduction**: Raw signals contain a lot of irrelevant information and noise. Features focus on what matters - patterns that distinguish coughs from other sounds.\n",
    "\n",
    "3. **Better learning**: Machine learning models learn faster and more accurately from compact, meaningful features than from thousands of raw signal values.\n",
    "\n",
    "**Analogy**: Imagine describing a person. Instead of showing a full high-resolution photo (raw signal), you describe key features: height, eye color, age, build (extracted features). Both convey information, but features are more compact and focused on what matters.\n",
    "\n",
    "In this notebook, we'll extract **105 features** that capture the unique characteristics of cough sounds and motions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gbh3xz74vaj",
   "metadata": {},
   "source": [
    "## Feature Categories Overview\n",
    "\n",
    "Our 105 features are organized into two main groups:\n",
    "\n",
    "| Category | Count | What It Captures | Example Use Case |\n",
    "|----------|-------|------------------|------------------|\n",
    "| **Audio Features** | **65** | Sound characteristics from microphone | |\n",
    "| ‚Üí MFCC | 52 | Sound \"texture\" or \"timbre\" | Distinguishing cough from speech |\n",
    "| ‚Üí Spectral | 10 | Frequency distribution | High-pitched vs low-pitched sounds |\n",
    "| ‚Üí Time-domain | 3 | Loudness and sharpness | Sudden bursts vs gradual sounds |\n",
    "| **IMU Features** | **40** | Body motion from sensors | |\n",
    "| ‚Üí Accelerometer | 20 | Linear motion (chest/body movement) | Detecting physical cough motion |\n",
    "| ‚Üí Gyroscope | 20 | Rotational motion (head/neck) | Detecting head movement during cough |\n",
    "| **Total** | **105** | Complete multimodal signature | Robust cough detection |\n",
    "\n",
    "### Why Use Both Audio AND Motion Sensors?\n",
    "\n",
    "**Multimodal sensing is more robust than either modality alone:**\n",
    "\n",
    "- **When audio struggles**: Noisy environments (music, traffic, other people talking)\n",
    "  - IMU still captures the physical motion of coughing\n",
    "  \n",
    "- **When IMU struggles**: Sitting still, gentle cough, other body movements\n",
    "  - Audio captures the distinctive cough sound\n",
    "  \n",
    "- **Together**: The combination is much more accurate than either sensor alone\n",
    "\n",
    "In the walkthrough below, you'll see exactly how each feature category works and why it helps detect coughs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2415e9",
   "metadata": {},
   "source": [
    "## Understanding Sampling and Sample Rate\n",
    "\n",
    "Before we can extract features from audio and IMU signals, we need to understand how these **continuous real-world signals** become **discrete digital data** that computers can process.\n",
    "\n",
    "### What is Sampling?\n",
    "\n",
    "**Sampling** is the process of converting a continuous analog signal (like sound waves or motion) into a discrete digital signal by taking measurements at regular intervals.\n",
    "\n",
    "**Analogy**: Imagine you're watching a car drive by:\n",
    "- **Continuous (analog)**: The car moves smoothly through space\n",
    "- **Discrete (digital)**: You take snapshots every 0.1 seconds to track its position\n",
    "\n",
    "The snapshots are your **samples**, and how often you take them is your **sampling rate**.\n",
    "\n",
    "### What is Sample Rate (Sampling Frequency)?\n",
    "\n",
    "**Sample rate** (also called **sampling frequency**) is the number of samples taken per second, measured in **Hertz (Hz)**.\n",
    "\n",
    "**Examples:**\n",
    "- **16,000 Hz** means 16,000 samples per second\n",
    "- **100 Hz** means 100 samples per second\n",
    "\n",
    "**Higher sample rate** = more snapshots per second = more detailed representation of the signal\n",
    "\n",
    "### Our Sensors and Their Sample Rates\n",
    "\n",
    "In this cough detection project, we use two types of sensors with different sample rates:\n",
    "\n",
    "| Sensor | Sample Rate | Samples per Second | What It Captures |\n",
    "|--------|-------------|-------------------|------------------|\n",
    "| **Microphone** | 16,000 Hz | 16,000 | Sound waves (cough, speech, ambient noise) |\n",
    "| **IMU (Accelerometer + Gyroscope)** | 100 Hz | 100 | Body motion (chest movement, head rotation) |\n",
    "\n",
    "**Why different rates?**\n",
    "- **Audio** changes very rapidly (sound frequencies range from 20 Hz to 20,000 Hz), so we need a high sample rate to capture it accurately\n",
    "- **Body motion** changes much more slowly than sound waves, so 100 samples/second is sufficient\n",
    "\n",
    "### From Real World to Digital Samples\n",
    "\n",
    "Here's what happens in practice:\n",
    "\n",
    "**When you cough:**\n",
    "\n",
    "1. **Microphone** measures air pressure 16,000 times per second ‚Üí Creates an array of 16,000 numbers per second\n",
    "2. **Accelerometer** measures chest acceleration 100 times per second ‚Üí Creates an array of 100 numbers per second\n",
    "3. **Gyroscope** measures body rotation 100 times per second ‚Üí Creates an array of 100 numbers per second\n",
    "\n",
    "**A 1-second recording produces:**\n",
    "- Audio: 16,000 values (numbers representing sound amplitude)\n",
    "- IMU: 100 values for each of the 6 channels (3 accelerometer axes + 3 gyroscope axes) = 600 values total\n",
    "\n",
    "These arrays of numbers are what we'll work with for feature extraction!\n",
    "\n",
    "### Why Does This Matter?\n",
    "\n",
    "Understanding sample rate is crucial because:\n",
    "1. It tells us **how much data** we're working with\n",
    "2. It determines **time-to-sample conversion** (e.g., \"0.5 seconds = 8,000 audio samples\")\n",
    "3. It affects **window sizes** for machine learning (explained in the next section)\n",
    "\n",
    "Let's verify our sensor sample rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d3470",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Audio sample rate: {FS_AUDIO} Hz\")\n",
    "print(f\"IMU sample rate: {FS_IMU} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bqlxbwbf6ng",
   "metadata": {},
   "source": [
    "## Understanding Windowing - Fixed-Length Segments\n",
    "\n",
    "### Why Do We Need Windows?\n",
    "\n",
    "Machine learning models require **fixed-size inputs**. But our recordings vary in length:\n",
    "- Some recordings are 10 seconds long\n",
    "- Others are 30 seconds or longer\n",
    "- A single cough might last 0.3 seconds, another might last 0.6 seconds\n",
    "\n",
    "**The solution**: Extract fixed-length **windows** (segments) from the recordings.\n",
    "\n",
    "### What is a Window?\n",
    "\n",
    "A **window** is a fixed-length segment of the signal, like cutting a movie into clips of exactly 10 seconds each.\n",
    "\n",
    "From the original paper:\n",
    "- **Window length**: 0.4 seconds\n",
    "- **Why 0.4 seconds?** This is long enough to capture a complete cough (typically 0.2-0.5s), but short enough to be efficient for ML\n",
    "\n",
    "### Window Length ‚Üî Sample Count Relationship\n",
    "\n",
    "The number of samples in a window depends on the **sampling frequency**:\n",
    "\n",
    "**Formula**: `number_of_samples = window_length_seconds √ó sampling_frequency`\n",
    "\n",
    "**For audio (16,000 Hz):**\n",
    "- 0.4 seconds √ó 16,000 samples/second = **6,400 samples**\n",
    "\n",
    "**For IMU (100 Hz):**\n",
    "- 0.4 seconds √ó 100 samples/second = **40 samples**\n",
    "\n",
    "**Analogy**: If you record video at 30 frames per second, a 2-second clip contains 2 √ó 30 = 60 frames. Same concept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sample_window(\n",
    "        sample: np.ndarray,\n",
    "        frequency: int, \n",
    "        window_duration: float,\n",
    "        start_point_index = None,\n",
    "        start_point_time = None,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Extract features from the given NumPy array, the window duration, and optionally, the starting point.\n",
    "    The starting point can either be an index of the sample, or a the time that sample should start,\n",
    "    which in this case will calculate the sample index automatically.\n",
    "\n",
    "    ## Example:\n",
    "    - Given the sample rate: 16000 Hz\n",
    "    - If window starts at 0.5s ‚Üí 0.5 * 16000 = 8000th sample\n",
    "    - The window length to extract is 0.4s ‚Üí 0.4 √ó 16000 = 6400 samples\n",
    "    - Thus, the sample region to extract is feature[8000: 8000 + 6400]\n",
    "    \"\"\"\n",
    "    window_length = int(window_duration * frequency)\n",
    "    if start_point_index is None:\n",
    "        if start_point_time is not None:\n",
    "            # Calculate the sample start point from the given time\n",
    "            start_point_index = int(start_point_time * frequency)\n",
    "        else:\n",
    "            raise ValueError(\"You must provide either start_point_index or start_point_time.\")\n",
    "    else:\n",
    "        # Clamp to [0, sample_length - window_length]\n",
    "        start_point_index = min(max(0, start_point_index), len(sample) - window_length)\n",
    "    \n",
    "    # Slice the sample using NumPy slicing\n",
    "    return sample[start_point_index: start_point_index + window_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77d112",
   "metadata": {},
   "source": [
    "### Two Types of Windows We'll Extract\n",
    "\n",
    "In this paper, we need to extract both cough and non-cough audios. Thus, we will have two types of windows.\n",
    "\n",
    "1. **Cough windows**\n",
    "   - We know the exact start/end time from `ground_truth.json`, so we can use it as a reference.\n",
    "   - We will extract their windows by centering around annotated cough events\n",
    "   - The window includes the cough, possibly shifted randomly for data augmentation\n",
    "\n",
    "2. **Non-cough windows**:\n",
    "   - There's no ground truth. We didn't cough!\n",
    "   - We will randomly sampled segments from non-cough sounds (laugh, throat-clearing, breathing recordings)\n",
    "   - Used as negative examples (label = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_window(audio, imu, start_point_time=None, start_point_index=None):\n",
    "    \"\"\"Extract audio and IMU window from the given window start point\"\"\"\n",
    "    audio_window = extract_sample_window(audio, FS_AUDIO, WINDOW_LEN, start_point_index, start_point_time)\n",
    "\n",
    "    # For IMU features, we need to stack all 6 IMU channels into a 2D array\n",
    "    # Channels: [Accel_x, Accel_y, Accel_z, Gyro_Y, Gyro_P, Gyro_R]\n",
    "    imu_window = np.column_stack([\n",
    "        extract_sample_window(feature, FS_IMU, WINDOW_LEN, start_point_index, start_point_time)\n",
    "        for feature in [imu.x, imu.y, imu.z, imu.Y, imu.P, imu.R]\n",
    "    ])\n",
    "    return audio_window, imu_window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240eb24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Understanding Features Through Examples\n",
    "\n",
    "We'll take **two real examples** from our dataset and walk through the feature extraction process step-by-step:\n",
    "\n",
    "- **Example A**: A real **cough** from the dataset\n",
    "- **Example B**: A **throat-clearing** sound (non-cough)\n",
    "\n",
    "By comparing these side-by-side, you'll understand:\n",
    "1. What each of the 105 features captures\n",
    "2. Why these features help distinguish coughs from other sounds\n",
    "3. How audio and IMU signals complement each other\n",
    "\n",
    "This hands-on walkthrough will make the abstract concept of \"features\" concrete and intuitive!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b31fcaa",
   "metadata": {},
   "source": [
    "### Step 0: Load dataset\n",
    "\n",
    "First, start by selecting one subject from the dataset as an example, and perform window extraction on that subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ed029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate dataset folder\n",
    "kaggle_dataset_dir = '/kaggle/input/edge-ai-cough-count'\n",
    "base_dir = kaggle_dataset_dir if os.path.exists(kaggle_dataset_dir) else \"..\"\n",
    "data_folder = base_dir + '/public_dataset/'\n",
    "\n",
    "# Check if exists, otherwise try alternative path\n",
    "if not os.path.exists(data_folder):\n",
    "    raise FileNotFoundError(\n",
    "        \"Cannot find public_dataset/. Please download from: \"\n",
    "        \"https://zenodo.org/record/7562332\"\n",
    "    )\n",
    "\n",
    "# Get list of subject IDs\n",
    "subject_ids = [d for d in os.listdir(data_folder) \n",
    "               if os.path.isdir(os.path.join(data_folder, d))]\n",
    "subject_ids = sorted(subject_ids)\n",
    "\n",
    "print(f\"‚úì Found {len(subject_ids)} subjects: {subject_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mug2ht9jwup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Load Example A: Cough Window\n",
    "# ===================================================================\n",
    "# Using subject \"14287\", trial 1, sitting, no noise, cough sound\n",
    "example_subj = \"14287\"\n",
    "\n",
    "# Load full audio signals (both microphones: air-facing and body-facing)\n",
    "cough_audio_air, cough_audio_skin = load_audio(\n",
    "    data_folder, example_subj, Trial.ONE, Movement.SIT, Noise.NONE, Sound.COUGH\n",
    ")\n",
    "\n",
    "# Load full IMU signals (all 6 channels: accel x,y,z + gyro Y,P,R)\n",
    "cough_imu = load_imu(\n",
    "    data_folder, example_subj, Trial.ONE, Movement.SIT, Noise.NONE, Sound.COUGH\n",
    ")\n",
    "\n",
    "# Load ground truth annotations (start/end times of each cough in seconds)\n",
    "cough_starts, cough_ends = load_annotation(\n",
    "    data_folder, example_subj, Trial.ONE, Movement.SIT, Noise.NONE, Sound.COUGH\n",
    ")\n",
    "\n",
    "cough_audio_window, cough_imu_window = extract_subject_window(\n",
    "    audio=cough_audio_air,\n",
    "    imu=cough_imu,\n",
    "    # For window start point, we have ground truth on when the cough happens\n",
    "    # Let's select the time when we first cough\n",
    "    start_point_time=cough_starts[0]\n",
    ")\n",
    "\n",
    "# ===================================================================\n",
    "# Load Example B: Throat-Clearing Window (Non-Cough)\n",
    "# ===================================================================\n",
    "# Load full throat-clearing signals\n",
    "throat_audio_air, throat_audio_skin = load_audio(\n",
    "    data_folder, example_subj, Trial.ONE, Movement.SIT, Noise.NONE, Sound.THROAT\n",
    ")\n",
    "throat_imu = load_imu(\n",
    "    data_folder, example_subj, Trial.ONE, Movement.SIT, Noise.NONE, Sound.THROAT\n",
    ")\n",
    "\n",
    "throat_audio_window, throat_imu_window = extract_subject_window(\n",
    "    audio=throat_audio_air,\n",
    "    imu=throat_imu,\n",
    "    # We don't have a starting point in mind, so pick randomly from the sample length\n",
    "    start_point_index=np.random.randint(0, len(throat_audio_air))\n",
    ")\n",
    "\n",
    "print(\"‚úì Loaded example windows:\")\n",
    "print(f\"  Cough: {len(cough_audio_window)} audio samples, {len(cough_imu_window)} IMU samples\")\n",
    "print(f\"  Throat-clearing: {len(throat_audio_window)} audio samples, {len(throat_imu_window)} IMU samples\")\n",
    "print(f\"\\n  Expected: 6400 audio samples (0.4s √ó 16kHz), 40 IMU samples (0.4s √ó 100Hz)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb8ff7",
   "metadata": {},
   "source": [
    "### Step 1: Select and Visualize Example Windows\n",
    "\n",
    "Let's look at the raw signals. Notice the differences in waveform patterns between cough and throat-clearing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4j7tku4bxe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize raw signals side-by-side\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# Cough audio\n",
    "time_audio = np.arange(len(cough_audio_window)) / FS_AUDIO\n",
    "axes[0, 0].plot(time_audio, cough_audio_window, color='steelblue', linewidth=0.8)\n",
    "axes[0, 0].set_title('Example A: Cough - Audio Signal', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Amplitude', fontsize=11)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "# Add annotation for sharp burst\n",
    "peak_idx = np.argmax(np.abs(cough_audio_window))\n",
    "peak_val = cough_audio_window[peak_idx]\n",
    "y_range = axes[0, 0].get_ylim()[1] - axes[0, 0].get_ylim()[0]\n",
    "text_offset = y_range * 0.20 if peak_val > 0 else -y_range * 0.20\n",
    "axes[0, 0].annotate('Sharp burst ‚Üí', \n",
    "                    xy=(peak_idx/FS_AUDIO, peak_val),\n",
    "                    xytext=(peak_idx/FS_AUDIO - 0.08, peak_val + text_offset),\n",
    "                    arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                    fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "# Cough IMU (accelerometer Z)\n",
    "time_imu = np.arange(len(cough_imu_window)) / FS_IMU\n",
    "axes[0, 1].plot(time_imu, -cough_imu_window[:, 2], color='darkgreen', linewidth=1.5)\n",
    "axes[0, 1].set_title('Example A: Cough - IMU Accel Z', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Acceleration', fontsize=11)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "# Add annotation for IMU spike\n",
    "imu_peak_idx = np.argmax(-cough_imu_window[:, 2])\n",
    "imu_peak_val = -cough_imu_window[imu_peak_idx, 2]\n",
    "y_range_imu = axes[0, 1].get_ylim()[1] - axes[0, 1].get_ylim()[0]\n",
    "text_offset_imu = y_range_imu * 0.20 if imu_peak_val > 0 else -y_range_imu * 0.20\n",
    "axes[0, 1].annotate('‚Üê IMU spike', \n",
    "                    xy=(imu_peak_idx/FS_IMU, imu_peak_val),\n",
    "                    xytext=(imu_peak_idx/FS_IMU + 0.05, imu_peak_val + text_offset_imu),\n",
    "                    arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                    fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "# Throat-clearing audio\n",
    "axes[1, 0].plot(time_audio, throat_audio_window, color='orange', linewidth=0.8)\n",
    "axes[1, 0].set_title('Example B: Throat-Clearing - Audio Signal', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Amplitude', fontsize=11)\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Throat-clearing IMU\n",
    "axes[1, 1].plot(time_imu, -throat_imu_window[:, 2], color='purple', linewidth=1.5)\n",
    "axes[1, 1].set_title('Example B: Throat-Clearing - IMU Accel Z', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Acceleration', fontsize=11)\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"  ‚Ä¢ Cough audio: Sharp, sudden burst with high amplitude\")\n",
    "print(\"  ‚Ä¢ Cough IMU: Clear spike corresponding to chest/body movement\")\n",
    "print(\"  ‚Ä¢ Throat-clearing: More gradual, sustained sound pattern\")\n",
    "print(\"  ‚Ä¢ Features will capture these differences quantitatively!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v34vph3l48d",
   "metadata": {},
   "source": [
    "### Step 2: Understanding Audio - The Spectrogram\n",
    "\n",
    "Audio signals have both **time** and **frequency** information. A **spectrogram** shows how frequencies change over time - like a musical score that displays which notes (frequencies) are played when.\n",
    "\n",
    "**Think of it like this:**\n",
    "- **Low frequencies** (bottom of spectrogram): Deep, bass sounds\n",
    "- **High frequencies** (top of spectrogram): Sharp, treble sounds like \"s\" or \"sh\"\n",
    "- **Color intensity**: How loud that frequency is at that moment\n",
    "\n",
    "Let's visualize the spectrograms for our cough vs throat-clearing examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3yabj7fw14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spectrograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cough spectrogram\n",
    "D_cough = librosa.amplitude_to_db(np.abs(librosa.stft(cough_audio_window)), ref=np.max)\n",
    "img1 = librosa.display.specshow(D_cough, sr=FS_AUDIO, x_axis='time', y_axis='hz', \n",
    "                                 ax=axes[0], cmap='viridis')\n",
    "axes[0].set_title('Example A: Cough - Spectrogram', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency (Hz)', fontsize=11)\n",
    "axes[0].set_ylim(0, 8000)\n",
    "fig.colorbar(img1, ax=axes[0], format='%+2.0f dB')\n",
    "\n",
    "# Throat-clearing spectrogram\n",
    "D_throat = librosa.amplitude_to_db(np.abs(librosa.stft(throat_audio_window)), ref=np.max)\n",
    "img2 = librosa.display.specshow(D_throat, sr=FS_AUDIO, x_axis='time', y_axis='hz',\n",
    "                                 ax=axes[1], cmap='viridis')\n",
    "axes[1].set_title('Example B: Throat-Clearing - Spectrogram', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Time (seconds)', fontsize=11)\n",
    "axes[1].set_ylabel('Frequency (Hz)', fontsize=11)\n",
    "axes[1].set_ylim(0, 8000)\n",
    "fig.colorbar(img2, ax=axes[1], format='%+2.0f dB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä What we observe:\")\n",
    "print(\"  ‚Ä¢ Cough: Sharp vertical bands (sudden burst) with strong high-frequency content (bright yellow at top)\")\n",
    "print(\"  ‚Ä¢ Throat-clearing: More horizontal spread (sustained) with energy concentrated in lower frequencies\")\n",
    "print(\"  ‚Ä¢ Coughs are like cymbal crashes - lots of high frequencies\")\n",
    "print(\"  ‚Ä¢ Throat-clearing is more like clearing vocal cords - lower, more gradual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rj5c2xgenie",
   "metadata": {},
   "source": [
    "### Step 3: MFCC Features - Capturing Audio \"Texture\"\n",
    "\n",
    "**MFCC** stands for **Mel-Frequency Cepstral Coefficients** - a fancy name for features that capture the \"shape\" or \"texture\" of sound.\n",
    "\n",
    "**Beginner-friendly explanation:**\n",
    "- MFCCs are inspired by how human ears perceive sound\n",
    "- The \"Mel\" scale mimics how we hear: we're better at distinguishing low frequencies than high ones\n",
    "- We extract **13 MFCC values** at each time point, then compute statistics (mean, std, min, max) across the window\n",
    "- This gives us **13 √ó 4 = 52 MFCC features** per audio window\n",
    "\n",
    "**Why MFCCs matter:**\n",
    "- MFCC coefficient 1-3 capture the overall spectral shape (coughs are sharper and more percussive)\n",
    "- Higher coefficients capture fine details of the sound texture\n",
    "- They're widely used in speech recognition and audio classification\n",
    "\n",
    "Let's define a function to extract these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2800ktre8p7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_mfcc_features(audio_window: np.ndarray, fs=16000):\n",
    "    \"\"\"\n",
    "    Extract 52 MFCC features from audio window\n",
    "    \n",
    "    Args:\n",
    "        audio_window: 1D array of audio samples\n",
    "        fs: Sampling frequency (default: 16000 Hz)\n",
    "    \n",
    "    Returns:\n",
    "        list: 52 MFCC features (13 coefficients √ó 4 statistics)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Compute MFCCs (13 coefficients over time frames)\n",
    "    mfccs = librosa.feature.mfcc(y=audio_window, sr=fs, n_mfcc=13)\n",
    "    \n",
    "    # For each coefficient, compute mean, std, min, max\n",
    "    for coef in mfccs:\n",
    "        features.extend([\n",
    "            np.mean(coef),  # Mean value over time\n",
    "            np.std(coef),   # Standard deviation\n",
    "            np.min(coef),   # Minimum value\n",
    "            np.max(coef)    # Maximum value\n",
    "        ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Test the function\n",
    "test_mfcc = extract_audio_mfcc_features(np.random.randn(6400))\n",
    "print(f\"‚úì MFCC feature extractor: {len(test_mfcc)} features\")\n",
    "assert len(test_mfcc) == 52, f\"Expected 52 features, got {len(test_mfcc)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fpd5brb6hnh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MFCCs for both examples\n",
    "mfcc_cough = librosa.feature.mfcc(y=cough_audio_window, sr=FS_AUDIO, n_mfcc=13)\n",
    "mfcc_throat = librosa.feature.mfcc(y=throat_audio_window, sr=FS_AUDIO, n_mfcc=13)\n",
    "\n",
    "# Compute statistics for comparison\n",
    "mfcc_cough_mean = np.mean(mfcc_cough, axis=1)\n",
    "mfcc_throat_mean = np.mean(mfcc_throat, axis=1)\n",
    "mfcc_cough_std = np.std(mfcc_cough, axis=1)\n",
    "mfcc_throat_std = np.std(mfcc_throat, axis=1)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# MFCC heatmap for cough\n",
    "img1 = axes[0, 0].imshow(mfcc_cough, aspect='auto', origin='lower', cmap='coolwarm')\n",
    "axes[0, 0].set_title('Example A: Cough - MFCC Heatmap', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time Frame', fontsize=11)\n",
    "axes[0, 0].set_ylabel('MFCC Coefficient', fontsize=11)\n",
    "axes[0, 0].set_yticks(range(13))\n",
    "fig.colorbar(img1, ax=axes[0, 0])\n",
    "\n",
    "# MFCC heatmap for throat-clearing\n",
    "img2 = axes[0, 1].imshow(mfcc_throat, aspect='auto', origin='lower', cmap='coolwarm')\n",
    "axes[0, 1].set_title('Example B: Throat-Clearing - MFCC Heatmap', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Time Frame', fontsize=11)\n",
    "axes[0, 1].set_ylabel('MFCC Coefficient', fontsize=11)\n",
    "axes[0, 1].set_yticks(range(13))\n",
    "fig.colorbar(img2, ax=axes[0, 1])\n",
    "\n",
    "# Bar chart of MFCC means\n",
    "x = np.arange(13)\n",
    "width = 0.35\n",
    "axes[1, 0].bar(x - width/2, mfcc_cough_mean, width, label='Cough', color='steelblue', alpha=0.8)\n",
    "axes[1, 0].bar(x + width/2, mfcc_throat_mean, width, label='Throat-clearing', color='orange', alpha=0.8)\n",
    "axes[1, 0].set_title('MFCC Mean Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('MFCC Coefficient', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Mean Value', fontsize=11)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Bar chart of MFCC standard deviations\n",
    "axes[1, 1].bar(x - width/2, mfcc_cough_std, width, label='Cough', color='steelblue', alpha=0.8)\n",
    "axes[1, 1].bar(x + width/2, mfcc_throat_std, width, label='Throat-clearing', color='orange', alpha=0.8)\n",
    "axes[1, 1].set_title('MFCC Standard Deviation Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('MFCC Coefficient', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Std Value', fontsize=11)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key insights:\")\n",
    "print(\"  ‚Ä¢ Notice how coefficients 0-3 differ significantly between cough and throat-clearing\")\n",
    "print(\"  ‚Ä¢ These capture the overall spectral shape - coughs are sharper and more percussive\")\n",
    "print(\"  ‚Ä¢ The standard deviation (variability over time) also differs\")\n",
    "print(\"  ‚Ä¢ Together, these 52 features (13 coefficients √ó 4 statistics) provide a powerful signature!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ia8z661l",
   "metadata": {},
   "source": [
    "### Step 4: Spectral Features - Frequency Characteristics\n",
    "\n",
    "Spectral features describe **WHERE the energy is concentrated** in the frequency spectrum. Think of them as describing the \"color\" of sound.\n",
    "\n",
    "**Beginner-friendly definitions:**\n",
    "- **Spectral Centroid**: The \"center of mass\" of frequencies - like finding the average frequency. A high centroid means more high-frequency content (like \"s\" sounds).\n",
    "- **Spectral Rolloff**: The frequency below which 85% of the energy is concentrated. Higher rolloff = more high-frequency energy.\n",
    "- **Spectral Bandwidth**: The \"spread\" of frequencies - how wide the frequency range is.\n",
    "- **Spectral Flatness**: How \"noisy\" vs \"tonal\" the sound is (white noise = 1, pure tone = 0).\n",
    "\n",
    "Let's visualize these for our examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6effee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_spectral_features(audio_window: np.ndarray, fs=16000):\n",
    "    \"\"\"\n",
    "    Extract 10 spectral features from audio window\n",
    "    \n",
    "    Args:\n",
    "        audio_window: 1D array of audio samples\n",
    "        fs: Sampling frequency (default: 16000 Hz)\n",
    "    \n",
    "    Returns:\n",
    "        list: 10 spectral features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Librosa-based spectral features (5)\n",
    "    features.append(np.mean(librosa.feature.spectral_centroid(y=audio_window, sr=fs)))\n",
    "    features.append(np.mean(librosa.feature.spectral_rolloff(y=audio_window, sr=fs)))\n",
    "    features.append(np.mean(librosa.feature.spectral_bandwidth(y=audio_window, sr=fs)))\n",
    "    features.append(np.mean(librosa.feature.spectral_flatness(y=audio_window)))\n",
    "    features.append(np.mean(librosa.feature.spectral_contrast(y=audio_window, sr=fs)))\n",
    "    \n",
    "    # PSD-based features (5)\n",
    "    f, psd = signal.welch(audio_window, fs=fs)\n",
    "    features.append(np.sum(psd))  # Total power\n",
    "    dom_freq_idx = np.argmax(psd)\n",
    "    features.append(f[dom_freq_idx])  # Dominant frequency\n",
    "    \n",
    "    # Spectral moments (spread, skewness, kurtosis)\n",
    "    psd_norm = psd / (np.sum(psd) + 1e-10)\n",
    "    spectral_mean = np.sum(f * psd_norm)\n",
    "    features.append(np.sqrt(np.sum(((f - spectral_mean)**2) * psd_norm)))  # Spread\n",
    "    features.append(np.sum(((f - spectral_mean)**3) * psd_norm))  # Skewness\n",
    "    features.append(np.sum(((f - spectral_mean)**4) * psd_norm))  # Kurtosis\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Test the function\n",
    "test_spectral = extract_audio_spectral_features(np.random.randn(6400))\n",
    "print(f\"‚úì Spectral feature extractor: {len(test_spectral)} features\")\n",
    "assert len(test_spectral) == 10, f\"Expected 10 features, got {len(test_spectral)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egxz8e0o0ct",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectral features\n",
    "\n",
    "# Compute PSD (Power Spectral Density)\n",
    "freqs_cough, psd_cough = signal.welch(cough_audio_window, fs=FS_AUDIO, nperseg=512)\n",
    "freqs_throat, psd_throat = signal.welch(throat_audio_window, fs=FS_AUDIO, nperseg=512)\n",
    "\n",
    "centroid_cough, rolloff_cough, bandwidth_cough, *_ = extract_audio_spectral_features(cough_audio_window)\n",
    "centroid_throat, rolloff_throat, bandwidth_throat, *_ = extract_audio_spectral_features(throat_audio_window)\n",
    "\n",
    "# Visualize PSD with spectral features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cough PSD\n",
    "axes[0].plot(freqs_cough, psd_cough, color='steelblue', linewidth=2, label='PSD')\n",
    "axes[0].axvline(centroid_cough, color='red', linestyle='--', linewidth=2, label=f'Centroid: {centroid_cough:.0f} Hz')\n",
    "axes[0].axvline(rolloff_cough, color='blue', linestyle='--', linewidth=2, label=f'Rolloff: {rolloff_cough:.0f} Hz')\n",
    "axes[0].axvspan(centroid_cough - bandwidth_cough/2, centroid_cough + bandwidth_cough/2, \n",
    "                alpha=0.2, color='green', label=f'Bandwidth: {bandwidth_cough:.0f} Hz')\n",
    "axes[0].set_title('Example A: Cough - Spectral Features', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Frequency (Hz)', fontsize=11)\n",
    "axes[0].set_ylabel('Power Spectral Density', fontsize=11)\n",
    "axes[0].set_xlim(0, 8000)\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Throat-clearing PSD\n",
    "axes[1].plot(freqs_throat, psd_throat, color='orange', linewidth=2, label='PSD')\n",
    "axes[1].axvline(centroid_throat, color='red', linestyle='--', linewidth=2, label=f'Centroid: {centroid_throat:.0f} Hz')\n",
    "axes[1].axvline(rolloff_throat, color='blue', linestyle='--', linewidth=2, label=f'Rolloff: {rolloff_throat:.0f} Hz')\n",
    "axes[1].axvspan(centroid_throat - bandwidth_throat/2, centroid_throat + bandwidth_throat/2,\n",
    "                alpha=0.2, color='green', label=f'Bandwidth: {bandwidth_throat:.0f} Hz')\n",
    "axes[1].set_title('Example B: Throat-Clearing - Spectral Features', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Frequency (Hz)', fontsize=11)\n",
    "axes[1].set_ylabel('Power Spectral Density', fontsize=11)\n",
    "axes[1].set_xlim(0, 8000)\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create comparison table\n",
    "print(\"\\nüìä Spectral Feature Comparison:\")\n",
    "print(f\"{'Feature':<20} {'Cough':>15} {'Throat-Clearing':>20}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Spectral Centroid':<20} {centroid_cough:>12.1f} Hz {centroid_throat:>17.1f} Hz\")\n",
    "print(f\"{'Spectral Rolloff':<20} {rolloff_cough:>12.1f} Hz {rolloff_throat:>17.1f} Hz\")\n",
    "print(f\"{'Spectral Bandwidth':<20} {bandwidth_cough:>12.1f} Hz {bandwidth_throat:>17.1f} Hz\")\n",
    "print(\"\\nüí° Insight:\")\n",
    "print(\"  ‚Ä¢ Coughs typically have HIGHER spectral centroid and rolloff (more high-frequency 'hiss')\")\n",
    "print(\"  ‚Ä¢ Throat-clearing has more energy in LOWER frequencies (vocal cord vibrations)\")\n",
    "print(\"  ‚Ä¢ These differences help the ML model distinguish coughs from other sounds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7py9fm3ubf",
   "metadata": {},
   "source": [
    "### Step 5: Time-Domain Features - Temporal Characteristics\n",
    "\n",
    "Time-domain features describe properties of the waveform **over time** (rather than in frequency).\n",
    "\n",
    "**Beginner-friendly definitions:**\n",
    "- **Zero-Crossing Rate (ZCR)**: How often the signal crosses zero (related to frequency). High ZCR = lots of oscillations.\n",
    "- **RMS Energy**: Root Mean Square energy - a measure of \"loudness\" or overall power.\n",
    "- **Crest Factor**: Ratio of peak amplitude to RMS. Measures how \"peaky\" or \"spiky\" the signal is.\n",
    "  - High crest factor = sharp transients (like a cough)\n",
    "  - Low crest factor = smooth, sustained sound (like a vowel)\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è Important Note on DC Offset and Preprocessing:**\n",
    "\n",
    "According to the research paper methodology:\n",
    "- **IMU signals**: DC offset (mean) is explicitly removed from each 0.4s window before feature extraction\n",
    "- **Audio signals**: No DC offset removal is mentioned - features are computed on raw audio\n",
    "\n",
    "**For the visualization cell below ONLY**, we remove DC offset from audio to improve visual clarity when comparing peak/RMS levels. This makes it easier to see the AC component (the actual oscillations we care about).\n",
    "\n",
    "**The actual feature extraction functions compute features on raw audio signals** (with DC offset intact), staying faithful to the paper. The visualizations below are for educational purposes only.\n",
    "\n",
    "Let's visualize these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed78f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_time_features(audio_window: np.ndarray):\n",
    "    \"\"\"\n",
    "    Extract 3 time-domain features from audio window\n",
    "    \n",
    "    Args:\n",
    "        audio_window: 1D array of audio samples\n",
    "    \n",
    "    Returns:\n",
    "        list: 3 time-domain features [zcr, rms, crest_factor]\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Zero-crossing rate\n",
    "    features.append(librosa.feature.zero_crossing_rate(audio_window)[0].mean())\n",
    "    \n",
    "    # RMS energy\n",
    "    rms = np.sqrt(np.mean(audio_window**2))\n",
    "    features.append(rms)\n",
    "    \n",
    "    # Crest factor (peak-to-RMS ratio)\n",
    "    features.append(np.max(np.abs(audio_window)) / (rms + 1e-10))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Test the function\n",
    "test_time = extract_audio_time_features(np.random.randn(6400))\n",
    "print(f\"‚úì Time-domain feature extractor: {len(test_time)} features\")\n",
    "assert len(test_time) == 3, f\"Expected 3 features, got {len(test_time)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toh75tdx49h",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# Extract ACTUAL features (used by ML model) - computed on RAW signals\n",
    "# ===================================================================\n",
    "zcr_cough, rms_cough, crest_cough = extract_audio_time_features(cough_audio_window)\n",
    "zcr_throat, rms_throat, crest_throat = extract_audio_time_features(throat_audio_window)\n",
    "\n",
    "# ===================================================================\n",
    "# For VISUALIZATION ONLY: Remove DC offset for clarity\n",
    "# ===================================================================\n",
    "cough_centered = cough_audio_window - np.mean(cough_audio_window)\n",
    "throat_centered = throat_audio_window - np.mean(throat_audio_window)\n",
    "\n",
    "# Compute visualization-only metrics on DC-centered signals\n",
    "zcr_cough_vis = np.sum(np.diff(np.sign(cough_centered)) != 0) / len(cough_centered)\n",
    "zcr_throat_vis = np.sum(np.diff(np.sign(throat_centered)) != 0) / len(throat_centered)\n",
    "\n",
    "rms_cough_vis = np.sqrt(np.mean(cough_centered**2))\n",
    "rms_throat_vis = np.sqrt(np.mean(throat_centered**2))\n",
    "\n",
    "peak_cough_vis = np.max(np.abs(cough_centered))\n",
    "peak_throat_vis = np.max(np.abs(throat_centered))\n",
    "\n",
    "crest_cough_vis = peak_cough_vis / (rms_cough_vis + 1e-10)\n",
    "crest_throat_vis = peak_throat_vis / (rms_throat_vis + 1e-10)\n",
    "\n",
    "# ===================================================================\n",
    "# Visualize\n",
    "# ===================================================================\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "\n",
    "# Row 1: Waveforms with zero crossings (DC-centered)\n",
    "time_audio = np.arange(len(cough_audio_window)) / FS_AUDIO\n",
    "\n",
    "axes[0, 0].plot(time_audio, cough_centered, color='steelblue', linewidth=0.8, label='Waveform (DC removed)')\n",
    "axes[0, 0].axhline(0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "zero_crossings_cough = np.where(np.diff(np.sign(cough_centered)))[0]\n",
    "axes[0, 0].scatter(zero_crossings_cough[:100]/FS_AUDIO, \n",
    "                   np.zeros(min(100, len(zero_crossings_cough))), \n",
    "                   color='red', s=10, alpha=0.5, label='Zero crossings (first 100)')\n",
    "axes[0, 0].set_title(f'Example A: Cough - Zero Crossings (ZCR={zcr_cough_vis:.4f})', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time (seconds)', fontsize=10)\n",
    "axes[0, 0].set_ylabel('Amplitude (DC removed)', fontsize=10)\n",
    "axes[0, 0].legend(fontsize=9)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(time_audio, throat_centered, color='orange', linewidth=0.8, label='Waveform (DC removed)')\n",
    "axes[0, 1].axhline(0, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "zero_crossings_throat = np.where(np.diff(np.sign(throat_centered)))[0]\n",
    "axes[0, 1].scatter(zero_crossings_throat[:100]/FS_AUDIO,\n",
    "                   np.zeros(min(100, len(zero_crossings_throat))),\n",
    "                   color='red', s=10, alpha=0.5, label='Zero crossings (first 100)')\n",
    "axes[0, 1].set_title(f'Example B: Throat-Clearing - Zero Crossings (ZCR={zcr_throat_vis:.4f})',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Time (seconds)', fontsize=10)\n",
    "axes[0, 1].set_ylabel('Amplitude (DC removed)', fontsize=10)\n",
    "axes[0, 1].legend(fontsize=9)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Row 2: RMS Energy comparison (visualization values)\n",
    "axes[1, 0].bar(['Cough', 'Throat-Clearing'], [rms_cough_vis, rms_throat_vis], \n",
    "               color=['steelblue', 'orange'], alpha=0.7)\n",
    "axes[1, 0].set_title('RMS Energy Comparison (DC removed)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('RMS Energy', fontsize=10)\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "for i, (label, val) in enumerate([('Cough', rms_cough_vis), ('Throat', rms_throat_vis)]):\n",
    "    axes[1, 0].text(i, val, f'{val:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Row 2: Crest Factor comparison (visualization values)\n",
    "axes[1, 1].bar(['Cough', 'Throat-Clearing'], [crest_cough_vis, crest_throat_vis],\n",
    "               color=['steelblue', 'orange'], alpha=0.7)\n",
    "axes[1, 1].set_title('Crest Factor Comparison (DC removed)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Crest Factor (Peak/RMS)', fontsize=10)\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "for i, (label, val) in enumerate([('Cough', crest_cough_vis), ('Throat', crest_throat_vis)]):\n",
    "    axes[1, 1].text(i, val, f'{val:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Row 3: Waveforms with peak and RMS lines (DC-centered)\n",
    "axes[2, 0].plot(time_audio, cough_centered, color='steelblue', linewidth=0.8, label='Waveform (DC removed)')\n",
    "axes[2, 0].axhline(rms_cough_vis, color='green', linestyle='--', linewidth=2, label=f'RMS: {rms_cough_vis:.4f}')\n",
    "axes[2, 0].axhline(-rms_cough_vis, color='green', linestyle='--', linewidth=2)\n",
    "axes[2, 0].axhline(peak_cough_vis, color='red', linestyle='--', linewidth=2, label=f'Peak: {peak_cough_vis:.4f}')\n",
    "axes[2, 0].axhline(-peak_cough_vis, color='red', linestyle='--', linewidth=2)\n",
    "axes[2, 0].set_title(f'Example A: Cough - Peak vs RMS (Crest={crest_cough_vis:.2f})',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[2, 0].set_xlabel('Time (seconds)', fontsize=10)\n",
    "axes[2, 0].set_ylabel('Amplitude (DC removed)', fontsize=10)\n",
    "axes[2, 0].legend(fontsize=9)\n",
    "axes[2, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[2, 1].plot(time_audio, throat_centered, color='orange', linewidth=0.8, label='Waveform (DC removed)')\n",
    "axes[2, 1].axhline(rms_throat_vis, color='green', linestyle='--', linewidth=2, label=f'RMS: {rms_throat_vis:.4f}')\n",
    "axes[2, 1].axhline(-rms_throat_vis, color='green', linestyle='--', linewidth=2)\n",
    "axes[2, 1].axhline(peak_throat_vis, color='red', linestyle='--', linewidth=2, label=f'Peak: {peak_throat_vis:.4f}')\n",
    "axes[2, 1].axhline(-peak_throat_vis, color='red', linestyle='--', linewidth=2)\n",
    "axes[2, 1].set_title(f'Example B: Throat-Clearing - Peak vs RMS (Crest={crest_throat_vis:.2f})',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[2, 1].set_xlabel('Time (seconds)', fontsize=10)\n",
    "axes[2, 1].set_ylabel('Amplitude (DC removed)', fontsize=10)\n",
    "axes[2, 1].legend(fontsize=9)\n",
    "axes[2, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# Print comparison table using ACTUAL features (raw signal)\n",
    "# ===================================================================\n",
    "print(\"\\nüìä Time-Domain Feature Comparison (ACTUAL features used by ML model):\")\n",
    "print(f\"{'Feature':<20} {'Cough':>15} {'Throat-Clearing':>20}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Zero-Crossing Rate':<20} {zcr_cough:>15.4f} {zcr_throat:>20.4f}\")\n",
    "print(f\"{'RMS Energy':<20} {rms_cough:>15.4f} {rms_throat:>20.4f}\")\n",
    "print(f\"{'Crest Factor':<20} {crest_cough:>15.2f} {crest_throat:>20.2f}\")\n",
    "print(f\"{'DC Offset (mean)':<20} {np.mean(cough_audio_window):>15.4f} {np.mean(throat_audio_window):>20.4f}\")\n",
    "\n",
    "print(\"\\nüìä Visualization-Only Metrics (DC-centered for clarity):\")\n",
    "print(f\"{'Feature':<20} {'Cough':>15} {'Throat-Clearing':>20}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Zero-Crossing Rate':<20} {zcr_cough_vis:>15.4f} {zcr_throat_vis:>20.4f}\")\n",
    "print(f\"{'RMS Energy':<20} {rms_cough_vis:>15.4f} {rms_throat_vis:>20.4f}\")\n",
    "print(f\"{'Crest Factor':<20} {crest_cough_vis:>15.2f} {crest_throat_vis:>20.2f}\")\n",
    "\n",
    "print(\"\\nüí° Insight:\")\n",
    "print(\"  ‚Ä¢ Coughs have HIGH crest factor (sharp peaks) - characteristic of impulsive sounds\")\n",
    "print(\"  ‚Ä¢ Throat-clearing has LOWER crest factor - more gradual and sustained\")\n",
    "print(\"  ‚Ä¢ Higher ZCR in coughs reflects the noisy, broadband nature of the sound\")\n",
    "print(\"\\n‚ö†Ô∏è Note:\")\n",
    "print(\"  ‚Ä¢ ACTUAL ML features are computed on raw audio (with DC offset)\")\n",
    "print(\"  ‚Ä¢ Visualizations use DC-centered audio for clarity only\")\n",
    "print(\"  ‚Ä¢ This follows the paper methodology: no DC removal for audio, but DC removal for IMU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc6303",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that we've defined all functions for extracting features from audio, let's combine them into a single extraction function for ease of use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ea15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_window, fs=16000):\n",
    "    \"\"\"\n",
    "    Extract all 65 audio features from a single window\n",
    "    \n",
    "    Combines:\n",
    "    - 52 MFCC features (13 coefficients √ó 4 statistics)\n",
    "    - 10 spectral features\n",
    "    - 3 time-domain features\n",
    "    \n",
    "    Args:\n",
    "        audio_window: 1D array of audio samples\n",
    "        fs: Sampling frequency (16000 Hz)\n",
    "    \n",
    "    Returns:\n",
    "        np.array: 65 audio features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # MFCC features (52)\n",
    "    features.extend(extract_audio_mfcc_features(audio_window, fs))\n",
    "    \n",
    "    # Spectral features (10)\n",
    "    features.extend(extract_audio_spectral_features(audio_window, fs))\n",
    "    \n",
    "    # Time-domain features (3)\n",
    "    features.extend(extract_audio_time_features(audio_window))\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Test the complete function\n",
    "test_audio_complete = np.random.randn(6400)\n",
    "test_features_complete = extract_audio_features(test_audio_complete)\n",
    "print(f\"‚úì Complete audio feature extractor: {len(test_features_complete)} features\")\n",
    "print(f\"  Breakdown: 52 (MFCC) + 10 (spectral) + 3 (time) = 65 total\")\n",
    "assert len(test_features_complete) == 65, f\"Expected 65 features, got {len(test_features_complete)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ov15ujw525",
   "metadata": {},
   "source": [
    "### Step 6: IMU Features - Motion Signals\n",
    "\n",
    "Now let's look at the **Inertial Measurement Unit (IMU)** - the motion sensors.\n",
    "\n",
    "**Beginner-friendly explanation:**\n",
    "- **IMU**: A sensor that measures motion using two components:\n",
    "  - **Accelerometer**: Measures linear acceleration (like feeling pushed in a car). Captures chest/body movement during cough.\n",
    "  - **Gyroscope**: Measures rotation (like turning your head). Captures head/neck movement during cough.\n",
    "\n",
    "- **Each sensor has 3 axes**: X, Y, Z (measuring motion in 3D space)\n",
    "- **L2 norm**: The total magnitude of motion, calculated as ‚àö(x¬≤ + y¬≤ + z¬≤)\n",
    "  - Think of it like calculating distance: if you walk 3 meters north and 4 meters east, you're ‚àö(3¬≤ + 4¬≤) = 5 meters from your starting point\n",
    "\n",
    "**Total**: 3 accel + 1 accel_L2 + 3 gyro + 1 gyro_L2 = **8 signals** √ó 5 features each = **40 IMU features**\n",
    "\n",
    "Let's visualize all 8 signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6k1w5msuon5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L2 norms\n",
    "# First, center the signals (subtract mean) as per the paper\n",
    "cough_imu_centered = cough_imu_window - np.mean(cough_imu_window, axis=0, keepdims=True)\n",
    "throat_imu_centered = throat_imu_window - np.mean(throat_imu_window, axis=0, keepdims=True)\n",
    "\n",
    "# Compute L2 norms\n",
    "cough_accel_l2 = np.linalg.norm(cough_imu_centered[:, 0:3], axis=1)\n",
    "cough_gyro_l2 = np.linalg.norm(cough_imu_centered[:, 3:6], axis=1)\n",
    "throat_accel_l2 = np.linalg.norm(throat_imu_centered[:, 0:3], axis=1)\n",
    "throat_gyro_l2 = np.linalg.norm(throat_imu_centered[:, 3:6], axis=1)\n",
    "\n",
    "# Visualize all 8 signals for cough\n",
    "fig, axes = plt.subplots(4, 2, figsize=(14, 12))\n",
    "time_imu = np.arange(40) / FS_IMU\n",
    "signal_names = ['Accel X', 'Accel Y', 'Accel Z', 'Accel L2', 'Gyro Y', 'Gyro P', 'Gyro R', 'Gyro L2']\n",
    "colors = ['steelblue', 'darkgreen', 'purple', 'red', 'orange', 'brown', 'pink', 'darkred']\n",
    "\n",
    "# Cough signals\n",
    "for idx, (name, color) in enumerate(zip(signal_names, colors)):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    if idx < 3:\n",
    "        sig = cough_imu_centered[:, idx]\n",
    "    elif idx == 3:\n",
    "        sig = cough_accel_l2\n",
    "    elif idx < 7:\n",
    "        sig = cough_imu_centered[:, idx - 1]\n",
    "    else:\n",
    "        sig = cough_gyro_l2\n",
    "    \n",
    "    axes[row, col].plot(time_imu, sig, color=color, linewidth=2)\n",
    "    axes[row, col].set_title(f'Cough - {name}', fontsize=11, fontweight='bold')\n",
    "    axes[row, col].set_xlabel('Time (seconds)', fontsize=9)\n",
    "    axes[row, col].set_ylabel('Magnitude', fontsize=9)\n",
    "    axes[row, col].grid(alpha=0.3)\n",
    "    axes[row, col].axhline(0, color='black', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "plt.suptitle('Example A: Cough - All 8 IMU Signals', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize all 8 signals for throat-clearing\n",
    "fig, axes = plt.subplots(4, 2, figsize=(14, 12))\n",
    "\n",
    "for idx, (name, color) in enumerate(zip(signal_names, colors)):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    if idx < 3:\n",
    "        sig = throat_imu_centered[:, idx]\n",
    "    elif idx == 3:\n",
    "        sig = throat_accel_l2\n",
    "    elif idx < 7:\n",
    "        sig = throat_imu_centered[:, idx - 1]\n",
    "    else:\n",
    "        sig = throat_gyro_l2\n",
    "    \n",
    "    axes[row, col].plot(time_imu, sig, color=color, linewidth=2)\n",
    "    axes[row, col].set_title(f'Throat-Clearing - {name}', fontsize=11, fontweight='bold')\n",
    "    axes[row, col].set_xlabel('Time (seconds)', fontsize=9)\n",
    "    axes[row, col].set_ylabel('Magnitude', fontsize=9)\n",
    "    axes[row, col].grid(alpha=0.3)\n",
    "    axes[row, col].axhline(0, color='black', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "plt.suptitle('Example B: Throat-Clearing - All 8 IMU Signals', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"  ‚Ä¢ Cough creates distinctive motion patterns:\")\n",
    "print(\"    - Sudden chest movement (accelerometer)\")\n",
    "print(\"    - Head/neck movement (gyroscope)\")\n",
    "print(\"  ‚Ä¢ L2 norms capture the total magnitude regardless of direction\")\n",
    "print(\"  ‚Ä¢ These motion signatures complement audio features for robust detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hq099vawkhr",
   "metadata": {},
   "source": [
    "### Step 7: IMU Statistical Features\n",
    "\n",
    "For each of the 8 IMU signals, we compute **5 statistical features**:\n",
    "\n",
    "**Beginner-friendly definitions:**\n",
    "- **Line Length**: Sum of absolute differences between consecutive samples. Measures how \"wiggly\" or active the signal is.\n",
    "- **Zero-Crossing Rate**: How often the signal changes sign (crosses zero). Similar to audio ZCR.\n",
    "- **Kurtosis**: A measure of \"peakiness\". High kurtosis = sharp spikes; low kurtosis = smooth signal.\n",
    "- **Crest Factor**: Peak-to-RMS ratio (same as audio). Measures impulsiveness.\n",
    "- **RMS**: Root Mean Square - overall magnitude of the signal.\n",
    "\n",
    "Let's define a function to extract all 40 features and compare cough vs throat-clearing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eplzdrr70yq",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_imu_features(imu_window):\n",
    "    \"\"\"\n",
    "    Extract 40 IMU features from a window\n",
    "    \n",
    "    Args:\n",
    "        imu_window: (40, 6) array - [Accel_x, Accel_y, Accel_z, Gyro_Y, Gyro_P, Gyro_R]\n",
    "    \n",
    "    Returns:\n",
    "        list: 40 features (8 signals √ó 5 features)\n",
    "    \"\"\"\n",
    "    # Subtract mean per channel (paper requirement)\n",
    "    imu_centered = imu_window - np.mean(imu_window, axis=0, keepdims=True)\n",
    "    \n",
    "    # Compute L2 norms for acceleration and gyroscope\n",
    "    accel_l2 = np.linalg.norm(imu_centered[:, 0:3], axis=1)\n",
    "    gyro_l2 = np.linalg.norm(imu_centered[:, 3:6], axis=1)\n",
    "    \n",
    "    # Stack all 8 signals: 3 accel + accel_L2 + 3 gyro + gyro_L2\n",
    "    signals = np.column_stack([\n",
    "        imu_centered[:, 0], imu_centered[:, 1], imu_centered[:, 2], accel_l2,\n",
    "        imu_centered[:, 3], imu_centered[:, 4], imu_centered[:, 5], gyro_l2\n",
    "    ])\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    # For each of the 8 signals, compute 5 statistical features\n",
    "    for i in range(8):\n",
    "        sig = signals[:, i]\n",
    "        \n",
    "        # 1. Line length\n",
    "        features.append(np.sum(np.abs(np.diff(sig))))\n",
    "        \n",
    "        # 2. Zero crossing rate\n",
    "        features.append(np.sum(np.diff(np.sign(sig)) != 0) / len(sig))\n",
    "        \n",
    "        # 3. Kurtosis\n",
    "        features.append(stats.kurtosis(sig))\n",
    "        \n",
    "        # 4. Crest factor\n",
    "        rms = np.sqrt(np.mean(sig**2))\n",
    "        features.append(np.max(np.abs(sig)) / (rms + 1e-10))\n",
    "        \n",
    "        # 5. RMS power\n",
    "        features.append(rms)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Test on random data\n",
    "test_imu = np.random.randn(40, 6)\n",
    "test_imu_features = extract_imu_features(test_imu)\n",
    "print(f\"‚úì IMU feature extractor: {len(test_imu_features)} features\")\n",
    "assert len(test_imu_features) == 40, f\"Expected 40 features, got {len(test_imu_features)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u7ic4ey7xjl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract full 40 IMU features using our feature extraction function\n",
    "imu_features_cough = extract_imu_features(cough_imu_window)\n",
    "imu_features_throat = extract_imu_features(throat_imu_window)\n",
    "\n",
    "# Create feature names for visualization\n",
    "feature_types = ['Line Length', 'ZCR', 'Kurtosis', 'Crest', 'RMS']\n",
    "signal_names_short = ['Acc_X', 'Acc_Y', 'Acc_Z', 'Acc_L2', 'Gyro_Y', 'Gyro_P', 'Gyro_R', 'Gyro_L2']\n",
    "feature_names = []\n",
    "for sig in signal_names_short:\n",
    "    for feat in feature_types:\n",
    "        feature_names.append(f'{sig}_{feat}')\n",
    "\n",
    "# Visualize all 40 features as a bar chart\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "x = np.arange(40)\n",
    "width = 0.35\n",
    "\n",
    "# Use raw feature values (no normalization for 2-sample comparison)\n",
    "ax.bar(x - width/2, imu_features_cough, width, label='Cough', color='steelblue', alpha=0.7)\n",
    "ax.bar(x + width/2, imu_features_throat, width, label='Throat-Clearing', color='orange', alpha=0.7)\n",
    "\n",
    "ax.set_title('All 40 IMU Features Comparison (Raw Values)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Feature Index', fontsize=12)\n",
    "ax.set_ylabel('Feature Value', fontsize=12)\n",
    "ax.set_xticks(x[::5])  # Show every 5th tick\n",
    "ax.set_xticklabels(range(0, 40, 5))\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "ax.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Add vertical lines to separate signal groups\n",
    "for i in range(1, 8):\n",
    "    ax.axvline(i * 5 - 0.5, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# Add signal labels\n",
    "signal_positions = [2.5, 7.5, 12.5, 17.5, 22.5, 27.5, 32.5, 37.5]\n",
    "for pos, sig_name in zip(signal_positions, signal_names_short):\n",
    "    ax.text(pos, ax.get_ylim()[1] * 0.9, sig_name, ha='center', fontsize=8,\n",
    "            rotation=0, fontweight='bold', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find top 10 most discriminative features (largest absolute difference)\n",
    "differences = np.abs(np.array(imu_features_cough) - np.array(imu_features_throat))\n",
    "top_indices = np.argsort(differences)[-10:][::-1]\n",
    "\n",
    "print(\"\\nüìä Top 10 Most Discriminative IMU Features:\")\n",
    "print(f\"{'Rank':<6} {'Feature Name':<25} {'Cough':>12} {'Throat':>12} {'Diff':>12}\")\n",
    "print(\"-\" * 75)\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    print(f\"{rank:<6} {feature_names[idx]:<25} {imu_features_cough[idx]:>12.4f} {imu_features_throat[idx]:>12.4f} {differences[idx]:>12.4f}\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  ‚Ä¢ **Gyroscope Line Length dominates** - all top 5 features measure rotational motion intensity\")\n",
    "print(\"  ‚Ä¢ Coughs produce **3x more gyroscope activity** than throat-clearing (head/neck rotation)\")\n",
    "print(\"  ‚Ä¢ Line Length (signal 'wiggliness') is far more discriminative than statistical measures\")\n",
    "print(\"  ‚Ä¢ Gyro_R (Roll) shows the largest difference - captures head tilting during cough\")\n",
    "print(\"  ‚Ä¢ IMU captures biomechanics: coughs = sudden head/neck movement, throat-clearing = more stationary\")\n",
    "print(\"  ‚Ä¢ These 40 features complement audio features for robust multimodal detection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mz0g5w6snto",
   "metadata": {},
   "source": [
    "### Step 8: Complete Feature Vector - Putting It All Together\n",
    "\n",
    "Now let's extract the **complete 105-feature vector** (65 audio + 40 IMU) for both examples and visualize the final result.\n",
    "\n",
    "This is what gets fed into the machine learning model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g33931zo6nq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract complete feature vectors\n",
    "audio_features_cough = extract_audio_features(cough_audio_window, fs=FS_AUDIO)\n",
    "audio_features_throat = extract_audio_features(throat_audio_window, fs=FS_AUDIO)\n",
    "\n",
    "# Combine audio + IMU\n",
    "complete_features_cough = np.concatenate([audio_features_cough, imu_features_cough])\n",
    "complete_features_throat = np.concatenate([audio_features_throat, imu_features_throat])\n",
    "\n",
    "print(f\"‚úì Complete feature vectors extracted:\")\n",
    "print(f\"  Cough: {len(complete_features_cough)} features\")\n",
    "print(f\"  Throat-clearing: {len(complete_features_throat)} features\")\n",
    "print(f\"\\n  Breakdown: 65 audio + 40 IMU = 105 total\\n\")\n",
    "\n",
    "# Visualize as grouped bar chart\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 14))\n",
    "\n",
    "# Use log scale for better visualization of features with different magnitudes\n",
    "# Apply log(1 + abs(x)) * sign(x) transformation to preserve sign\n",
    "def log_transform(x):\n",
    "    return np.sign(x) * np.log1p(np.abs(x))\n",
    "\n",
    "cough_log = log_transform(complete_features_cough)\n",
    "throat_log = log_transform(complete_features_throat)\n",
    "\n",
    "# Panel 1: All 105 features\n",
    "x = np.arange(105)\n",
    "width = 0.35\n",
    "axes[0].bar(x - width/2, cough_log, width, label='Cough', color='steelblue', alpha=0.7)\n",
    "axes[0].bar(x + width/2, throat_log, width, label='Throat-Clearing', color='orange', alpha=0.7)\n",
    "axes[0].set_title('Complete 105-Feature Vector (Log-Scaled)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature Index', fontsize=12)\n",
    "axes[0].set_ylabel('Log-Scaled Feature Value', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "axes[0].axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "# Add divider between audio and IMU\n",
    "axes[0].axvline(64.5, color='red', linestyle='--', linewidth=2, label='Audio | IMU')\n",
    "axes[0].text(32, axes[0].get_ylim()[1] * 0.85, 'Audio Features (65)', \n",
    "             ha='center', fontsize=11, fontweight='bold',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "axes[0].text(82, axes[0].get_ylim()[1] * 0.85, 'IMU Features (40)',\n",
    "             ha='center', fontsize=11, fontweight='bold',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "# Panel 2: Audio features only (first 65)\n",
    "x_audio = np.arange(65)\n",
    "axes[1].bar(x_audio - width/2, cough_log[:65], width, label='Cough', color='steelblue', alpha=0.7)\n",
    "axes[1].bar(x_audio + width/2, throat_log[:65], width, label='Throat-Clearing', color='orange', alpha=0.7)\n",
    "axes[1].set_title('Audio Features Only (65 features, Log-Scaled)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Feature Index', fontsize=11)\n",
    "axes[1].set_ylabel('Log-Scaled Feature Value', fontsize=11)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "axes[1].axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "# Add labels for MFCC, Spectral, Time-domain regions\n",
    "axes[1].text(26, axes[1].get_ylim()[1] * 0.85, 'MFCC (52)', ha='center', fontsize=9,\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.4))\n",
    "axes[1].text(57, axes[1].get_ylim()[1] * 0.85, 'Spectral (10)', ha='center', fontsize=9,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.4))\n",
    "axes[1].text(63, axes[1].get_ylim()[1] * 0.85, 'Time (3)', ha='center', fontsize=9,\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.4))\n",
    "\n",
    "# Panel 3: IMU features only (last 40)\n",
    "x_imu = np.arange(40)\n",
    "axes[2].bar(x_imu - width/2, cough_log[65:], width, label='Cough', color='steelblue', alpha=0.7)\n",
    "axes[2].bar(x_imu + width/2, throat_log[65:], width, label='Throat-Clearing', color='orange', alpha=0.7)\n",
    "axes[2].set_title('IMU Features Only (40 features, Log-Scaled)', fontsize=13, fontweight='bold')\n",
    "axes[2].set_xlabel('Feature Index', fontsize=11)\n",
    "axes[2].set_ylabel('Log-Scaled Feature Value', fontsize=11)\n",
    "axes[2].legend(fontsize=10)\n",
    "axes[2].grid(alpha=0.3, axis='y')\n",
    "axes[2].axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute overall statistics using raw values\n",
    "differences_all = np.abs(complete_features_cough - complete_features_throat)\n",
    "top_15_indices = np.argsort(differences_all)[-15:][::-1]\n",
    "\n",
    "# Map feature indices to types\n",
    "feature_type_map = {\n",
    "    (0, 52): \"MFCC\",\n",
    "    52: \"Spectral Centroid\",\n",
    "    53: \"Spectral Rolloff\", \n",
    "    54: \"Spectral Bandwidth\",\n",
    "    55: \"Spectral Flatness\",\n",
    "    56: \"Spectral Contrast\",\n",
    "    57: \"Total Power\",\n",
    "    58: \"Dominant Frequency\",\n",
    "    59: \"Spectral Spread\",\n",
    "    60: \"Spectral Skewness\",\n",
    "    61: \"Spectral Kurtosis\",\n",
    "    62: \"Zero-Crossing Rate\",\n",
    "    63: \"RMS Energy\",\n",
    "    64: \"Crest Factor\",\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üìä TOP 15 MOST DISCRIMINATIVE FEATURES (out of 105)\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Rank':<6} {'Type':<10} {'Feature Name':<30} {'Cough':>15} {'Throat':>15}\")\n",
    "print(\"-\" * 90)\n",
    "for rank, idx in enumerate(top_15_indices, 1):\n",
    "    if idx < 65:\n",
    "        feat_type = \"Audio\"\n",
    "        if idx < 52:\n",
    "            feat_name = f\"MFCC #{idx}\"\n",
    "        else:\n",
    "            feat_name = feature_type_map.get(idx, f\"Audio #{idx}\")\n",
    "    else:\n",
    "        feat_type = \"IMU\"\n",
    "        imu_idx = idx - 65\n",
    "        sig_idx = imu_idx // 5\n",
    "        feat_idx = imu_idx % 5\n",
    "        feat_name = f\"{signal_names_short[sig_idx]}_{feature_types[feat_idx]}\"\n",
    "    \n",
    "    print(f\"{rank:<6} {feat_type:<10} {feat_name:<30} {complete_features_cough[idx]:>15.2f} \"\n",
    "          f\"{complete_features_throat[idx]:>15.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"üéØ CRITICAL INSIGHTS FROM THIS EXAMPLE\")\n",
    "print(\"=\"*90)\n",
    "print()\n",
    "print(\"1. **Audio features dominate discrimination** - all top 15 are audio, NO IMU in top 15!\")\n",
    "print()\n",
    "print(\"2. **Spectral features >> MFCC features**:\")\n",
    "print(\"   ‚Ä¢ Spectral Kurtosis (rank 1): Measures frequency distribution 'peakiness'\")\n",
    "print(\"   ‚Ä¢ Spectral Skewness (rank 2): Measures frequency distribution asymmetry\")\n",
    "print(\"   ‚Ä¢ Spectral Rolloff (rank 3): 6x higher for cough (2547 vs 414 Hz)\")\n",
    "print(\"   ‚Ä¢ Spectral Centroid (rank 4): 3x higher for cough (988 vs 312 Hz)\")\n",
    "print()\n",
    "print(\"3. **Why spectral features win in THIS example**:\")\n",
    "print(\"   ‚Ä¢ This is a quiet environment (sitting, no background noise)\")\n",
    "print(\"   ‚Ä¢ Coughs have sharp, broadband spectral bursts (high kurtosis/skewness)\")\n",
    "print(\"   ‚Ä¢ Throat-clearing has concentrated low-frequency energy\")\n",
    "print(\"   ‚Ä¢ The spectral difference is dramatic and easily separable\")\n",
    "print()\n",
    "print(\"4. **Context matters - this is ONE example**:\")\n",
    "print(\"   ‚Ä¢ In noisy environments (music, traffic), IMU features become MORE critical\")\n",
    "print(\"   ‚Ä¢ Multimodal (audio + IMU) provides robustness across ALL conditions\")\n",
    "print(\"   ‚Ä¢ This notebook shows feature extraction; actual model uses ALL 105 features\")\n",
    "print()\n",
    "print(\"5. **Next step**: Train XGBoost on the full dataset ‚Üí Model_Training_XGBoost.ipynb\")\n",
    "print(\"   ‚Ä¢ You'll see how feature importance changes across 15 subjects and all conditions\")\n",
    "print(\"   ‚Ä¢ The model learns when to rely on audio vs IMU vs both\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ouawm2j09k",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Batch Feature Extraction Function\n",
    "\n",
    "Finally, let's define a function to process the entire dataset efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jue82gnxfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_dataset(audio_data, imu_data, modality='all'):\n",
    "    \"\"\"\n",
    "    Extract features for entire dataset\n",
    "    \n",
    "    Args:\n",
    "        audio_data: (N, 6400, 2) - [outer_mic, body_mic]\n",
    "        imu_data: (N, 40, 6)\n",
    "        modality: 'imu_only', 'audio_only', or 'all'\n",
    "    \n",
    "    Returns:\n",
    "        X: (N, n_features) feature matrix\n",
    "    \"\"\"\n",
    "    N = audio_data.shape[0]\n",
    "    features_list = []\n",
    "    \n",
    "    for i in tqdm(range(N), desc=f\"Extracting {modality} features\"):\n",
    "        sample_features = []\n",
    "        \n",
    "        if modality in ['audio_only', 'all']:\n",
    "            # Use outer microphone (index 0)\n",
    "            audio_outer = audio_data[i, :, 0]\n",
    "            sample_features.extend(extract_audio_features(audio_outer))\n",
    "        \n",
    "        if modality in ['imu_only', 'all']:\n",
    "            imu_window = imu_data[i, :, :]\n",
    "            sample_features.extend(extract_imu_features(imu_window))\n",
    "        \n",
    "        features_list.append(sample_features)\n",
    "    \n",
    "    X = np.array(features_list)\n",
    "    \n",
    "    # Handle NaN/Inf values\n",
    "    if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n",
    "        print(f\"Warning: Replacing {np.sum(np.isnan(X))} NaN and {np.sum(np.isinf(X))} Inf values\")\n",
    "        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return X\n",
    "\n",
    "print(\"‚úì Batch feature extraction function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90a440",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Loading and Feature Extraction\n",
    "\n",
    "Now that you understand how features work through our examples, let's apply this to the **entire dataset**!\n",
    "\n",
    "We'll:\n",
    "1. Load raw windowed data from all 15 subjects\n",
    "2. Extract 105 features from each window\n",
    "3. Save the features for model training\n",
    "\n",
    "This process will convert ~4 hours of raw biosignal recordings into a compact feature matrix ready for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8b2f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw windowed data from all subjects\n",
    "all_audio = []\n",
    "all_imu = []\n",
    "all_labels = []\n",
    "all_subjects = []\n",
    "\n",
    "print(\"Loading dataset (this may take a few minutes)...\\n\")\n",
    "\n",
    "for subj_id in tqdm(subject_ids, desc=\"Loading subjects\"):\n",
    "    try:\n",
    "        audio, imu, labels, n_coughs = get_samples_for_subject(\n",
    "            data_folder, subj_id,\n",
    "            window_len=WINDOW_LEN,\n",
    "            aug_factor=AUG_FACTOR\n",
    "        )\n",
    "        \n",
    "        all_audio.append(audio)\n",
    "        all_imu.append(imu)\n",
    "        all_labels.append(labels)\n",
    "        all_subjects.extend([subj_id] * len(labels))\n",
    "        \n",
    "        print(f\"  {subj_id}: {n_coughs} coughs ‚Üí {len(labels)} windows \"\n",
    "              f\"({np.sum(labels==1)} cough, {np.sum(labels==0)} non-cough)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {subj_id}: Error - {e}\")\n",
    "        continue\n",
    "\n",
    "# Concatenate all subjects\n",
    "audio_data = np.concatenate(all_audio, axis=0)\n",
    "imu_data = np.concatenate(all_imu, axis=0)\n",
    "labels = np.concatenate(all_labels, axis=0)\n",
    "subjects = np.array(all_subjects)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Total dataset:\")\n",
    "print(f\"  Audio shape: {audio_data.shape}\")\n",
    "print(f\"  IMU shape: {imu_data.shape}\")\n",
    "print(f\"  Labels: {len(labels)} ({np.sum(labels==1)} coughs, {np.sum(labels==0)} non-coughs)\")\n",
    "print(f\"  Unique subjects: {len(np.unique(subjects))}\")\n",
    "print(f\"  Class balance: {np.sum(labels==1)/len(labels)*100:.1f}% coughs\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "assert audio_data.shape[1] == 6400, f\"Expected 6400 audio samples, got {audio_data.shape[1]}\"\n",
    "assert imu_data.shape[1] == 40, f\"Expected 40 IMU samples, got {imu_data.shape[1]}\"\n",
    "assert len(np.unique(subjects)) == 15, f\"Expected 15 subjects, got {len(np.unique(subjects))}\"\n",
    "\n",
    "# Visualize one cough sample\n",
    "idx = np.where(labels == 1)[0][0]\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "axes[0].plot(audio_data[idx, :, 0], linewidth=0.5)\n",
    "axes[0].set_title(f\"Sample Cough - Outer Microphone (Subject {subjects[idx]})\")\n",
    "axes[0].set_xlabel(\"Sample Index\")\n",
    "axes[0].set_ylabel(\"Amplitude\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(-imu_data[idx, :, 2], linewidth=1)\n",
    "axes[1].set_title(\"Accelerometer Z (negated)\")\n",
    "axes[1].set_xlabel(\"Sample Index\")\n",
    "axes[1].set_ylabel(\"Acceleration\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Data loaded and verified successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9ac80",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Extract handcrafted features for all three modalities:\n",
    "1. IMU-only: 40 features\n",
    "2. Audio-only: 65 features\n",
    "3. Multimodal: 105 features\n",
    "\n",
    "**Note**: This may take 10-20 minutes depending on hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "print(\"Extracting features for all modalities...\\n\")\n",
    "\n",
    "N = audio_data.shape[0]\n",
    "n_cpus = multiprocessing.cpu_count()\n",
    "\n",
    "# Configure parallelization based on available cores\n",
    "if n_cpus >= 8:\n",
    "    n_jobs = 8\n",
    "    blas_threads = 2\n",
    "else:\n",
    "    # Run with all CPUs, but without blas threads\n",
    "    n_jobs = n_cpus\n",
    "    blas_threads = 1\n",
    "\n",
    "# Limit BLAS threading to prevent oversubscription\n",
    "os.environ['OMP_NUM_THREADS'] = str(blas_threads)\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = str(blas_threads)\n",
    "os.environ['MKL_NUM_THREADS'] = str(blas_threads)\n",
    "\n",
    "print(f\"Hardware: {n_cpus} CPU cores detected\")\n",
    "print(f\"Configuration: {n_jobs} workers √ó {blas_threads} BLAS threads = {n_jobs * blas_threads} total\\n\")\n",
    "\n",
    "# ===================================================================\n",
    "# Step 1/2: Extract audio features (65 features from outer mic)\n",
    "# ===================================================================\n",
    "print(\"Step 1/2: Extracting audio features...\")\n",
    "audio_features_list = Parallel(n_jobs=n_jobs, backend='loky')(\n",
    "    delayed(extract_audio_features)(audio_data[i, :, 0])\n",
    "    for i in tqdm(range(N), desc=\"Audio features\")\n",
    ")\n",
    "X_audio = np.array(audio_features_list)\n",
    "\n",
    "# Handle NaN/Inf in audio features\n",
    "if np.any(np.isnan(X_audio)) or np.any(np.isinf(X_audio)):\n",
    "    print(f\"  Warning: Replacing {np.sum(np.isnan(X_audio))} NaN and {np.sum(np.isinf(X_audio))} Inf values in audio\")\n",
    "    X_audio = np.nan_to_num(X_audio, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# ===================================================================\n",
    "# Step 2/2: Extract IMU features (40 features)\n",
    "# ===================================================================\n",
    "print(\"\\nStep 2/2: Extracting IMU features...\")\n",
    "imu_features_list = Parallel(n_jobs=n_jobs, backend='loky')(\n",
    "    delayed(extract_imu_features)(imu_data[i, :, :])\n",
    "    for i in tqdm(range(N), desc=\"IMU features\")\n",
    ")\n",
    "X_imu = np.array(imu_features_list)\n",
    "\n",
    "# Handle NaN/Inf in IMU features\n",
    "if np.any(np.isnan(X_imu)) or np.any(np.isinf(X_imu)):\n",
    "    print(f\"  Warning: Replacing {np.sum(np.isnan(X_imu))} NaN and {np.sum(np.isinf(X_imu))} Inf values in IMU\")\n",
    "    X_imu = np.nan_to_num(X_imu, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# ===================================================================\n",
    "# Combine for multimodal (65 audio + 40 IMU = 105 features)\n",
    "# ===================================================================\n",
    "X_all = np.concatenate([X_audio, X_imu], axis=1)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Feature extraction complete:\")\n",
    "print(f\"  Audio-only: {X_audio.shape} (65 features)\")\n",
    "print(f\"  IMU-only: {X_imu.shape} (40 features)\")\n",
    "print(f\"  Multimodal: {X_all.shape} (105 features)\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7979ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features to avoid re-extraction\n",
    "save_path = 'extracted_features.npz'\n",
    "np.savez(\n",
    "    save_path,\n",
    "    X_imu=X_imu, \n",
    "    X_audio=X_audio, \n",
    "    X_all=X_all,\n",
    "    labels=labels, \n",
    "    subjects=subjects\n",
    ")\n",
    "print(f\"‚úì Features saved to {save_path}\")\n",
    "print(f\"  To load: data = np.load('{save_path}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hdtu2frzi1t",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Summary and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "In this notebook, we transformed raw biosignal data into machine learning-ready features:\n",
    "\n",
    "**Raw Data ‚Üí Features:**\n",
    "- **Input**: 0.4-second windows (6,400 audio samples + 40 IMU samples per window)\n",
    "- **Output**: 105 compact features per window\n",
    "- **Compression**: From ~6,440 values ‚Üí 105 values (61√ó reduction!)\n",
    "\n",
    "### Feature Breakdown\n",
    "\n",
    "| Category | Count | Purpose |\n",
    "|----------|-------|---------|\n",
    "| **MFCC** | 52 | Capture sound texture/timbre |\n",
    "| **Spectral** | 10 | Describe frequency distribution |\n",
    "| **Time-domain** | 3 | Measure loudness and sharpness |\n",
    "| **IMU** | 40 | Capture body motion patterns |\n",
    "| **TOTAL** | **105** | Complete multimodal signature |\n",
    "\n",
    "### Key Insights from Our Examples\n",
    "\n",
    "Through our step-by-step walkthrough, we learned:\n",
    "\n",
    "1. **Coughs have distinct characteristics:**\n",
    "   - Sharp, impulsive audio bursts (high crest factor)\n",
    "   - Strong high-frequency content (high spectral centroid/rolloff)\n",
    "   - Distinctive motion patterns (accelerometer + gyroscope spikes)\n",
    "\n",
    "2. **Multimodal sensing is powerful:**\n",
    "   - Audio alone: Good, but struggles in noisy environments\n",
    "   - IMU alone: Captures motion, but misses acoustic details\n",
    "   - Combined: Robust detection across all conditions\n",
    "\n",
    "3. **Features enable machine learning:**\n",
    "   - Each window becomes a point in 105-dimensional space\n",
    "   - Coughs cluster in one region, non-coughs in another\n",
    "   - ML models learn to find the optimal decision boundary\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Immediate next step:**\n",
    "- üìò **Open `Model_Training_XGBoost.ipynb`** to train classifiers on these features\n",
    "- Learn how XGBoost achieves ~96% ROC-AUC using multimodal features\n",
    "- Understand cross-validation, class balancing, and performance metrics\n",
    "\n",
    "**Beyond training:**\n",
    "- Feature selection (reducing 105 ‚Üí fewer features while maintaining accuracy)\n",
    "- Hyperparameter tuning for optimal performance\n",
    "- Model explainability (which features matter most?)\n",
    "- Edge deployment for real-time cough counting\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `extracted_features.npz`: Complete feature dataset\n",
    "  - `X_audio`: (N, 65) - Audio-only features\n",
    "  - `X_imu`: (N, 40) - IMU-only features\n",
    "  - `X_all`: (N, 105) - Multimodal features\n",
    "  - `labels`: (N,) - Ground truth (1=cough, 0=non-cough)\n",
    "  - `subjects`: (N,) - Subject IDs for cross-validation\n",
    "\n",
    "**To reload these features:**\n",
    "```python\n",
    "data = np.load('extracted_features.npz')\n",
    "X_all = data['X_all']\n",
    "labels = data['labels']\n",
    "subjects = data['subjects']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**You now understand the complete feature extraction pipeline!** üéâ\n",
    "\n",
    "The journey from raw biosignals ‚Üí features ‚Üí trained model is a fundamental workflow in biomedical signal processing and machine learning. The skills you've learned here apply to many other domains: speech recognition, activity recognition, ECG analysis, and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge-ai-cough-count",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
