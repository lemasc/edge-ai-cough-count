{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Cough Detection Training\n",
    "\n",
    "This notebook reproduces the classical ML pipeline from the research paper for cough detection using multimodal biosignals.\n",
    "\n",
    "## Objective\n",
    "\n",
    "Train XGBoost classifiers on three modality configurations:\n",
    "1. **IMU-only**: 40 handcrafted features from accelerometer and gyroscope\n",
    "2. **Audio-only**: 65 features from outer microphone (MFCC + spectral + time-domain)\n",
    "3. **Multimodal**: Combined 105 features (Audio + IMU)\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "Based on the paper, 5-fold subject-wise cross-validation should yield:\n",
    "- IMU-only: ROC-AUC ~0.90 ± 0.02\n",
    "- Audio-only: ROC-AUC ~0.92 ± 0.01\n",
    "- Multimodal: ROC-AUC ~0.96 ± 0.01\n",
    "\n",
    "## Method\n",
    "\n",
    "- **Window size**: 0.4 seconds (6400 audio samples @ 16kHz, 40 IMU samples @ 100Hz)\n",
    "- **Data augmentation**: Random temporal shifts (aug_factor=2)\n",
    "- **Class balancing**: SMOTE oversampling on training splits\n",
    "- **Feature scaling**: StandardScaler (fit on train, applied to train/val)\n",
    "- **Cross-validation**: Subject-wise GroupKFold (n=5) to prevent data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for required dependencies\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import xgboost\n",
    "    import imblearn\n",
    "    print(\"✓ All required dependencies installed\")\n",
    "    print(f\"  - xgboost version: {xgboost.__version__}\")\n",
    "    print(f\"  - imbalanced-learn version: {imblearn.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Missing dependency: {e}\")\n",
    "    print(\"\\nInstall with: pip install xgboost imbalanced-learn shap\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, signal\n",
    "import librosa\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, f1_score, confusion_matrix,\n",
    "    precision_score, recall_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if os.path.exists(\"/kaggle/usr/lib/\"):\n",
    "    # Load from Kaggle as utility scripts\n",
    "    from edge_ai_cough_count_helpers import * # pyright: ignore[reportMissingImports]\n",
    "    from edge_ai_cough_count_dataset_gen import * # pyright: ignore[reportMissingImports]\n",
    "    from edge_ai_cough_count_features import extract_audio_features, extract_imu_features # pyright: ignore[reportMissingImports]\n",
    "else:\n",
    "    # Add src directory to path\n",
    "    sys.path.append(os.path.abspath('../src'))\n",
    "    from helpers import *\n",
    "    from dataset_gen import *\n",
    "    from features import extract_audio_features, extract_imu_features\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Constants from paper\n",
    "WINDOW_LEN = 0.4  # 0.4 second windows\n",
    "AUG_FACTOR = 2    # Data augmentation factor\n",
    "N_FOLDS = 5       # Number of CV folds\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Window length: {WINDOW_LEN}s\")\n",
    "print(f\"  Expected audio samples: {int(WINDOW_LEN * FS_AUDIO)}\")\n",
    "print(f\"  Expected IMU samples: {int(WINDOW_LEN * FS_IMU)}\")\n",
    "print(f\"  Augmentation factor: {AUG_FACTOR}\")\n",
    "print(f\"  CV folds: {N_FOLDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load raw windowed data from all 15 subjects using `get_samples_for_subject()` from `dataset_gen.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate dataset folder\n",
    "kaggle_dataset_dir = '/kaggle/input/edge-ai-cough-count'\n",
    "base_dir = kaggle_dataset_dir if os.path.exists(kaggle_dataset_dir) else \"..\"\n",
    "data_folder = base_dir + '/public_dataset/'\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    raise FileNotFoundError(\n",
    "        \"Cannot find public_dataset/. Please download from: \"\n",
    "        \"https://zenodo.org/record/7562332\"\n",
    "    )\n",
    "\n",
    "# Get list of subject IDs\n",
    "subject_ids = [d for d in os.listdir(data_folder) \n",
    "               if os.path.isdir(os.path.join(data_folder, d))]\n",
    "subject_ids = sorted(subject_ids)\n",
    "\n",
    "print(f\"✓ Found {len(subject_ids)} subjects: {subject_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw windowed data from all subjects\n",
    "all_audio = []\n",
    "all_imu = []\n",
    "all_labels = []\n",
    "all_subjects = []\n",
    "\n",
    "print(\"Loading dataset (this may take a few minutes)...\\n\")\n",
    "\n",
    "for subj_id in tqdm(subject_ids, desc=\"Loading subjects\"):\n",
    "    try:\n",
    "        audio, imu, labels, n_coughs = get_samples_for_subject(\n",
    "            data_folder, subj_id,\n",
    "            window_len=WINDOW_LEN,\n",
    "            aug_factor=AUG_FACTOR\n",
    "        )\n",
    "        \n",
    "        all_audio.append(audio)\n",
    "        all_imu.append(imu)\n",
    "        all_labels.append(labels)\n",
    "        all_subjects.extend([subj_id] * len(labels))\n",
    "        \n",
    "        print(f\"  {subj_id}: {n_coughs} coughs → {len(labels)} windows \"\n",
    "              f\"({np.sum(labels==1)} cough, {np.sum(labels==0)} non-cough)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {subj_id}: Error - {e}\")\n",
    "        continue\n",
    "\n",
    "# Concatenate all subjects\n",
    "audio_data = np.concatenate(all_audio, axis=0)\n",
    "imu_data = np.concatenate(all_imu, axis=0)\n",
    "labels = np.concatenate(all_labels, axis=0)\n",
    "subjects = np.array(all_subjects)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Total dataset:\")\n",
    "print(f\"  Audio shape: {audio_data.shape}\")\n",
    "print(f\"  IMU shape: {imu_data.shape}\")\n",
    "print(f\"  Labels: {len(labels)} ({np.sum(labels==1)} coughs, {np.sum(labels==0)} non-coughs)\")\n",
    "print(f\"  Unique subjects: {len(np.unique(subjects))}\")\n",
    "print(f\"  Class balance: {np.sum(labels==1)/len(labels)*100:.1f}% coughs\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "assert audio_data.shape[1] == 6400, f\"Expected 6400 audio samples, got {audio_data.shape[1]}\"\n",
    "assert imu_data.shape[1] == 40, f\"Expected 40 IMU samples, got {imu_data.shape[1]}\"\n",
    "assert len(np.unique(subjects)) == 15, f\"Expected 15 subjects, got {len(np.unique(subjects))}\"\n",
    "\n",
    "# Visualize one cough sample\n",
    "idx = np.where(labels == 1)[0][0]\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "axes[0].plot(audio_data[idx, :, 0], linewidth=0.5)\n",
    "axes[0].set_title(f\"Sample Cough - Outer Microphone (Subject {subjects[idx]})\")\n",
    "axes[0].set_xlabel(\"Sample Index\")\n",
    "axes[0].set_ylabel(\"Amplitude\")\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(-imu_data[idx, :, 2], linewidth=1)\n",
    "axes[1].set_title(\"Accelerometer Z (negated)\")\n",
    "axes[1].set_xlabel(\"Sample Index\")\n",
    "axes[1].set_ylabel(\"Acceleration\")\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Data loaded and verified successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Extract handcrafted features for all three modalities:\n",
    "1. IMU-only: 40 features\n",
    "2. Audio-only: 65 features\n",
    "3. Multimodal: 105 features\n",
    "\n",
    "**Note**: This may take 10-20 minutes depending on hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "print(\"Extracting features for all modalities...\\n\")\n",
    "\n",
    "N = audio_data.shape[0]\n",
    "n_cpus = multiprocessing.cpu_count()\n",
    "\n",
    "# Configure parallelization based on available cores\n",
    "if n_cpus >= 8:\n",
    "    n_jobs = 8\n",
    "    n_jobs = 8\n",
    "    blas_threads = 2\n",
    "else:\n",
    "    # Run with all CPUs, but without blas threads\n",
    "    n_jobs = n_cpus\n",
    "    blas_threads = 1\n",
    "\n",
    "# Limit BLAS threading to prevent oversubscription\n",
    "os.environ['OMP_NUM_THREADS'] = str(blas_threads)\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = str(blas_threads)\n",
    "os.environ['MKL_NUM_THREADS'] = str(blas_threads)\n",
    "\n",
    "print(f\"Hardware: {n_cpus} CPU cores detected\")\n",
    "print(f\"Configuration: {n_jobs} workers × {blas_threads} BLAS threads = {n_jobs * blas_threads} total\\n\")\n",
    "\n",
    "# ===================================================================\n",
    "# Step 1/2: Extract audio features (65 features from outer mic)\n",
    "# ===================================================================\n",
    "print(\"Step 1/2: Extracting audio features...\")\n",
    "print(f\"  Using {n_jobs} parallel workers\")\n",
    "\n",
    "audio_features_list = Parallel(n_jobs=n_jobs, backend='loky')(\n",
    "    delayed(extract_audio_features)(audio_data[i, :, 0])\n",
    "    for i in tqdm(range(N), desc=\"Audio features\")\n",
    ")\n",
    "X_audio = np.array(audio_features_list)\n",
    "\n",
    "# Handle NaN/Inf in audio features\n",
    "if np.any(np.isnan(X_audio)) or np.any(np.isinf(X_audio)):\n",
    "    print(f\"  Warning: Replacing {np.sum(np.isnan(X_audio))} NaN and {np.sum(np.isinf(X_audio))} Inf values in audio\")\n",
    "    X_audio = np.nan_to_num(X_audio, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# ===================================================================\n",
    "# Step 2/2: Extract IMU features (40 features)\n",
    "# ===================================================================\n",
    "print(\"\\nStep 2/2: Extracting IMU features...\")\n",
    "print(f\"  Using {n_jobs} parallel workers\")\n",
    "\n",
    "imu_features_list = Parallel(n_jobs=n_jobs, backend='loky')(\n",
    "    delayed(extract_imu_features)(imu_data[i, :, :])\n",
    "    for i in tqdm(range(N), desc=\"IMU features\")\n",
    ")\n",
    "X_imu = np.array(imu_features_list)\n",
    "\n",
    "# Handle NaN/Inf in IMU features\n",
    "if np.any(np.isnan(X_imu)) or np.any(np.isinf(X_imu)):\n",
    "    print(f\"  Warning: Replacing {np.sum(np.isnan(X_imu))} NaN and {np.sum(np.isinf(X_imu))} Inf values in IMU\")\n",
    "    X_imu = np.nan_to_num(X_imu, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# ===================================================================\n",
    "# Combine for multimodal (65 audio + 40 IMU = 105 features)\n",
    "# ===================================================================\n",
    "X_all = np.concatenate([X_audio, X_imu], axis=1)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Feature extraction complete:\")\n",
    "print(f\"  Audio-only: {X_audio.shape} (65 features)\")\n",
    "print(f\"  IMU-only: {X_imu.shape} (40 features)\")\n",
    "print(f\"  Multimodal: {X_all.shape} (105 features)\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features to avoid re-extraction\n",
    "save_path = 'extracted_features.npz'\n",
    "np.savez(\n",
    "    save_path,\n",
    "    X_imu=X_imu, \n",
    "    X_audio=X_audio, \n",
    "    X_all=X_all,\n",
    "    labels=labels, \n",
    "    subjects=subjects\n",
    ")\n",
    "print(f\"✓ Features saved to {save_path}\")\n",
    "print(f\"  To load: data = np.load('{save_path}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "\n",
    "Subject-wise cross-validation with:\n",
    "- GroupKFold (n=5) to prevent data leakage between subjects\n",
    "- StandardScaler for feature normalization\n",
    "- SMOTE for handling class imbalance (applied only to training splits)\n",
    "- XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_cv(X, y, groups, n_folds=5, model_name=\"XGBoost\"):\n",
    "    \"\"\"\n",
    "    Subject-wise cross-validation with SMOTE and StandardScaler\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix (N, n_features)\n",
    "        y: Labels (N,)\n",
    "        groups: Subject IDs (N,)\n",
    "        n_folds: Number of CV folds\n",
    "        model_name: Model name for logging\n",
    "    \n",
    "    Returns:\n",
    "        dict: Fold results and metrics\n",
    "    \"\"\"\n",
    "    # Map subject IDs to numeric indices for GroupKFold\n",
    "    unique_subjects = np.unique(groups)\n",
    "    subject_to_idx = {subj: idx for idx, subj in enumerate(unique_subjects)}\n",
    "    group_indices = np.array([subject_to_idx[s] for s in groups])\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=n_folds)\n",
    "    \n",
    "    results = {\n",
    "        'fold_aucs': [],\n",
    "        'fold_predictions': [],\n",
    "        'fold_true_labels': [],\n",
    "        'fold_train_subjects': [],\n",
    "        'fold_val_subjects': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {model_name} with {n_folds}-fold subject-wise CV\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(gkf.split(X, y, group_indices)):\n",
    "        print(f\"Fold {fold_idx + 1}/{n_folds}\")\n",
    "        \n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        train_subjects = np.unique(groups[train_idx])\n",
    "        val_subjects = np.unique(groups[val_idx])\n",
    "        print(f\"  Train: {len(train_subjects)} subjects, {len(y_train)} samples \"\n",
    "              f\"({np.sum(y_train==1)} coughs, {np.sum(y_train==0)} non-coughs)\")\n",
    "        print(f\"  Val: {len(val_subjects)} subjects, {len(y_val)} samples \"\n",
    "              f\"({np.sum(y_val==1)} coughs, {np.sum(y_val==0)} non-coughs)\")\n",
    "        \n",
    "        # Scale features (fit on train only)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        # Apply SMOTE (train only)\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "        print(f\"  After SMOTE: {len(y_train_resampled)} samples \"\n",
    "              f\"({np.sum(y_train_resampled==1)} coughs, {np.sum(y_train_resampled==0)} non-coughs)\")\n",
    "        \n",
    "        # Train XGBoost\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            eval_metric='logloss',\n",
    "            verbosity=0\n",
    "        )\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "        auc = roc_auc_score(y_val, y_pred_proba)\n",
    "        print(f\"  Validation AUC: {auc:.4f}\\n\")\n",
    "        \n",
    "        results['fold_aucs'].append(auc)\n",
    "        results['fold_predictions'].append(y_pred_proba)\n",
    "        results['fold_true_labels'].append(y_val)\n",
    "        results['fold_train_subjects'].append(train_subjects)\n",
    "        results['fold_val_subjects'].append(val_subjects)\n",
    "    \n",
    "    results['mean_auc'] = np.mean(results['fold_aucs'])\n",
    "    results['std_auc'] = np.std(results['fold_aucs'])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CV Results: {results['mean_auc']:.4f} ± {results['std_auc']:.4f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ Training pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(results):\n",
    "    \"\"\"\n",
    "    Find threshold that maximizes F1 score across all folds\n",
    "    \n",
    "    Args:\n",
    "        results: Output from train_and_evaluate_cv\n",
    "    \n",
    "    Returns:\n",
    "        best_threshold: Optimal threshold\n",
    "        best_f1: F1 score at optimal threshold\n",
    "        thresholds: All tested thresholds\n",
    "        f1_scores: F1 scores for all thresholds\n",
    "    \"\"\"\n",
    "    all_preds = np.concatenate(results['fold_predictions'])\n",
    "    all_true = np.concatenate(results['fold_true_labels'])\n",
    "    \n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred_binary = (all_preds >= thresh).astype(int)\n",
    "        f1 = f1_score(all_true, y_pred_binary, zero_division=0)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    return thresholds[best_idx], f1_scores[best_idx], thresholds, f1_scores\n",
    "\n",
    "print(\"✓ Threshold optimization function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_at_threshold(results, threshold):\n",
    "    \"\"\"\n",
    "    Compute classification metrics at a specific threshold\n",
    "    \n",
    "    Args:\n",
    "        results: Output from train_and_evaluate_cv\n",
    "        threshold: Classification threshold\n",
    "    \n",
    "    Returns:\n",
    "        dict: Sensitivity, specificity, precision, F1, confusion matrix\n",
    "    \"\"\"\n",
    "    all_preds = np.concatenate(results['fold_predictions'])\n",
    "    all_true = np.concatenate(results['fold_true_labels'])\n",
    "    y_pred_binary = (all_preds >= threshold).astype(int)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(all_true, y_pred_binary).ravel()\n",
    "    \n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'sensitivity': recall_score(all_true, y_pred_binary),\n",
    "        'specificity': tn / (tn + fp),\n",
    "        'precision': precision_score(all_true, y_pred_binary, zero_division=0),\n",
    "        'f1': f1_score(all_true, y_pred_binary, zero_division=0),\n",
    "        'tp': int(tp), 'tn': int(tn), 'fp': int(fp), 'fn': int(fn)\n",
    "    }\n",
    "\n",
    "print(\"✓ Metrics computation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: IMU-Only Model\n",
    "\n",
    "Train using only 40 IMU features (accelerometer + gyroscope).\n",
    "\n",
    "**Expected**: ROC-AUC ~0.90 ± 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 1: IMU-ONLY MODEL\")\n",
    "print(\"Expected CV AUC: ~0.90 ± 0.02\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_imu = train_and_evaluate_cv(\n",
    "    X_imu, labels, subjects, \n",
    "    n_folds=N_FOLDS, \n",
    "    model_name=\"XGBoost (IMU-only)\"\n",
    ")\n",
    "\n",
    "thresh_imu, f1_imu, _, _ = find_optimal_threshold(results_imu)\n",
    "metrics_imu = compute_metrics_at_threshold(results_imu, thresh_imu)\n",
    "\n",
    "print(f\"\\nOptimal Operating Point:\")\n",
    "print(f\"  Threshold: {thresh_imu:.3f}\")\n",
    "print(f\"  Sensitivity (Recall): {metrics_imu['sensitivity']:.3f}\")\n",
    "print(f\"  Specificity: {metrics_imu['specificity']:.3f}\")\n",
    "print(f\"  Precision: {metrics_imu['precision']:.3f}\")\n",
    "print(f\"  F1 Score: {metrics_imu['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Audio-Only Model\n",
    "\n",
    "Train using only 65 audio features from the outer microphone.\n",
    "\n",
    "**Expected**: ROC-AUC ~0.92 ± 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 2: AUDIO-ONLY MODEL (Outer Microphone)\")\n",
    "print(\"Expected CV AUC: ~0.92 ± 0.01\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_audio = train_and_evaluate_cv(\n",
    "    X_audio, labels, subjects, \n",
    "    n_folds=N_FOLDS, \n",
    "    model_name=\"XGBoost (Audio-only)\"\n",
    ")\n",
    "\n",
    "thresh_audio, f1_audio, _, _ = find_optimal_threshold(results_audio)\n",
    "metrics_audio = compute_metrics_at_threshold(results_audio, thresh_audio)\n",
    "\n",
    "print(f\"\\nOptimal Operating Point:\")\n",
    "print(f\"  Threshold: {thresh_audio:.3f}\")\n",
    "print(f\"  Sensitivity (Recall): {metrics_audio['sensitivity']:.3f}\")\n",
    "print(f\"  Specificity: {metrics_audio['specificity']:.3f}\")\n",
    "print(f\"  Precision: {metrics_audio['precision']:.3f}\")\n",
    "print(f\"  F1 Score: {metrics_audio['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Multimodal Model\n",
    "\n",
    "Train using all 105 features (65 audio + 40 IMU).\n",
    "\n",
    "**Expected**: ROC-AUC ~0.96 ± 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 3: MULTIMODAL MODEL (Audio + IMU)\")\n",
    "print(\"Expected CV AUC: ~0.96 ± 0.01\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_all = train_and_evaluate_cv(\n",
    "    X_all, labels, subjects, \n",
    "    n_folds=N_FOLDS, \n",
    "    model_name=\"XGBoost (Multimodal)\"\n",
    ")\n",
    "\n",
    "thresh_all, f1_all, _, _ = find_optimal_threshold(results_all)\n",
    "metrics_all = compute_metrics_at_threshold(results_all, thresh_all)\n",
    "\n",
    "print(f\"\\nOptimal Operating Point:\")\n",
    "print(f\"  Threshold: {thresh_all:.3f}\")\n",
    "print(f\"  Sensitivity (Recall): {metrics_all['sensitivity']:.3f}\")\n",
    "print(f\"  Specificity: {metrics_all['specificity']:.3f}\")\n",
    "print(f\"  Precision: {metrics_all['precision']:.3f}\")\n",
    "print(f\"  F1 Score: {metrics_all['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Comparison of all three modalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': ['IMU-only', 'Audio-only', 'Multimodal'],\n",
    "    'ROC-AUC': [\n",
    "        f\"{results_imu['mean_auc']:.4f} ± {results_imu['std_auc']:.4f}\",\n",
    "        f\"{results_audio['mean_auc']:.4f} ± {results_audio['std_auc']:.4f}\",\n",
    "        f\"{results_all['mean_auc']:.4f} ± {results_all['std_auc']:.4f}\"\n",
    "    ],\n",
    "    'Sensitivity': [\n",
    "        f\"{metrics_imu['sensitivity']:.3f}\",\n",
    "        f\"{metrics_audio['sensitivity']:.3f}\",\n",
    "        f\"{metrics_all['sensitivity']:.3f}\"\n",
    "    ],\n",
    "    'Specificity': [\n",
    "        f\"{metrics_imu['specificity']:.3f}\",\n",
    "        f\"{metrics_audio['specificity']:.3f}\",\n",
    "        f\"{metrics_all['specificity']:.3f}\"\n",
    "    ],\n",
    "    'Precision': [\n",
    "        f\"{metrics_imu['precision']:.3f}\",\n",
    "        f\"{metrics_audio['precision']:.3f}\",\n",
    "        f\"{metrics_all['precision']:.3f}\"\n",
    "    ],\n",
    "    'F1': [\n",
    "        f\"{metrics_imu['f1']:.3f}\",\n",
    "        f\"{metrics_audio['f1']:.3f}\",\n",
    "        f\"{metrics_all['f1']:.3f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Expected from paper:\")\n",
    "print(\"  IMU-only:    0.90 ± 0.02\")\n",
    "print(\"  Audio-only:  0.92 ± 0.01\")\n",
    "print(\"  Multimodal:  0.96 ± 0.01\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: ROC Curves\n",
    "\n",
    "Plot ROC curves for all folds of each modality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (results, name, color) in enumerate([\n",
    "    (results_imu, 'IMU-only', 'blue'),\n",
    "    (results_audio, 'Audio-only', 'green'),\n",
    "    (results_all, 'Multimodal', 'red')\n",
    "]):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot each fold\n",
    "    for fold_idx in range(N_FOLDS):\n",
    "        y_true = results['fold_true_labels'][fold_idx]\n",
    "        y_pred = results['fold_predictions'][fold_idx]\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "        auc = results['fold_aucs'][fold_idx]\n",
    "        ax.plot(fpr, tpr, alpha=0.3, color=color, \n",
    "                label=f'Fold {fold_idx+1} (AUC={auc:.3f})')\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=2)\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax.set_title(f'{name}\\nMean AUC: {results[\"mean_auc\"]:.4f} ± {results[\"std_auc\"]:.4f}',\n",
    "                fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=9, loc='lower right')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ ROC curves saved to roc_curves_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Confusion Matrices\n",
    "\n",
    "Show classification results at optimal thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (metrics, name) in enumerate([\n",
    "    (metrics_imu, 'IMU-only'),\n",
    "    (metrics_audio, 'Audio-only'),\n",
    "    (metrics_all, 'Multimodal')\n",
    "]):\n",
    "    ax = axes[idx]\n",
    "    cm = np.array([[metrics['tn'], metrics['fp']], \n",
    "                   [metrics['fn'], metrics['tp']]])\n",
    "    \n",
    "    im = ax.imshow(cm, cmap='Blues', interpolation='nearest')\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(['Non-cough', 'Cough'])\n",
    "    ax.set_yticklabels(['Non-cough', 'Cough'])\n",
    "    ax.set_xlabel('Predicted', fontsize=11)\n",
    "    ax.set_ylabel('True', fontsize=11)\n",
    "    ax.set_title(f'{name}\\nF1={metrics[\"f1\"]:.3f} (thresh={metrics[\"threshold\"]:.2f})',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                   color='white' if cm[i, j] > cm.max()/2 else 'black',\n",
    "                   fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrices saved to confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: F1 Score vs Threshold\n",
    "\n",
    "Show how F1 score varies with classification threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for results, name, color, metrics in [\n",
    "    (results_imu, 'IMU-only', 'blue', metrics_imu),\n",
    "    (results_audio, 'Audio-only', 'green', metrics_audio),\n",
    "    (results_all, 'Multimodal', 'red', metrics_all)\n",
    "]:\n",
    "    thresh, best_f1, thresholds, f1_scores = find_optimal_threshold(results)\n",
    "    ax.plot(thresholds, f1_scores, \n",
    "            label=f'{name} (max F1={best_f1:.3f} @ {thresh:.2f})',\n",
    "            color=color, linewidth=2)\n",
    "    ax.axvline(thresh, color=color, linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Classification Threshold', fontsize=12)\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.set_title('F1 Score vs Classification Threshold', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('f1_vs_threshold.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ F1 vs threshold plot saved to f1_vs_threshold.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 4: Per-Fold AUC Comparison\n",
    "\n",
    "Compare AUC scores across all folds for each modality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(N_FOLDS)\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, results_imu['fold_aucs'], width, \n",
    "       label='IMU-only', color='blue', alpha=0.7)\n",
    "ax.bar(x, results_audio['fold_aucs'], width, \n",
    "       label='Audio-only', color='green', alpha=0.7)\n",
    "ax.bar(x + width, results_all['fold_aucs'], width, \n",
    "       label='Multimodal', color='red', alpha=0.7)\n",
    "\n",
    "# Add mean lines\n",
    "ax.axhline(results_imu['mean_auc'], color='blue', linestyle='--', \n",
    "          alpha=0.5, linewidth=2, label=f'IMU mean: {results_imu[\"mean_auc\"]:.3f}')\n",
    "ax.axhline(results_audio['mean_auc'], color='green', linestyle='--', \n",
    "          alpha=0.5, linewidth=2, label=f'Audio mean: {results_audio[\"mean_auc\"]:.3f}')\n",
    "ax.axhline(results_all['mean_auc'], color='red', linestyle='--', \n",
    "          alpha=0.5, linewidth=2, label=f'Multimodal mean: {results_all[\"mean_auc\"]:.3f}')\n",
    "\n",
    "ax.set_xlabel('Fold', fontsize=12)\n",
    "ax.set_ylabel('ROC-AUC', fontsize=12)\n",
    "ax.set_title('Per-Fold AUC Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'Fold {i+1}' for i in range(N_FOLDS)])\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "ax.set_ylim(0.8, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_fold_auc.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Per-fold AUC comparison saved to per_fold_auc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Successfully reproduced the paper's XGBoost training pipeline with three modality configurations.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Multimodal fusion** (audio + IMU) achieves best performance (~0.96 AUC)\n",
    "2. **Audio alone** is strong (~0.92 AUC) - outer microphone captures cough signatures well\n",
    "3. **IMU adds value** - provides ~4% AUC improvement when combined with audio\n",
    "4. **Subject-wise CV** ensures generalization to new subjects\n",
    "5. **Class balancing** with SMOTE improves performance on imbalanced data\n",
    "\n",
    "### Model Comparison\n",
    "\n",
    "- **IMU-only**: Good baseline using motion sensors alone (useful for privacy-preserving scenarios)\n",
    "- **Audio-only**: Strong performance, but may struggle in noisy environments\n",
    "- **Multimodal**: Best of both worlds - robust across conditions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Feature selection**: Use RFECV to reduce feature count while maintaining performance\n",
    "2. **Hyperparameter tuning**: RandomizedSearchCV or Optuna for optimal XGBoost parameters\n",
    "3. **Explainability**: SHAP analysis to understand which features drive predictions\n",
    "4. **Final validation**: Test on held-out subjects for unbiased performance estimate\n",
    "5. **Edge deployment**: Model quantization and optimization for resource-constrained devices\n",
    "6. **Real-time inference**: Implement sliding window approach for continuous monitoring\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `extracted_features.npz`: Cached features (can be reloaded to skip extraction)\n",
    "- `roc_curves_comparison.png`: ROC curves for all modalities\n",
    "- `confusion_matrices.png`: Classification results at optimal thresholds\n",
    "- `f1_vs_threshold.png`: F1 score sensitivity to threshold choice\n",
    "- `per_fold_auc.png`: Cross-validation stability analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Persistence for Interactive Testing\n",
    "\n",
    "Save trained models to disk for use in the Interactive Model Testing notebook.\n",
    "\n",
    "This trains final models on **all available data** (with SMOTE balancing) and saves:\n",
    "- Trained XGBoost model\n",
    "- StandardScaler fitted on full dataset\n",
    "- Optimal classification threshold (from CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models for interactive testing\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_DIR = Path(\"./models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Store models in memory for SHAP analysis\n",
    "trained_models = {}\n",
    "\n",
    "print(\"Training final models on all data for deployment...\\n\")\n",
    "\n",
    "for modality, X, thresh in [\n",
    "    ('imu', X_imu, thresh_imu),\n",
    "    ('audio', X_audio, thresh_audio),\n",
    "    ('multimodal', X_all, thresh_all)\n",
    "]:\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply SMOTE for balanced training\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X_scaled, labels)\n",
    "    \n",
    "    # Train final model\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_balanced, y_balanced)\n",
    "    \n",
    "    # Store in memory for SHAP\n",
    "    trained_models[modality] = {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'X_balanced': X_balanced,\n",
    "        'y_balanced': y_balanced,\n",
    "        'threshold': thresh\n",
    "    }\n",
    "    \n",
    "    # Save model + scaler + threshold\n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'threshold': thresh\n",
    "    }\n",
    "    \n",
    "    save_path = MODEL_DIR / f'xgb_{modality}.pkl'\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    \n",
    "    print(f\"✓ Saved {modality} model to {save_path}\")\n",
    "\n",
    "print(f\"\\n✓ All models saved to {MODEL_DIR}/\")\n",
    "print(\"  These can now be loaded in Interactive_Model_Testing.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Explainability Analysis\n",
    "\n",
    "Understand which features drive cough detection predictions using SHAP (SHapley Additive exPlanations).\n",
    "\n",
    "SHAP provides:\n",
    "- **Global explanations**: Which features are most important overall\n",
    "- **Local explanations**: How each feature contributes to individual predictions\n",
    "- **Feature interactions**: How features work together\n",
    "\n",
    "We use the final models trained in the previous section (on full dataset with SMOTE balancing) to compute SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check SHAP installation\n",
    "try:\n",
    "    import shap\n",
    "    print(f\"✓ SHAP version: {shap.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"✗ SHAP not installed. Install with: pip install shap\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "# Import feature name functions from features.py\n",
    "from features import get_feature_names\n",
    "\n",
    "# Create output directory for SHAP plots\n",
    "SHAP_OUTPUT_DIR = Path(\"./shap\")\n",
    "SHAP_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "print(f\"✓ SHAP output directory: {SHAP_OUTPUT_DIR}\")\n",
    "\n",
    "# Verify feature name generation\n",
    "print(f\"\\nFeature name counts:\")\n",
    "print(f\"  Audio: {len(get_feature_names('audio'))}\")\n",
    "print(f\"  IMU: {len(get_feature_names('imu'))}\")\n",
    "print(f\"  Multimodal: {len(get_feature_names('multimodal'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values using the trained models from cell 33\n",
    "shap_results = {}\n",
    "\n",
    "for modality, modality_name in [\n",
    "    ('imu', 'IMU-only'),\n",
    "    ('audio', 'Audio-only'),\n",
    "    ('multimodal', 'Multimodal')\n",
    "]:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Computing SHAP values: {modality_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    # Retrieve stored model and data\n",
    "    stored = trained_models[modality]\n",
    "    model = stored['model']\n",
    "    X_balanced = stored['X_balanced']\n",
    "    y_balanced = stored['y_balanced']\n",
    "\n",
    "    # Get feature names from features.py\n",
    "    feature_names = get_feature_names(modality)\n",
    "\n",
    "    print(f\"Dataset: {len(X_balanced)} samples (SMOTE-balanced)\")\n",
    "    print(f\"Features: {X_balanced.shape[1]} ({modality_name})\")\n",
    "    print(f\"Computing SHAP values using TreeExplainer...\")\n",
    "\n",
    "    # Compute SHAP values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_balanced)\n",
    "\n",
    "    print(f\"✓ SHAP values computed\")\n",
    "\n",
    "    # Store for visualization\n",
    "    shap_results[modality] = {\n",
    "        'shap_values': shap_values,\n",
    "        'model': model,\n",
    "        'X_balanced': X_balanced,\n",
    "        'y_balanced': y_balanced,\n",
    "        'feature_names': feature_names\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SHAP computation complete for all modalities\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SHAP visualizations for each modality\n",
    "\n",
    "for modality_key, modality_name in [\n",
    "    ('imu', 'IMU-only'),\n",
    "    ('audio', 'Audio-only'),\n",
    "    ('multimodal', 'Multimodal')\n",
    "]:\n",
    "    print(f\"\\nGenerating SHAP plots for {modality_name}...\")\n",
    "\n",
    "    result = shap_results[modality_key]\n",
    "    shap_values = result['shap_values']\n",
    "    X_balanced = result['X_balanced']\n",
    "    feature_names = result['feature_names']\n",
    "\n",
    "    # =====================================================================\n",
    "    # Visualization 1: Summary Bar Plot (Top 20 features)\n",
    "    # =====================================================================\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        X_balanced,\n",
    "        feature_names=feature_names,\n",
    "        plot_type=\"bar\",\n",
    "        max_display=20,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'SHAP Feature Importance: {modality_name}',\n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    save_path = SHAP_OUTPUT_DIR / f'summary_bar_{modality_key}.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  ✓ Saved {save_path}\")\n",
    "\n",
    "    # =====================================================================\n",
    "    # Visualization 2: Beeswarm Plot (Top 20 features)\n",
    "    # =====================================================================\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        X_balanced,\n",
    "        feature_names=feature_names,\n",
    "        max_display=20,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'SHAP Value Distribution: {modality_name}',\n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    save_path = SHAP_OUTPUT_DIR / f'summary_beeswarm_{modality_key}.png'\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"  ✓ Saved {save_path}\")\n",
    "\n",
    "    # =====================================================================\n",
    "    # Visualization 3: Dependence Plots (Top 3 features)\n",
    "    # =====================================================================\n",
    "    # Compute feature importance to identify top features\n",
    "    feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "    top_indices = np.argsort(feature_importance)[-3:][::-1]  # Top 3\n",
    "\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        feature_name = feature_names[idx]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        shap.dependence_plot(\n",
    "            idx,\n",
    "            shap_values,\n",
    "            X_balanced,\n",
    "            feature_names=feature_names,\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(f'SHAP Dependence: {feature_name} ({modality_name})',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        save_path = SHAP_OUTPUT_DIR / f'dependence_{rank}_{modality_key}.png'\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  ✓ Saved {save_path} (feature: {feature_name})\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"All SHAP visualizations saved to {SHAP_OUTPUT_DIR}/\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Display summary of top features per modality\n",
    "print(\"\\nTop 5 Most Important Features per Modality:\\n\")\n",
    "\n",
    "for modality_key, modality_name in [\n",
    "    ('imu', 'IMU-only'),\n",
    "    ('audio', 'Audio-only'),\n",
    "    ('multimodal', 'Multimodal')\n",
    "]:\n",
    "    result = shap_results[modality_key]\n",
    "    shap_values = result['shap_values']\n",
    "    feature_names = result['feature_names']\n",
    "\n",
    "    feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "    top_5_indices = np.argsort(feature_importance)[-5:][::-1]\n",
    "\n",
    "    print(f\"{modality_name}:\")\n",
    "    for rank, idx in enumerate(top_5_indices, 1):\n",
    "        print(f\"  {rank}. {feature_names[idx]}: {feature_importance[idx]:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"✓ SHAP analysis complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge-ai-cough-count",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
