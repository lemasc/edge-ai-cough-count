{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Cough Detection Model Tester\n",
    "\n",
    "This notebook provides an interactive Gradio interface for testing XGBoost cough detection models trained on multimodal biosignals (audio + IMU).\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Load pre-trained models**: IMU-only, Audio-only, and Multimodal classifiers\n",
    "- **Test on dataset recordings**: Select from public_dataset with ground truth comparison\n",
    "- **Upload custom files**: Test on your own WAV audio and CSV IMU data\n",
    "- **Interactive visualizations**: Waveforms with color-coded detections\n",
    "- **Threshold adjustment**: Fine-tune classification threshold in real-time\n",
    "- **Event-based metrics**: TP/FP/FN counts with sensitivity, precision, F1 scores\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**IMPORTANT**: You must first run `Model_Training_XGBoost.ipynb` to completion to generate the saved models in `models/`.\n",
    "\n",
    "The training notebook should create:\n",
    "- `models/xgb_imu.pkl`\n",
    "- `models/xgb_audio.pkl`\n",
    "- `models/xgb_multimodal.pkl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for required dependencies\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import gradio as gr\n",
    "    import xgboost\n",
    "    import joblib\n",
    "    print(\"✓ All required dependencies installed\")\n",
    "    print(f\"  - gradio version: {gr.__version__}\")\n",
    "    print(f\"  - xgboost version: {xgboost.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Missing dependency: {e}\")\n",
    "    print(\"\\nInstall with: uv add gradio xgboost joblib\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from helpers import *\n",
    "from features import extract_audio_features, extract_imu_features\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "FS_AUDIO_CONST = 16000  # Audio sampling frequency\n",
    "FS_IMU_CONST = 100      # IMU sampling frequency\n",
    "WINDOW_LEN = 0.4        # Window length in seconds\n",
    "HOP_SIZE = 0.05         # Default hop size for sliding window (50ms)\n",
    "\n",
    "# Locate dataset folder\n",
    "kaggle_dataset_dir = '/kaggle/input/edge-ai-cough-count'\n",
    "base_dir = kaggle_dataset_dir if os.path.exists(kaggle_dataset_dir) else \"..\"\n",
    "data_folder = base_dir + '/public_dataset/'\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    raise FileNotFoundError(\n",
    "        \"Cannot find public_dataset/. Please download from: \"\n",
    "        \"https://zenodo.org/record/7562332\"\n",
    "    )\n",
    "\n",
    "# Locate models directory\n",
    "MODEL_DIR = Path(\"./models\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Audio FS: {FS_AUDIO_CONST} Hz\")\n",
    "print(f\"  IMU FS: {FS_IMU_CONST} Hz\")\n",
    "print(f\"  Window length: {WINDOW_LEN}s\")\n",
    "print(f\"  Dataset folder: {data_folder if data_folder else 'Not found'}\")\n",
    "print(f\"  Models directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Model Loading System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_models():\n",
    "    \"\"\"\n",
    "    Load all three trained models from disk.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with keys 'imu', 'audio', 'multimodal'\n",
    "              Each value is {'model': XGBClassifier, 'scaler': StandardScaler, 'threshold': float}\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    for modality in ['imu', 'audio', 'multimodal']:\n",
    "        model_path = MODEL_DIR / f'xgb_{modality}.pkl'\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"\\n{'='*70}\\n\"\n",
    "                f\"ERROR: Model file not found: {model_path}\\n\\n\"\n",
    "                f\"Please run Model_Training_XGBoost.ipynb first to train and save models.\\n\"\n",
    "                f\"The training notebook should create the following files:\\n\"\n",
    "                f\"  - models/xgb_imu.pkl\\n\"\n",
    "                f\"  - models/xgb_audio.pkl\\n\"\n",
    "                f\"  - models/xgb_multimodal.pkl\\n\"\n",
    "                f\"{'='*70}\"\n",
    "            )\n",
    "        \n",
    "        with open(model_path, 'rb') as f:\n",
    "            models[modality] = pickle.load(f)\n",
    "        \n",
    "        print(f\"✓ Loaded {modality} model from {model_path}\")\n",
    "        print(f\"  Threshold: {models[modality]['threshold']:.3f}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Load models\n",
    "try:\n",
    "    MODELS = load_trained_models()\n",
    "    print(f\"\\n✓ All models loaded successfully\")\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    MODELS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Feature Extraction Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_window(audio_window, imu_window, modality='multimodal'):\n",
    "    \"\"\"\n",
    "    Extract features from a single window of audio and IMU data.\n",
    "    \n",
    "    Args:\n",
    "        audio_window: (N_audio,) audio samples\n",
    "        imu_window: (N_imu, 6) IMU samples\n",
    "        modality: 'imu', 'audio', or 'multimodal'\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Feature vector\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    if modality in ['audio', 'multimodal']:\n",
    "        audio_feat = extract_audio_features(audio_window, fs=FS_AUDIO_CONST)\n",
    "        # Handle NaN/Inf\n",
    "        audio_feat = np.nan_to_num(audio_feat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        features.append(audio_feat)\n",
    "    \n",
    "    if modality in ['imu', 'multimodal']:\n",
    "        imu_feat = extract_imu_features(imu_window)\n",
    "        # Handle NaN/Inf\n",
    "        imu_feat = np.nan_to_num(imu_feat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        features.append(imu_feat)\n",
    "    \n",
    "    return np.concatenate(features)\n",
    "\n",
    "print(\"✓ Feature extraction utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Sliding Window Prediction Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_predict(audio, imu, model_data, modality='multimodal', \n",
    "                          window_len=0.4, hop_size=0.05, threshold=None):\n",
    "    \"\"\"\n",
    "    Apply model to continuous recording using sliding windows.\n",
    "    \n",
    "    Args:\n",
    "        audio: (N_audio,) audio samples\n",
    "        imu: (N_imu, 6) IMU samples\n",
    "        model_data: Dict with 'model', 'scaler', 'threshold'\n",
    "        modality: 'imu', 'audio', or 'multimodal'\n",
    "        window_len: Window length in seconds\n",
    "        hop_size: Hop size in seconds\n",
    "        threshold: Classification threshold (None = use optimal from model)\n",
    "    \n",
    "    Returns:\n",
    "        predictions: List of (start_time, end_time, probability) tuples\n",
    "        all_probs: Array of probabilities for each window\n",
    "        window_times: Array of window center times\n",
    "    \"\"\"\n",
    "    model = model_data['model']\n",
    "    scaler = model_data['scaler']\n",
    "    if threshold is None:\n",
    "        threshold = model_data['threshold']\n",
    "    \n",
    "    # Calculate window and hop in samples\n",
    "    audio_win_samples = int(window_len * FS_AUDIO_CONST)\n",
    "    audio_hop_samples = int(hop_size * FS_AUDIO_CONST)\n",
    "    imu_win_samples = int(window_len * FS_IMU_CONST)\n",
    "    imu_hop_samples = int(hop_size * FS_IMU_CONST)\n",
    "    \n",
    "    # Extract windows\n",
    "    n_windows = (len(audio) - audio_win_samples) // audio_hop_samples + 1\n",
    "    features_list = []\n",
    "    window_times = []\n",
    "    \n",
    "    for i in range(n_windows):\n",
    "        audio_start = i * audio_hop_samples\n",
    "        audio_end = audio_start + audio_win_samples\n",
    "        imu_start = i * imu_hop_samples\n",
    "        imu_end = imu_start + imu_win_samples\n",
    "        \n",
    "        if audio_end > len(audio) or imu_end > len(imu):\n",
    "            break\n",
    "        \n",
    "        audio_window = audio[audio_start:audio_end]\n",
    "        imu_window = imu[imu_start:imu_end, :]\n",
    "        \n",
    "        features = extract_features_for_window(audio_window, imu_window, modality)\n",
    "        features_list.append(features)\n",
    "        \n",
    "        # Window center time\n",
    "        center_time = (audio_start + audio_win_samples / 2) / FS_AUDIO_CONST\n",
    "        window_times.append(center_time)\n",
    "    \n",
    "    # Batch predict\n",
    "    X = np.array(features_list)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    probs = model.predict_proba(X_scaled)[:, 1]\n",
    "    \n",
    "    # Convert to event-based predictions\n",
    "    predictions = []\n",
    "    for i, (prob, center) in enumerate(zip(probs, window_times)):\n",
    "        if prob >= threshold:\n",
    "            start = center - window_len / 2\n",
    "            end = center + window_len / 2\n",
    "            predictions.append((start, end, prob))\n",
    "    \n",
    "    return predictions, probs, np.array(window_times)\n",
    "\n",
    "print(\"✓ Sliding window prediction engine ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_detections(predictions, gap_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Merge consecutive detections that are close together.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of (start, end, prob) tuples\n",
    "        gap_threshold: Maximum gap between events to merge (seconds)\n",
    "    \n",
    "    Returns:\n",
    "        merged: List of (start, end, max_prob) tuples\n",
    "    \"\"\"\n",
    "    if not predictions:\n",
    "        return []\n",
    "    \n",
    "    # Sort by start time\n",
    "    sorted_preds = sorted(predictions, key=lambda x: x[0])\n",
    "    \n",
    "    merged = []\n",
    "    current_start, current_end, current_prob = sorted_preds[0]\n",
    "    \n",
    "    for start, end, prob in sorted_preds[1:]:\n",
    "        # If gap is small, merge\n",
    "        if start - current_end <= gap_threshold:\n",
    "            current_end = max(current_end, end)\n",
    "            current_prob = max(current_prob, prob)\n",
    "        else:\n",
    "            # Save current event and start new one\n",
    "            merged.append((current_start, current_end, current_prob))\n",
    "            current_start, current_end, current_prob = start, end, prob\n",
    "    \n",
    "    # Add last event\n",
    "    merged.append((current_start, current_end, current_prob))\n",
    "    \n",
    "    return merged\n",
    "\n",
    "print(\"✓ Detection merging ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(audio, imu, predictions, ground_truth=None, figsize=(14, 8)):\n",
    "    \"\"\"\n",
    "    Plot waveform with highlighted detections.\n",
    "    \n",
    "    Args:\n",
    "        audio: (N,) audio samples\n",
    "        imu: (M, 6) IMU samples\n",
    "        predictions: List of (start, end, prob) tuples\n",
    "        ground_truth: Optional list of (start, end) tuples for ground truth\n",
    "        figsize: Figure size\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 1, figsize=figsize)\n",
    "    \n",
    "    # Audio time axis\n",
    "    audio_time = np.arange(len(audio)) / FS_AUDIO_CONST\n",
    "    \n",
    "    # IMU time axis\n",
    "    imu_time = np.arange(len(imu)) / FS_IMU_CONST\n",
    "    \n",
    "    # Plot audio\n",
    "    axes[0].plot(audio_time, audio, linewidth=0.5, color='black', alpha=0.7)\n",
    "    axes[0].set_ylabel('Amplitude', fontsize=11)\n",
    "    axes[0].set_title('Audio Waveform (Outer Microphone)', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot IMU (Z-axis, negated)\n",
    "    axes[1].plot(imu_time, -imu[:, 2], linewidth=1, color='blue', alpha=0.7)\n",
    "    axes[1].set_xlabel('Time (s)', fontsize=11)\n",
    "    axes[1].set_ylabel('Acceleration', fontsize=11)\n",
    "    axes[1].set_title('IMU Accelerometer Z (negated)', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # Add predictions as red spans\n",
    "    for start, end, prob in predictions:\n",
    "        for ax in axes:\n",
    "            ax.axvspan(start, end, alpha=0.3, color='red', label='Prediction' if start == predictions[0][0] else '')\n",
    "            # Add confidence label on audio plot\n",
    "            if ax == axes[0]:\n",
    "                mid = (start + end) / 2\n",
    "                ax.text(mid, ax.get_ylim()[1] * 0.9, f'{prob:.2f}', \n",
    "                       ha='center', va='top', fontsize=9, color='red', fontweight='bold')\n",
    "    \n",
    "    # Add ground truth as green spans\n",
    "    if ground_truth:\n",
    "        for start, end in ground_truth:\n",
    "            for ax in axes:\n",
    "                ax.axvspan(start, end, alpha=0.2, color='green', \n",
    "                          label='Ground Truth' if start == ground_truth[0][0] else '')\n",
    "    \n",
    "    # Add legends\n",
    "    for ax in axes:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        if handles:\n",
    "            # Remove duplicates\n",
    "            by_label = dict(zip(labels, handles))\n",
    "            ax.legend(by_label.values(), by_label.keys(), loc='upper right', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"✓ Prediction plotting ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Metrics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_event_metrics(predictions, ground_truth, tolerance_start=0.25, \n",
    "                         tolerance_end=0.25, min_overlap=0.1):\n",
    "    \"\"\"\n",
    "    Compute event-based metrics (TP, FP, FN).\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of (start, end, prob) tuples\n",
    "        ground_truth: List of (start, end) tuples\n",
    "        tolerance_start: Start tolerance in seconds\n",
    "        tolerance_end: End tolerance in seconds\n",
    "        min_overlap: Minimum overlap ratio to count as TP\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metrics including TP, FP, FN, Sensitivity, Precision, F1\n",
    "    \"\"\"\n",
    "    if not ground_truth:\n",
    "        # No ground truth - just return detection count\n",
    "        return {\n",
    "            'TP': None,\n",
    "            'FP': None,\n",
    "            'FN': None,\n",
    "            'Sensitivity': None,\n",
    "            'Precision': None,\n",
    "            'F1': None,\n",
    "            'Total_Detections': len(predictions),\n",
    "            'Ground_Truth_Count': 0\n",
    "        }\n",
    "    \n",
    "    # Convert to start/end arrays\n",
    "    pred_starts = np.array([p[0] for p in predictions])\n",
    "    pred_ends = np.array([p[1] for p in predictions])\n",
    "    gt_starts = np.array([g[0] for g in ground_truth])\n",
    "    gt_ends = np.array([g[1] for g in ground_truth])\n",
    "    \n",
    "    # Track which GT events have been matched\n",
    "    gt_matched = np.zeros(len(ground_truth), dtype=bool)\n",
    "    tp_count = 0\n",
    "    fp_count = 0\n",
    "    \n",
    "    # For each prediction, check if it matches a GT event\n",
    "    for pred_start, pred_end in zip(pred_starts, pred_ends):\n",
    "        matched = False\n",
    "        \n",
    "        for i, (gt_start, gt_end) in enumerate(zip(gt_starts, gt_ends)):\n",
    "            if gt_matched[i]:\n",
    "                continue\n",
    "            \n",
    "            # Check overlap\n",
    "            overlap_start = max(pred_start, gt_start - tolerance_start)\n",
    "            overlap_end = min(pred_end, gt_end + tolerance_end)\n",
    "            \n",
    "            if overlap_end > overlap_start:\n",
    "                overlap_duration = overlap_end - overlap_start\n",
    "                gt_duration = gt_end - gt_start\n",
    "                \n",
    "                if overlap_duration / gt_duration >= min_overlap:\n",
    "                    # Match!\n",
    "                    tp_count += 1\n",
    "                    gt_matched[i] = True\n",
    "                    matched = True\n",
    "                    break\n",
    "        \n",
    "        if not matched:\n",
    "            fp_count += 1\n",
    "    \n",
    "    # Unmatched GT events are false negatives\n",
    "    fn_count = np.sum(~gt_matched)\n",
    "    \n",
    "    # Compute metrics\n",
    "    sensitivity = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0\n",
    "    precision = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0\n",
    "    f1 = 2 * precision * sensitivity / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'TP': int(tp_count),\n",
    "        'FP': int(fp_count),\n",
    "        'FN': int(fn_count),\n",
    "        'Sensitivity': float(sensitivity),\n",
    "        'Precision': float(precision),\n",
    "        'F1': float(f1),\n",
    "        'Total_Detections': len(predictions),\n",
    "        'Ground_Truth_Count': len(ground_truth)\n",
    "    }\n",
    "\n",
    "print(\"✓ Metrics computation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Data Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_recording(subject_id, trial, movement, noise, sound):\n",
    "    \"\"\"\n",
    "    Load a recording from the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        audio: (N,) audio samples (outer mic)\n",
    "        imu_data: (M, 6) IMU samples\n",
    "        ground_truth: List of (start, end) or None\n",
    "    \"\"\"\n",
    "    # Load audio (outer mic only)\n",
    "    audio_air, _ = load_audio(data_folder, subject_id, trial, movement, noise, sound)\n",
    "    \n",
    "    # Load IMU\n",
    "    imu_obj = load_imu(data_folder, subject_id, trial, movement, noise, sound)\n",
    "    imu_data = imu_obj.make_segment_df().values\n",
    "    \n",
    "    # Load ground truth if cough recording\n",
    "    ground_truth = None\n",
    "    if sound == Sound.COUGH:\n",
    "        try:\n",
    "            start_times, end_times = load_annotation(data_folder, subject_id, trial, movement, noise, sound)\n",
    "            ground_truth = list(zip(start_times, end_times))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return audio_air, imu_data, ground_truth\n",
    "\n",
    "def load_uploaded_wav(file_obj):\n",
    "    \"\"\"\n",
    "    Load WAV from Gradio file upload.\n",
    "    \n",
    "    Returns:\n",
    "        audio: (N,) normalized audio samples\n",
    "    \"\"\"\n",
    "    fs, audio = wavfile.read(file_obj.name)\n",
    "    if fs != 16000:\n",
    "        raise ValueError(f\"Audio must be 16 kHz, got {fs} Hz\")\n",
    "    \n",
    "    # Normalize by 2^29 (matching dataset preprocessing)\n",
    "    audio = audio / (1 << 29)\n",
    "    return audio\n",
    "\n",
    "def load_uploaded_imu(file_obj):\n",
    "    \"\"\"\n",
    "    Load IMU CSV from Gradio file upload.\n",
    "    \n",
    "    Returns:\n",
    "        imu: (N, 6) IMU samples\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_obj.name)\n",
    "    required_cols = ['Accel x', 'Accel y', 'Accel z', 'Gyro Y', 'Gyro P', 'Gyro R']\n",
    "    \n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        raise ValueError(f\"IMU CSV must contain: {required_cols}\")\n",
    "    \n",
    "    return df[required_cols].values\n",
    "\n",
    "print(\"✓ Data loading utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Main Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(data_source, subject_id, trial, movement, noise, sound,\n",
    "                  audio_file, imu_file, modality, threshold_override):\n",
    "    \"\"\"\n",
    "    Main prediction function called by Gradio interface.\n",
    "    \"\"\"\n",
    "    if MODELS is None:\n",
    "        return None, {\"Error\": \"Models not loaded\"}, pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        # Load data based on source\n",
    "        if data_source == \"Dataset Selector\":\n",
    "            if not data_folder:\n",
    "                return None, {\"Error\": \"Dataset not found\"}, pd.DataFrame()\n",
    "            \n",
    "            # Convert dropdown values to Enum\n",
    "            trial_enum = Trial(trial)\n",
    "            mov_enum = Movement(movement.lower())\n",
    "            noise_enum = Noise(noise.lower().replace(' ', '_'))\n",
    "            sound_enum = Sound(sound.lower().replace(' ', '_'))\n",
    "            \n",
    "            audio, imu, ground_truth = load_dataset_recording(\n",
    "                subject_id, trial_enum, mov_enum, noise_enum, sound_enum\n",
    "            )\n",
    "        else:  # Upload Files\n",
    "            if audio_file is None or imu_file is None:\n",
    "                return None, {\"Error\": \"Please upload both audio and IMU files\"}, pd.DataFrame()\n",
    "            \n",
    "            audio = load_uploaded_wav(audio_file)\n",
    "            imu = load_uploaded_imu(imu_file)\n",
    "            ground_truth = None\n",
    "        \n",
    "        # Map modality to model key\n",
    "        modality_map = {\n",
    "            \"IMU-only\": \"imu\",\n",
    "            \"Audio-only\": \"audio\",\n",
    "            \"Multimodal\": \"multimodal\"\n",
    "        }\n",
    "        model_key = modality_map[modality]\n",
    "        model_data = MODELS[model_key]\n",
    "        \n",
    "        # Override threshold if specified (0.0 means use optimal)\n",
    "        threshold = model_data['threshold'] if threshold_override == 0.0 else threshold_override\n",
    "        \n",
    "        # Run prediction\n",
    "        raw_predictions, probs, window_times = sliding_window_predict(\n",
    "            audio, imu, model_data, modality=model_key, threshold=threshold\n",
    "        )\n",
    "        \n",
    "        # Merge detections\n",
    "        predictions = merge_detections(raw_predictions, gap_threshold=0.3)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = compute_event_metrics(predictions, ground_truth)\n",
    "        \n",
    "        # Add threshold info to metrics\n",
    "        metrics['Threshold_Used'] = float(threshold)\n",
    "        metrics['Is_Optimal_Threshold'] = (threshold_override == 0.0)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig = plot_predictions(audio, imu, predictions, ground_truth)\n",
    "        \n",
    "        # Create events table\n",
    "        if predictions:\n",
    "            events_df = pd.DataFrame([\n",
    "                {'Start (s)': f'{s:.2f}', 'End (s)': f'{e:.2f}', 'Confidence': f'{p:.3f}'}\n",
    "                for s, e, p in predictions\n",
    "            ])\n",
    "        else:\n",
    "            events_df = pd.DataFrame({'Message': ['No coughs detected']})\n",
    "        \n",
    "        return fig, metrics, events_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_msg = f\"Error: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        return None, {\"Error\": error_msg}, pd.DataFrame()\n",
    "\n",
    "print(\"✓ Main prediction function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset parameters for dropdowns\n",
    "if data_folder:\n",
    "    subject_ids = [d for d in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, d))]\n",
    "    subject_ids = sorted(subject_ids)\n",
    "else:\n",
    "    subject_ids = []\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"Interactive Cough Detection Model Tester\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Interactive Cough Detection Model Tester\n",
    "        \n",
    "        Test XGBoost cough detection models on multimodal biosignals (audio + IMU).\n",
    "        \n",
    "        **Instructions:**\n",
    "        1. Choose data source: Dataset recordings or upload your own files\n",
    "        2. Select model: IMU-only, Audio-only, or Multimodal\n",
    "        3. Adjust threshold if needed (0 = use optimal from training)\n",
    "        4. Click \"Run Prediction\" to see results\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            # Data source selector\n",
    "            data_source = gr.Radio(\n",
    "                choices=[\"Dataset Selector\", \"Upload Files\"],\n",
    "                value=\"Dataset Selector\" if data_folder else \"Upload Files\",\n",
    "                label=\"Data Source\"\n",
    "            )\n",
    "            \n",
    "            # Dataset selector (visible when Dataset Selector is chosen)\n",
    "            with gr.Group(visible=(data_folder is not None)) as dataset_group:\n",
    "                gr.Markdown(\"### Dataset Recording\")\n",
    "                subject_dropdown = gr.Dropdown(\n",
    "                    choices=subject_ids,\n",
    "                    value=subject_ids[0] if subject_ids else None,\n",
    "                    label=\"Subject ID\"\n",
    "                )\n",
    "                trial_dropdown = gr.Dropdown(\n",
    "                    choices=[\"1\", \"2\", \"3\"],\n",
    "                    value=\"1\",\n",
    "                    label=\"Trial\"\n",
    "                )\n",
    "                movement_dropdown = gr.Dropdown(\n",
    "                    choices=[\"Sit\", \"Walk\"],\n",
    "                    value=\"Sit\",\n",
    "                    label=\"Movement\"\n",
    "                )\n",
    "                noise_dropdown = gr.Dropdown(\n",
    "                    choices=[\"Nothing\", \"Music\", \"Someone else cough\", \"Traffic\"],\n",
    "                    value=\"Nothing\",\n",
    "                    label=\"Background Noise\"\n",
    "                )\n",
    "                sound_dropdown = gr.Dropdown(\n",
    "                    choices=[\"Cough\", \"Laugh\", \"Deep breathing\", \"Throat clearing\"],\n",
    "                    value=\"Cough\",\n",
    "                    label=\"Sound Type\"\n",
    "                )\n",
    "            \n",
    "            # File upload (visible when Upload Files is chosen)\n",
    "            with gr.Group(visible=(data_folder is None)) as upload_group:\n",
    "                gr.Markdown(\"### Upload Files\")\n",
    "                audio_upload = gr.File(\n",
    "                    label=\"Audio WAV (16 kHz)\",\n",
    "                    file_types=[\".wav\"]\n",
    "                )\n",
    "                imu_upload = gr.File(\n",
    "                    label=\"IMU CSV (100 Hz)\",\n",
    "                    file_types=[\".csv\"]\n",
    "                )\n",
    "                gr.Markdown(\n",
    "                    \"*CSV must contain: Accel x, Accel y, Accel z, Gyro Y, Gyro P, Gyro R*\"\n",
    "                )\n",
    "            \n",
    "            # Toggle visibility based on data source\n",
    "            def toggle_data_source(choice):\n",
    "                if choice == \"Dataset Selector\":\n",
    "                    return gr.update(visible=True), gr.update(visible=False)\n",
    "                else:\n",
    "                    return gr.update(visible=False), gr.update(visible=True)\n",
    "            \n",
    "            data_source.change(\n",
    "                toggle_data_source,\n",
    "                inputs=[data_source],\n",
    "                outputs=[dataset_group, upload_group]\n",
    "            )\n",
    "            \n",
    "            # Model selection\n",
    "            gr.Markdown(\"### Model Settings\")\n",
    "            modality_radio = gr.Radio(\n",
    "                choices=[\"IMU-only\", \"Audio-only\", \"Multimodal\"],\n",
    "                value=\"Multimodal\",\n",
    "                label=\"Model\"\n",
    "            )\n",
    "            \n",
    "            # Display optimal threshold for selected model\n",
    "            if MODELS is not None:\n",
    "                optimal_thresh_display = gr.Markdown(\n",
    "                    f\"**Optimal Threshold:** {MODELS['multimodal']['threshold']:.3f}\"\n",
    "                )\n",
    "            else:\n",
    "                optimal_thresh_display = gr.Markdown(\"**Optimal Threshold:** Models not loaded\")\n",
    "            \n",
    "            # Update optimal threshold display when model changes\n",
    "            def update_optimal_threshold(modality):\n",
    "                if MODELS is None:\n",
    "                    return \"**Optimal Threshold:** Models not loaded\"\n",
    "                model_key = modality.lower().replace('-only', '')\n",
    "                thresh = MODELS[model_key]['threshold']\n",
    "                return f\"**Optimal Threshold:** {thresh:.3f}\"\n",
    "            \n",
    "            modality_radio.change(\n",
    "                update_optimal_threshold,\n",
    "                inputs=[modality_radio],\n",
    "                outputs=[optimal_thresh_display]\n",
    "            )\n",
    "            \n",
    "            threshold_slider = gr.Slider(\n",
    "                minimum=0.0,\n",
    "                maximum=1.0,\n",
    "                value=0.0,\n",
    "                step=0.05,\n",
    "                label=\"Threshold Override\",\n",
    "                info=\"Set to 0.0 to use optimal threshold above, or override with custom value\"\n",
    "            )\n",
    "            \n",
    "            # Run button\n",
    "            run_btn = gr.Button(\n",
    "                \"Run Prediction\",\n",
    "                variant=\"primary\",\n",
    "                size=\"lg\"\n",
    "            )\n",
    "        \n",
    "        with gr.Column(scale=2):\n",
    "            # Outputs\n",
    "            plot_output = gr.Plot(label=\"Waveform with Detections\")\n",
    "            metrics_output = gr.JSON(label=\"Metrics\")\n",
    "            events_output = gr.Dataframe(label=\"Detected Events\")\n",
    "    \n",
    "    # Connect button to prediction function\n",
    "    run_btn.click(\n",
    "        run_prediction,\n",
    "        inputs=[\n",
    "            data_source, subject_dropdown, trial_dropdown, movement_dropdown,\n",
    "            noise_dropdown, sound_dropdown, audio_upload, imu_upload,\n",
    "            modality_radio, threshold_slider\n",
    "        ],\n",
    "        outputs=[plot_output, metrics_output, events_output]\n",
    "    )\n",
    "\n",
    "print(\"✓ Gradio interface created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Launch Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Gradio app\n",
    "if MODELS is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Launching Interactive Cough Detection Model Tester...\")\n",
    "    print(\"=\"*70)\n",
    "    demo.launch(share=False, debug=True)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Cannot launch: Models not loaded\")\n",
    "    print(\"Please run Model_Training_XGBoost.ipynb first to train models\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Usage Examples\n",
    "\n",
    "### Example 1: Test on Dataset Recording\n",
    "\n",
    "1. Select \"Dataset Selector\" as data source\n",
    "2. Choose Subject: `14287`, Trial: `1`, Movement: `Sit`, Noise: `Nothing`, Sound: `Cough`\n",
    "3. Select Model: `Multimodal`\n",
    "4. Keep threshold at `0.0` (auto-optimal)\n",
    "5. Click \"Run Prediction\"\n",
    "\n",
    "**Expected output:**\n",
    "- Waveform plot with red prediction spans and green ground truth spans\n",
    "- Metrics showing TP/FP/FN counts, Sensitivity ~0.9+, Precision ~0.8+\n",
    "- Events table listing detected cough times with confidence scores\n",
    "\n",
    "### Example 2: Compare Models\n",
    "\n",
    "Run the same recording through all three models:\n",
    "- IMU-only: Lower sensitivity, may miss some coughs\n",
    "- Audio-only: Good performance on coughs\n",
    "- Multimodal: Best overall performance\n",
    "\n",
    "### Example 3: Threshold Adjustment\n",
    "\n",
    "1. Run prediction with threshold `0.0` (optimal)\n",
    "2. Increase threshold to `0.7` - fewer detections, higher precision\n",
    "3. Decrease threshold to `0.3` - more detections, lower precision\n",
    "\n",
    "### Example 4: Test on Non-Cough Sounds\n",
    "\n",
    "1. Select Sound: `Laugh` or `Throat clearing`\n",
    "2. Model should show low/no detections (good specificity)\n",
    "3. No ground truth will be shown (only available for coughs)\n",
    "\n",
    "### Example 5: Upload Custom Files\n",
    "\n",
    "1. Export a recording from the dataset as WAV + CSV\n",
    "2. Select \"Upload Files\" as data source\n",
    "3. Upload both files\n",
    "4. Run prediction (no ground truth comparison available)\n",
    "\n",
    "## Performance Notes\n",
    "\n",
    "- **Processing time**: ~2-5 seconds for 10-second recording (depends on hardware)\n",
    "- **Window size**: 0.4 seconds (fixed, from training)\n",
    "- **Hop size**: 0.05 seconds (50ms overlap between windows)\n",
    "- **Multimodal model**: Best performance but requires both audio and IMU\n",
    "- **IMU-only**: Useful for privacy-preserving scenarios (no audio)\n",
    "- **Audio-only**: Strong baseline, works well in quiet environments\n",
    "\n",
    "## Limitations\n",
    "\n",
    "1. **Dataset bias**: Models trained on 15 subjects, may not generalize to all populations\n",
    "2. **Microphone dependency**: Audio features tuned to specific hardware\n",
    "3. **Fixed window**: 0.4s windows may miss very long/short coughs\n",
    "4. **Threshold sensitivity**: Performance varies with threshold choice\n",
    "5. **No real-time processing**: Batch processing only (not streaming)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Test on different subjects to assess generalization\n",
    "- Experiment with threshold values for your use case\n",
    "- Compare model performance across different noise conditions\n",
    "- Analyze false positives/negatives to understand model weaknesses\n",
    "- Consider deploying to edge device for real-time monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge-ai-cough-count",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
