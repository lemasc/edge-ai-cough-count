{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Cough Detection Model Tester\n",
    "\n",
    "This notebook provides an interactive Gradio interface for testing XGBoost cough detection models trained on multimodal biosignals (audio + IMU).\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Load pre-trained models**: IMU-only, Audio-only, and Multimodal classifiers\n",
    "- **Test on dataset recordings**: Select from public_dataset with ground truth comparison\n",
    "- **Upload custom files**: Test on your own audio (WAV, MP3, OGG, M4A, WEBM) and/or CSV IMU data\n",
    "- **Automatic audio conversion**: Handles various formats and sample rates (auto-converts to 16 kHz)\n",
    "- **Flexible file uploads**: Upload only the required file(s) based on selected model (audio-only, IMU-only, or both)\n",
    "- **Audio playback**: Listen to recordings while viewing predictions\n",
    "- **Interactive Plotly visualizations**: \n",
    "  - Waveforms with color-coded TP/FP/FN detections\n",
    "  - Raw window predictions (all sliding windows with probabilities)\n",
    "  - Probability timeline showing continuous model confidence\n",
    "  - Probability distribution histogram\n",
    "  - Zoom, pan, and hover tooltips for detailed inspection\n",
    "- **Window-level analysis**: View every individual sliding window prediction before merging\n",
    "- **Threshold adjustment**: Fine-tune classification threshold in real-time\n",
    "- **Event-based metrics**: TP/FP/FN counts with sensitivity, precision, F1 scores\n",
    "- **Comprehensive statistics**: Window counts, merge ratios, probability distributions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**IMPORTANT**: You must first run `Model_Training_XGBoost.ipynb` to completion to generate the saved models in `models/`.\n",
    "\n",
    "The training notebook should create:\n",
    "- `models/xgb_imu.pkl`\n",
    "- `models/xgb_audio.pkl`\n",
    "- `models/xgb_multimodal.pkl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for required dependencies\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import gradio as gr\n",
    "    import xgboost\n",
    "    import joblib\n",
    "    import plotly\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    print(\"✓ All required dependencies installed\")\n",
    "    print(f\"  - gradio version: {gr.__version__}\")\n",
    "    print(f\"  - xgboost version: {xgboost.__version__}\")\n",
    "    print(f\"  - plotly version: {plotly.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Missing dependency: {e}\")\n",
    "    print(\"\\nInstall with: uv add gradio xgboost joblib plotly\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path\n",
    "if os.path.exists(\"/kaggle/usr/lib/\"):\n",
    "    # Load from Kaggle as utility scripts\n",
    "    from edge_ai_cough_count_helpers import * # pyright: ignore[reportMissingImports]\n",
    "    from edge_ai_cough_count_dataset_gen import * # pyright: ignore[reportMissingImports]\n",
    "    from edge_ai_cough_count_features import * # pyright: ignore[reportMissingImports]\n",
    "else:\n",
    "    # Add src directory to path\n",
    "    sys.path.append(os.path.abspath('../src'))\n",
    "    from helpers import *\n",
    "    from dataset_gen import *\n",
    "    from features import *\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants\n",
    "FS_AUDIO_CONST = 16000  # Audio sampling frequency\n",
    "FS_IMU_CONST = 100      # IMU sampling frequency\n",
    "WINDOW_LEN = 0.4        # Window length in seconds\n",
    "HOP_SIZE = 0.05         # Default hop size for sliding window (50ms)\n",
    "\n",
    "# Locate dataset folder\n",
    "kaggle_dataset_dir = '/kaggle/input/edge-ai-cough-count'\n",
    "base_dir = kaggle_dataset_dir if os.path.exists(kaggle_dataset_dir) else \"..\"\n",
    "data_folder = base_dir + '/public_dataset/'\n",
    "\n",
    "if not os.path.exists(data_folder):\n",
    "    raise FileNotFoundError(\n",
    "        \"Cannot find public_dataset/. Please download from: \"\n",
    "        \"https://zenodo.org/record/7562332\"\n",
    "    )\n",
    "\n",
    "# Locate models directory\n",
    "kaggle_model_dir = '/kaggle/input/model-training-xgboost'\n",
    "model_base_dir = kaggle_model_dir if os.path.exists(kaggle_model_dir) else \".\"\n",
    "MODEL_DIR = Path(model_base_dir + \"/models\")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Audio FS: {FS_AUDIO_CONST} Hz\")\n",
    "print(f\"  IMU FS: {FS_IMU_CONST} Hz\")\n",
    "print(f\"  Window length: {WINDOW_LEN}s\")\n",
    "print(f\"  Dataset folder: {data_folder if data_folder else 'Not found'}\")\n",
    "print(f\"  Models directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Model Loading System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_models():\n",
    "    \"\"\"\n",
    "    Load all three trained models from disk.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with keys 'imu', 'audio', 'multimodal'\n",
    "              Each value is {'model': XGBClassifier, 'scaler': StandardScaler, 'threshold': float}\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    for modality in ['imu', 'audio', 'multimodal']:\n",
    "        model_path = MODEL_DIR / f'xgb_{modality}.pkl'\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"\\n{'='*70}\\n\"\n",
    "                f\"ERROR: Model file not found: {model_path}\\n\\n\"\n",
    "                f\"Please run Model_Training_XGBoost.ipynb first to train and save models.\\n\"\n",
    "                f\"The training notebook should create the following files:\\n\"\n",
    "                f\"  - models/xgb_imu.pkl\\n\"\n",
    "                f\"  - models/xgb_audio.pkl\\n\"\n",
    "                f\"  - models/xgb_multimodal.pkl\\n\"\n",
    "                f\"{'='*70}\"\n",
    "            )\n",
    "        \n",
    "        with open(model_path, 'rb') as f:\n",
    "            models[modality] = pickle.load(f)\n",
    "        \n",
    "        print(f\"✓ Loaded {modality} model from {model_path}\")\n",
    "        print(f\"  Threshold: {models[modality]['threshold']:.3f}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Load models\n",
    "try:\n",
    "    MODELS = load_trained_models()\n",
    "    print(f\"\\n✓ All models loaded successfully\")\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    MODELS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Feature Extraction Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_window(audio_window, imu_window, modality='multimodal'):\n",
    "    \"\"\"\n",
    "    Extract features from a single window of audio and IMU data.\n",
    "    \n",
    "    Args:\n",
    "        audio_window: (N_audio,) audio samples\n",
    "        imu_window: (N_imu, 6) IMU samples\n",
    "        modality: 'imu', 'audio', or 'multimodal'\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Feature vector\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    if modality in ['audio', 'multimodal']:\n",
    "        audio_feat = extract_audio_features(audio_window, fs=FS_AUDIO_CONST)\n",
    "        # Handle NaN/Inf\n",
    "        audio_feat = np.nan_to_num(audio_feat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        features.append(audio_feat)\n",
    "    \n",
    "    if modality in ['imu', 'multimodal']:\n",
    "        imu_feat = extract_imu_features(imu_window)\n",
    "        # Handle NaN/Inf\n",
    "        imu_feat = np.nan_to_num(imu_feat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        features.append(imu_feat)\n",
    "    \n",
    "    return np.concatenate(features)\n",
    "\n",
    "print(\"✓ Feature extraction utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Sliding Window Prediction Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_predict(audio, imu, model_data, modality='multimodal', \n",
    "                          window_len=0.4, hop_size=0.05, threshold=None):\n",
    "    \"\"\"\n",
    "    Apply model to continuous recording using sliding windows.\n",
    "    \n",
    "    Args:\n",
    "        audio: (N_audio,) audio samples\n",
    "        imu: (N_imu, 6) IMU samples\n",
    "        model_data: Dict with 'model', 'scaler', 'threshold'\n",
    "        modality: 'imu', 'audio', or 'multimodal'\n",
    "        window_len: Window length in seconds\n",
    "        hop_size: Hop size in seconds\n",
    "        threshold: Classification threshold (None = use optimal from model)\n",
    "    \n",
    "    Returns:\n",
    "        predictions: List of (start_time, end_time, probability) tuples (only above threshold)\n",
    "        all_probs: Array of probabilities for each window\n",
    "        window_times: Array of window center times\n",
    "        all_windows: List of (start, end, center, prob) for ALL windows\n",
    "    \"\"\"\n",
    "    model = model_data['model']\n",
    "    scaler = model_data['scaler']\n",
    "    if threshold is None:\n",
    "        threshold = model_data['threshold']\n",
    "    \n",
    "    # Calculate window and hop in samples\n",
    "    audio_win_samples = int(window_len * FS_AUDIO_CONST)\n",
    "    audio_hop_samples = int(hop_size * FS_AUDIO_CONST)\n",
    "    imu_win_samples = int(window_len * FS_IMU_CONST)\n",
    "    imu_hop_samples = int(hop_size * FS_IMU_CONST)\n",
    "    \n",
    "    # Extract windows\n",
    "    n_windows = (len(audio) - audio_win_samples) // audio_hop_samples + 1\n",
    "    features_list = []\n",
    "    window_times = []\n",
    "    \n",
    "    for i in range(n_windows):\n",
    "        audio_start = i * audio_hop_samples\n",
    "        audio_end = audio_start + audio_win_samples\n",
    "        imu_start = i * imu_hop_samples\n",
    "        imu_end = imu_start + imu_win_samples\n",
    "        \n",
    "        if audio_end > len(audio) or imu_end > len(imu):\n",
    "            break\n",
    "        \n",
    "        audio_window = audio[audio_start:audio_end]\n",
    "        imu_window = imu[imu_start:imu_end, :]\n",
    "        \n",
    "        features = extract_features_for_window(audio_window, imu_window, modality)\n",
    "        features_list.append(features)\n",
    "        \n",
    "        # Window center time\n",
    "        center_time = (audio_start + audio_win_samples / 2) / FS_AUDIO_CONST\n",
    "        window_times.append(center_time)\n",
    "    \n",
    "    # Batch predict\n",
    "    X = np.array(features_list)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    probs = model.predict_proba(X_scaled)[:, 1]\n",
    "    \n",
    "    # Create list of ALL windows with their probabilities\n",
    "    all_windows = []\n",
    "    # Convert to event-based predictions (only above threshold)\n",
    "    predictions = []\n",
    "    for i, (prob, center) in enumerate(zip(probs, window_times)):\n",
    "        start = center - window_len / 2\n",
    "        end = center + window_len / 2\n",
    "        all_windows.append((start, end, center, prob))\n",
    "        if prob >= threshold:\n",
    "            predictions.append((start, end, prob))\n",
    "    \n",
    "    return predictions, probs, np.array(window_times), all_windows\n",
    "\n",
    "print(\"✓ Sliding window prediction engine ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_detections(predictions, gap_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Merge consecutive detections that are close together.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of (start, end, prob) tuples\n",
    "        gap_threshold: Maximum gap between events to merge (seconds)\n",
    "    \n",
    "    Returns:\n",
    "        merged: List of (start, end, max_prob) tuples\n",
    "    \"\"\"\n",
    "    if not predictions:\n",
    "        return []\n",
    "    \n",
    "    # Sort by start time\n",
    "    sorted_preds = sorted(predictions, key=lambda x: x[0])\n",
    "    \n",
    "    merged = []\n",
    "    current_start, current_end, current_prob = sorted_preds[0]\n",
    "    \n",
    "    for start, end, prob in sorted_preds[1:]:\n",
    "        # If gap is small, merge\n",
    "        if start - current_end <= gap_threshold:\n",
    "            current_end = max(current_end, end)\n",
    "            current_prob = max(current_prob, prob)\n",
    "        else:\n",
    "            # Save current event and start new one\n",
    "            merged.append((current_start, current_end, current_prob))\n",
    "            current_start, current_end, current_prob = start, end, prob\n",
    "    \n",
    "    # Add last event\n",
    "    merged.append((current_start, current_end, current_prob))\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def classify_predictions(predictions, ground_truth, tolerance_start=0.25, \n",
    "                        tolerance_end=0.25, min_overlap=0.1):\n",
    "    \"\"\"\n",
    "    Classify predictions as TP/FP and identify FN.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of (start, end, prob) tuples\n",
    "        ground_truth: List of (start, end) tuples\n",
    "        tolerance_start: Start tolerance in seconds\n",
    "        tolerance_end: End tolerance in seconds\n",
    "        min_overlap: Minimum overlap ratio to count as TP\n",
    "    \n",
    "    Returns:\n",
    "        tp_list: List of TP predictions (start, end, prob)\n",
    "        fp_list: List of FP predictions (start, end, prob)\n",
    "        fn_list: List of FN ground truth events (start, end)\n",
    "    \"\"\"\n",
    "    if not ground_truth:\n",
    "        # No ground truth - all predictions are unknown\n",
    "        return [], [], []\n",
    "    \n",
    "    if not predictions:\n",
    "        # No predictions - all ground truth are FN\n",
    "        return [], [], ground_truth\n",
    "    \n",
    "    # Convert to arrays\n",
    "    pred_starts = np.array([p[0] for p in predictions])\n",
    "    pred_ends = np.array([p[1] for p in predictions])\n",
    "    gt_starts = np.array([g[0] for g in ground_truth])\n",
    "    gt_ends = np.array([g[1] for g in ground_truth])\n",
    "    \n",
    "    # Track matches\n",
    "    gt_matched = np.zeros(len(ground_truth), dtype=bool)\n",
    "    tp_list = []\n",
    "    fp_list = []\n",
    "    \n",
    "    # Classify each prediction\n",
    "    for pred_start, pred_end, prob in predictions:\n",
    "        matched = False\n",
    "        \n",
    "        for i, (gt_start, gt_end) in enumerate(zip(gt_starts, gt_ends)):\n",
    "            if gt_matched[i]:\n",
    "                continue\n",
    "            \n",
    "            # Check overlap\n",
    "            overlap_start = max(pred_start, gt_start - tolerance_start)\n",
    "            overlap_end = min(pred_end, gt_end + tolerance_end)\n",
    "            \n",
    "            if overlap_end > overlap_start:\n",
    "                overlap_duration = overlap_end - overlap_start\n",
    "                gt_duration = gt_end - gt_start\n",
    "                \n",
    "                if overlap_duration / gt_duration >= min_overlap:\n",
    "                    # True Positive\n",
    "                    tp_list.append((pred_start, pred_end, prob))\n",
    "                    gt_matched[i] = True\n",
    "                    matched = True\n",
    "                    break\n",
    "        \n",
    "        if not matched:\n",
    "            # False Positive\n",
    "            fp_list.append((pred_start, pred_end, prob))\n",
    "    \n",
    "    # Unmatched GT events are false negatives\n",
    "    fn_list = [ground_truth[i] for i in range(len(ground_truth)) if not gt_matched[i]]\n",
    "    \n",
    "    return tp_list, fp_list, fn_list\n",
    "\n",
    "print(\"✓ Detection merging and classification ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_cough_events(audio, candidate_segments, fs_audio=16000,\n",
    "                        t_dedup=0.23, t_bout=0.55):\n",
    "    \"\"\"\n",
    "    Post-process merged candidate segments using Cough-E-style peak refinement.\n",
    "\n",
    "    Fixes two failure modes of gap-based merging:\n",
    "      1. Overlapping windows double-count the same cough.\n",
    "      2. Multiple coughs in a bout collapse into one long merged region.\n",
    "\n",
    "    Strategy\n",
    "    --------\n",
    "    Rather than computing power thresholds *within* each candidate chunk\n",
    "    (which fails when one loud cough dominates the threshold and masks quieter\n",
    "    ones in the same segment), we call segment_cough() on the **full audio**.\n",
    "    Global thresholds are calibrated to the whole recording — mostly silence —\n",
    "    so the RMS is low and every real cough clears the bar, regardless of whether\n",
    "    it is the loudest or not.\n",
    "\n",
    "    We then keep only physical coughs whose peak falls inside a confirmed\n",
    "    candidate region (from the ML classifier), applying a ±0.3 s grace window\n",
    "    to account for window-level timing uncertainty.\n",
    "\n",
    "    After filtering, we apply the paper's two physiology-based rules:\n",
    "      - Deduplicate peaks < t_dedup=0.23 s apart  (same cough, keep union)\n",
    "      - Bout-split: if next peak < t_bout=0.55 s away, set end[n] = start[n+1]\n",
    "\n",
    "    Args:\n",
    "        audio:              (N,) audio samples at fs_audio Hz\n",
    "        candidate_segments: List of (start, end, prob) from merge_detections()\n",
    "        fs_audio:           Audio sampling rate in Hz (default 16000)\n",
    "        t_dedup:            Min inter-peak gap to count as a new cough (0.23 s)\n",
    "        t_bout:             Max inter-peak gap within a cough bout (0.55 s)\n",
    "\n",
    "    Returns:\n",
    "        refined: List of (start, end, prob) — one tuple per refined cough event\n",
    "    \"\"\"\n",
    "    if not candidate_segments:\n",
    "        return []\n",
    "\n",
    "    # ── Step 1: Run segment_cough() on the FULL audio ─────────────────────────\n",
    "    # Thresholds (th_h = 2*rms, th_l = 0.1*rms) are computed globally, so a\n",
    "    # single loud spike does NOT raise the bar for quieter coughs elsewhere.\n",
    "    _, _, starts_idx, ends_idx, _, peak_locs_idx = segment_cough(\n",
    "        audio, fs_audio,\n",
    "        cough_padding=0.1,    # 100 ms padding (smaller — we keep our own margins)\n",
    "        min_cough_len=0.1,    # allow events as short as 100 ms\n",
    "        th_l_multiplier=0.1,\n",
    "        th_h_multiplier=2.0,\n",
    "    )\n",
    "\n",
    "    if len(starts_idx) == 0:\n",
    "        # segment_cough found nothing — fall back to coarse candidate list\n",
    "        return candidate_segments\n",
    "\n",
    "    # Convert sample indices → seconds\n",
    "    seg_starts = starts_idx / fs_audio\n",
    "    seg_ends   = ends_idx   / fs_audio\n",
    "    seg_peaks  = np.array(peak_locs_idx) / fs_audio\n",
    "\n",
    "    # ── Step 2: Filter to confirmed candidate regions ─────────────────────────\n",
    "    # Only keep physical coughs whose peak lies within (or within ±0.3 s of)\n",
    "    # a ML-confirmed candidate region.  This suppresses false alarms from\n",
    "    # segment_cough in silent / non-cough sections of the recording.\n",
    "    cand_tol = 0.3\n",
    "    filtered = []\n",
    "    for seg_st, seg_et, seg_pt in zip(seg_starts, seg_ends, seg_peaks):\n",
    "        for cand_st, cand_et, prob in candidate_segments:\n",
    "            if cand_st - cand_tol <= seg_pt <= cand_et + cand_tol:\n",
    "                filtered.append([seg_st, seg_et, seg_pt, prob])\n",
    "                break\n",
    "\n",
    "    if not filtered:\n",
    "        # No overlap found — fall back so we don't silently drop everything\n",
    "        return candidate_segments\n",
    "\n",
    "    # Sort by peak time before applying the next two rules\n",
    "    filtered.sort(key=lambda x: x[2])\n",
    "\n",
    "    # ── Step 3: Deduplicate peaks closer than t_dedup (same physical cough) ──\n",
    "    deduped = [filtered[0][:]]\n",
    "    for r in filtered[1:]:\n",
    "        prev = deduped[-1]\n",
    "        if r[2] - prev[2] < t_dedup:\n",
    "            # Same cough: take union of regions, keep max probability\n",
    "            prev[1] = max(prev[1], r[1])\n",
    "            prev[3] = max(prev[3], r[3])\n",
    "        else:\n",
    "            deduped.append(r[:])\n",
    "\n",
    "    # ── Step 4: Bout splitting ────────────────────────────────────────────────\n",
    "    # If the next cough's peak is within t_bout of the current peak, the two\n",
    "    # coughs are in a bout.  Hard-split by setting end[n] = start[n+1].\n",
    "    final = []\n",
    "    for i, r in enumerate(deduped):\n",
    "        start_t, end_t, peak_t, prob = r\n",
    "        if i + 1 < len(deduped):\n",
    "            next_start = deduped[i + 1][0]\n",
    "            next_peak  = deduped[i + 1][2]\n",
    "            if next_peak - peak_t < t_bout:\n",
    "                end_t = next_start   # hard split at next region start\n",
    "        final.append((start_t, end_t, prob))\n",
    "\n",
    "    return final\n",
    "\n",
    "print(\"✓ Cough-E post-processing (refine_cough_events) ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_plotly(audio, imu, predictions, ground_truth, all_windows, \n",
    "                           threshold, window_times, all_probs):\n",
    "    \"\"\"\n",
    "    Create interactive Plotly visualization with multiple views.\n",
    "    \n",
    "    Args:\n",
    "        audio: (N,) audio samples\n",
    "        imu: (M, 6) IMU samples\n",
    "        predictions: List of merged (start, end, prob) tuples\n",
    "        ground_truth: Optional list of (start, end) tuples\n",
    "        all_windows: List of ALL (start, end, center, prob) tuples\n",
    "        threshold: Classification threshold used\n",
    "        window_times: Array of window center times\n",
    "        all_probs: Array of all window probabilities\n",
    "    \n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure\n",
    "    \"\"\"\n",
    "    # Classify predictions if ground truth available\n",
    "    if ground_truth:\n",
    "        tp_list, fp_list, fn_list = classify_predictions(predictions, ground_truth)\n",
    "    else:\n",
    "        tp_list, fp_list, fn_list = [], [], []\n",
    "\n",
    "    # Map (start, end) → global event index so numbers stay consistent across\n",
    "    # TP/FP/FN classification and match the rows in the events table.\n",
    "    pred_index_map = {(s, e): i for i, (s, e, _) in enumerate(predictions)}\n",
    "\n",
    "    # Color pairs [even-index, odd-index] — alternate so adjacent events are\n",
    "    # always visually distinct even when their boundaries touch.\n",
    "    if ground_truth:\n",
    "        tp_colors = ['rgba(40, 167, 69, 0.4)',  'rgba(23, 162, 184, 0.4)']  # green / teal\n",
    "        fp_colors = ['rgba(220, 53, 69, 0.4)',  'rgba(253, 126, 20, 0.4)']  # red / orange\n",
    "        fn_color  = 'rgba(255, 193, 7, 0.25)'                                # amber (no alternation)\n",
    "    else:\n",
    "        pred_colors = ['rgba(220, 53, 69, 0.4)', 'rgba(52, 120, 219, 0.4)'] # red / steel-blue\n",
    "\n",
    "    # Create subplots: Audio, Raw Windows, Probability Timeline, IMU, Histogram\n",
    "    fig = make_subplots(\n",
    "        rows=5, cols=1,\n",
    "        row_heights=[0.25, 0.15, 0.15, 0.25, 0.20],\n",
    "        subplot_titles=(\n",
    "            f'Audio Waveform + Merged Events ({len(predictions)} detections)',\n",
    "            f'Raw Window Predictions ({len(all_windows)} windows, {sum(all_probs >= threshold)} above threshold)',\n",
    "            f'Probability Timeline (threshold = {threshold:.3f})',\n",
    "            'IMU Accelerometer Z (negated) + Merged Events',\n",
    "            f'Probability Distribution ({len(all_probs)} windows)'\n",
    "        ),\n",
    "        vertical_spacing=0.08,\n",
    "        specs=[[{\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Time axes\n",
    "    audio_time = np.arange(len(audio)) / FS_AUDIO_CONST\n",
    "    imu_time = np.arange(len(imu)) / FS_IMU_CONST\n",
    "    \n",
    "    # ========== Subplot 1: Audio Waveform ==========\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=audio_time, y=audio, mode='lines', \n",
    "                  line=dict(color='black', width=0.5),\n",
    "                  name='Audio', showlegend=True,\n",
    "                  hovertemplate='Time: %{x:.3f}s<br>Amplitude: %{y:.3f}<extra></extra>'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add merged events — alternating colors + thin border + event number label.\n",
    "    # The border ensures adjacent events are visually separated even when they\n",
    "    # share an exact boundary (e.g. event #1 ends at 2.00s, #2 starts at 2.00s).\n",
    "    if ground_truth:\n",
    "        for start, end, prob in tp_list:\n",
    "            idx = pred_index_map.get((start, end), 0)\n",
    "            fig.add_vrect(\n",
    "                x0=start, x1=end,\n",
    "                fillcolor=tp_colors[idx % 2], opacity=1.0,\n",
    "                layer=\"below\", line_width=1, line_color='rgba(40,167,69,0.8)',\n",
    "                row=1, col=1,\n",
    "                annotation_text=f\"TP #{idx + 1}\",\n",
    "                annotation_position=\"top left\",\n",
    "            )\n",
    "        for start, end, prob in fp_list:\n",
    "            idx = pred_index_map.get((start, end), 0)\n",
    "            fig.add_vrect(\n",
    "                x0=start, x1=end,\n",
    "                fillcolor=fp_colors[idx % 2], opacity=1.0,\n",
    "                layer=\"below\", line_width=1, line_color='rgba(180,40,40,0.8)',\n",
    "                row=1, col=1,\n",
    "                annotation_text=f\"FP #{idx + 1}\",\n",
    "                annotation_position=\"top left\",\n",
    "            )\n",
    "        for start, end in fn_list:\n",
    "            fig.add_vrect(\n",
    "                x0=start, x1=end,\n",
    "                fillcolor=fn_color, opacity=1.0,\n",
    "                layer=\"below\", line_width=1, line_color='rgba(180,140,0,0.6)',\n",
    "                row=1, col=1,\n",
    "                annotation_text=\"FN\",\n",
    "                annotation_position=\"top left\",\n",
    "            )\n",
    "    else:\n",
    "        for i, (start, end, prob) in enumerate(predictions):\n",
    "            fig.add_vrect(\n",
    "                x0=start, x1=end,\n",
    "                fillcolor=pred_colors[i % 2], opacity=1.0,\n",
    "                layer=\"below\", line_width=1, line_color='rgba(80,80,80,0.5)',\n",
    "                row=1, col=1,\n",
    "                annotation_text=f\"#{i + 1}\",\n",
    "                annotation_position=\"top left\",\n",
    "            )\n",
    "    \n",
    "    # Add ground truth spans\n",
    "    if ground_truth:\n",
    "        gt_legend_added = False\n",
    "        for start, end in ground_truth:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=[start, end], y=[0, 0], mode='lines',\n",
    "                          line=dict(color='green', width=3),\n",
    "                          name='Ground Truth', showlegend=(not gt_legend_added),\n",
    "                          legendgroup='ground_truth',\n",
    "                          hovertemplate=f'GT: {start:.3f}s - {end:.3f}s<extra></extra>'),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            gt_legend_added = True\n",
    "    \n",
    "    # ========== Subplot 2: Raw Window Predictions ==========\n",
    "    # Color code windows by probability\n",
    "    colors = []\n",
    "    hover_texts = []\n",
    "    for start, end, center, prob in all_windows:\n",
    "        if prob >= threshold:\n",
    "            colors.append(f'rgba(255, 0, 0, {min(prob, 1.0)})')  # Red with alpha = probability\n",
    "        else:\n",
    "            colors.append(f'rgba(100, 100, 100, {max(0.1, prob)})')  # Gray for below threshold\n",
    "        \n",
    "        hover_texts.append(\n",
    "            f'Window #{all_windows.index((start, end, center, prob))}<br>' +\n",
    "            f'Center: {center:.3f}s<br>' +\n",
    "            f'Range: {start:.3f}s - {end:.3f}s<br>' +\n",
    "            f'Duration: {end-start:.3f}s<br>' +\n",
    "            f'Probability: {prob:.4f}<br>' +\n",
    "            f'Above threshold: {\"Yes\" if prob >= threshold else \"No\"}'\n",
    "        )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[w[2] for w in all_windows],  # center times\n",
    "            y=[w[3] for w in all_windows],  # probabilities\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=[w[3] for w in all_windows],\n",
    "                colorscale='Reds',\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Probability\", x=1.12, len=0.2, y=0.75,yanchor='middle'),\n",
    "                line=dict(width=1, color='black')\n",
    "            ),\n",
    "            name='Raw Windows',\n",
    "            text=hover_texts,\n",
    "            hovertemplate='%{text}<extra></extra>'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Add threshold line\n",
    "    fig.add_hline(y=threshold, line_dash=\"dash\", line_color=\"red\",\n",
    "                 annotation_text=f\"Threshold\",\n",
    "                 annotation_position=\"right\", row=2, col=1\n",
    "                 )\n",
    "    \n",
    "    # ========== Subplot 3: Probability Timeline ==========\n",
    "    # Continuous probability curve\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=window_times,\n",
    "            y=all_probs,\n",
    "            mode='lines',\n",
    "            line=dict(color='blue', width=2),\n",
    "            fill='tonexty',\n",
    "            name='Probability',\n",
    "            hovertemplate='Time: %{x:.3f}s<br>Probability: %{y:.4f}<extra></extra>'\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # Highlight above-threshold regions\n",
    "    above_thresh = all_probs >= threshold\n",
    "    for i in range(len(window_times) - 1):\n",
    "        if above_thresh[i]:\n",
    "            fig.add_vrect(\n",
    "                x0=window_times[i], x1=window_times[i+1],\n",
    "                fillcolor=\"red\", opacity=0.2,\n",
    "                layer=\"below\", line_width=0, row=3, col=1\n",
    "            )\n",
    "    \n",
    "    # Add threshold line\n",
    "    fig.add_hline(y=threshold, line_dash=\"dash\", line_color=\"red\",\n",
    "                 row=3, col=1)\n",
    "    \n",
    "    # ========== Subplot 4: IMU Signal ==========\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=imu_time, y=-imu[:, 2], mode='lines',\n",
    "                  line=dict(color='blue', width=1),\n",
    "                  name='IMU Z-axis', showlegend=True,\n",
    "                  hovertemplate='Time: %{x:.3f}s<br>Accel: %{y:.3f}<extra></extra>'),\n",
    "        row=4, col=1\n",
    "    )\n",
    "    \n",
    "    # IMU events — same alternating colors and borders as audio, no labels\n",
    "    # (labels would be redundant and clutter the smaller IMU subplot).\n",
    "    if ground_truth:\n",
    "        for start, end, prob in tp_list:\n",
    "            idx = pred_index_map.get((start, end), 0)\n",
    "            fig.add_vrect(\n",
    "                x0=start, x1=end,\n",
    "                fillcolor=tp_colors[idx % 2], opacity=1.0,\n",
    "                layer=\"below\", line_width=1, line_color='rgba(40,167,69,0.8)',\n",
    "                row=4, col=1,\n",
    "            )\n",
    "        for start, end, prob in fp_list:\n",
    "            idx = pred_index_map.get((start, end), 0)\n",
    "            fig.add_vrect(\n",
    "                x0=start, x1=end,\n",
    "                fillcolor=fp_colors[idx % 2], opacity=1.0,\n",
    "                layer=\"below\", line_width=1, line_color='rgba(180,40,40,0.8)',\n",
    "                row=4, col=1,\n",
    "            )\n",
    "        for start, end in fn_list:\n",
    "            fig.add_vrect(\n",
    "                x0=start, x1=end,\n",
    "                fillcolor=fn_color, opacity=1.0,\n",
    "                layer=\"below\", line_width=1, line_color='rgba(180,140,0,0.6)',\n",
    "                row=4, col=1,\n",
    "            )\n",
    "    else:\n",
    "        for i, (start, end, prob) in enumerate(predictions):\n",
    "            fig.add_vrect(\n",
    "                x0=start, x1=end,\n",
    "                fillcolor=pred_colors[i % 2], opacity=1.0,\n",
    "                layer=\"below\", line_width=1, line_color='rgba(80,80,80,0.5)',\n",
    "                row=4, col=1,\n",
    "            )\n",
    "    \n",
    "    # ========== Subplot 5: Probability Distribution ==========\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=all_probs,\n",
    "            nbinsx=50,\n",
    "            name='All Windows',\n",
    "            marker_color='lightblue',\n",
    "            hovertemplate='Probability: %{x:.3f}<br>Count: %{y}<extra></extra>'\n",
    "        ),\n",
    "        row=5, col=1\n",
    "    )\n",
    "    \n",
    "    # Add threshold line\n",
    "    fig.add_vline(x=threshold, line_dash=\"dash\", line_color=\"red\",\n",
    "                 annotation_text=f\"Threshold\",\n",
    "                 annotation_position=\"top\", row=5, col=1)\n",
    "    \n",
    "    # Statistics annotation\n",
    "    stats_text = (\n",
    "        f\"Total Windows: {len(all_probs)}<br>\" +\n",
    "        f\"Above Threshold: {sum(all_probs >= threshold)} ({100*sum(all_probs >= threshold)/len(all_probs):.1f}%)<br>\" +\n",
    "        f\"Mean Prob: {np.mean(all_probs):.3f}<br>\" +\n",
    "        f\"Merged Events: {len(predictions)}<br>\" +\n",
    "        f\"Merge Ratio: {len(all_probs)}/{len(predictions)} = {len(all_probs)/max(len(predictions),1):.1f}x\"\n",
    "    )\n",
    "    \n",
    "    fig.add_annotation(\n",
    "        text=stats_text,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.02, y=0.98, showarrow=False,\n",
    "        bgcolor=\"white\", bordercolor=\"black\", borderwidth=1,\n",
    "        align=\"left\", font=dict(size=10)\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_xaxes(title_text=\"Time (s)\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Time (s)\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Time (s)\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"Time (s)\", row=4, col=1)\n",
    "    fig.update_xaxes(title_text=\"Probability\", row=5, col=1)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Amplitude\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Probability\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Probability\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Acceleration\", row=4, col=1)\n",
    "    fig.update_yaxes(title_text=\"Count\", row=5, col=1)\n",
    "    \n",
    "    # Sync x-axes for time-based plots\n",
    "    fig.update_xaxes(matches='x', row=1, col=1)\n",
    "    fig.update_xaxes(matches='x', row=2, col=1)\n",
    "    fig.update_xaxes(matches='x', row=3, col=1)\n",
    "    fig.update_xaxes(matches='x', row=4, col=1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=1400,\n",
    "        showlegend=True,\n",
    "        hovermode='x unified',\n",
    "        title_text=f\"Interactive Cough Detection Analysis\",\n",
    "        title_font_size=16\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "print(\"✓ Interactive Plotly visualization ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Metrics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_event_metrics(predictions, ground_truth, tolerance_start=0.25, \n",
    "                         tolerance_end=0.25, min_overlap=0.1):\n",
    "    \"\"\"\n",
    "    Compute event-based metrics (TP, FP, FN).\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of (start, end, prob) tuples\n",
    "        ground_truth: List of (start, end) tuples\n",
    "        tolerance_start: Start tolerance in seconds\n",
    "        tolerance_end: End tolerance in seconds\n",
    "        min_overlap: Minimum overlap ratio to count as TP\n",
    "    \n",
    "    Returns:\n",
    "        dict: Metrics including TP, FP, FN, Sensitivity, Precision, F1\n",
    "    \"\"\"\n",
    "    if not ground_truth:\n",
    "        # No ground truth - just return detection count\n",
    "        return {\n",
    "            'TP': None,\n",
    "            'FP': None,\n",
    "            'FN': None,\n",
    "            'Sensitivity': None,\n",
    "            'Precision': None,\n",
    "            'F1': None,\n",
    "            'Total_Detections': len(predictions),\n",
    "            'Ground_Truth_Count': 0\n",
    "        }\n",
    "    \n",
    "    # Convert to start/end arrays\n",
    "    pred_starts = np.array([p[0] for p in predictions])\n",
    "    pred_ends = np.array([p[1] for p in predictions])\n",
    "    gt_starts = np.array([g[0] for g in ground_truth])\n",
    "    gt_ends = np.array([g[1] for g in ground_truth])\n",
    "    \n",
    "    # Track which GT events have been matched\n",
    "    gt_matched = np.zeros(len(ground_truth), dtype=bool)\n",
    "    tp_count = 0\n",
    "    fp_count = 0\n",
    "    \n",
    "    # For each prediction, check if it matches a GT event\n",
    "    for pred_start, pred_end in zip(pred_starts, pred_ends):\n",
    "        matched = False\n",
    "        \n",
    "        for i, (gt_start, gt_end) in enumerate(zip(gt_starts, gt_ends)):\n",
    "            if gt_matched[i]:\n",
    "                continue\n",
    "            \n",
    "            # Check overlap\n",
    "            overlap_start = max(pred_start, gt_start - tolerance_start)\n",
    "            overlap_end = min(pred_end, gt_end + tolerance_end)\n",
    "            \n",
    "            if overlap_end > overlap_start:\n",
    "                overlap_duration = overlap_end - overlap_start\n",
    "                gt_duration = gt_end - gt_start\n",
    "                \n",
    "                if overlap_duration / gt_duration >= min_overlap:\n",
    "                    # Match!\n",
    "                    tp_count += 1\n",
    "                    gt_matched[i] = True\n",
    "                    matched = True\n",
    "                    break\n",
    "        \n",
    "        if not matched:\n",
    "            fp_count += 1\n",
    "    \n",
    "    # Unmatched GT events are false negatives\n",
    "    fn_count = np.sum(~gt_matched)\n",
    "    \n",
    "    # Compute metrics\n",
    "    sensitivity = tp_count / (tp_count + fn_count) if (tp_count + fn_count) > 0 else 0\n",
    "    precision = tp_count / (tp_count + fp_count) if (tp_count + fp_count) > 0 else 0\n",
    "    f1 = 2 * precision * sensitivity / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'TP': int(tp_count),\n",
    "        'FP': int(fp_count),\n",
    "        'FN': int(fn_count),\n",
    "        'Sensitivity': float(sensitivity),\n",
    "        'Precision': float(precision),\n",
    "        'F1': float(f1),\n",
    "        'Total_Detections': len(predictions),\n",
    "        'Ground_Truth_Count': len(ground_truth)\n",
    "    }\n",
    "\n",
    "print(\"✓ Metrics computation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Data Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_recording(subject_id, trial, movement, noise, sound):\n",
    "    \"\"\"\n",
    "    Load a recording from the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        audio: (N,) audio samples (outer mic)\n",
    "        imu_data: (M, 6) IMU samples\n",
    "        ground_truth: List of (start, end) or None\n",
    "    \"\"\"\n",
    "    # Load audio (outer mic only) - uses peak normalization by default\n",
    "    audio_air, _ = load_audio(data_folder, subject_id, trial, movement, noise, sound)\n",
    "    \n",
    "    # Load IMU\n",
    "    imu_obj = load_imu(data_folder, subject_id, trial, movement, noise, sound)\n",
    "    imu_data = imu_obj.make_segment_df().values\n",
    "    \n",
    "    # Load ground truth if cough recording\n",
    "    ground_truth = None\n",
    "    if sound == Sound.COUGH:\n",
    "        try:\n",
    "            start_times, end_times = load_annotation(data_folder, subject_id, trial, movement, noise, sound)\n",
    "            ground_truth = list(zip(start_times, end_times))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return audio_air, imu_data, ground_truth\n",
    "\n",
    "def load_uploaded_audio(file_obj):\n",
    "    \"\"\"\n",
    "    Load audio from various formats (WAV, MP3, OGG, M4A, WEBM) and convert to 16 kHz.\n",
    "    \n",
    "    Returns:\n",
    "        audio: (N,) normalized audio samples in [-1, +1] range at 16 kHz\n",
    "    \"\"\"\n",
    "    # librosa.load automatically:\n",
    "    # - Handles multiple formats (WAV, MP3, OGG, M4A, WEBM, FLAC, etc.)\n",
    "    # - Resamples to target sr (16000 Hz)\n",
    "    # - Returns float32 normalized audio\n",
    "    audio, fs = librosa.load(file_obj.name, sr=FS_AUDIO_CONST, mono=True)\n",
    "    \n",
    "    # Apply peak normalization (matching training preprocessing)\n",
    "    audio = audio - np.mean(audio)\n",
    "    audio = audio / (np.max(np.abs(audio)) + 1e-17)\n",
    "    \n",
    "    return audio\n",
    "\n",
    "def load_uploaded_imu(file_obj):\n",
    "    \"\"\"\n",
    "    Load IMU CSV from Gradio file upload.\n",
    "    \n",
    "    Returns:\n",
    "        imu: (N, 6) IMU samples\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_obj.name)\n",
    "    required_cols = ['Accel x', 'Accel y', 'Accel z', 'Gyro Y', 'Gyro P', 'Gyro R']\n",
    "    \n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        raise ValueError(f\"IMU CSV must contain: {required_cols}\")\n",
    "    \n",
    "    return df[required_cols].values\n",
    "\n",
    "def create_dummy_audio(duration_seconds):\n",
    "    \"\"\"\n",
    "    Create dummy audio data for when only IMU is provided.\n",
    "    \n",
    "    Args:\n",
    "        duration_seconds: Duration in seconds\n",
    "    \n",
    "    Returns:\n",
    "        audio: (N,) zero-filled audio samples\n",
    "    \"\"\"\n",
    "    n_samples = int(duration_seconds * FS_AUDIO_CONST)\n",
    "    return np.zeros(n_samples, dtype=np.float32)\n",
    "\n",
    "def create_dummy_imu(duration_seconds):\n",
    "    \"\"\"\n",
    "    Create dummy IMU data for when only audio is provided.\n",
    "    \n",
    "    Args:\n",
    "        duration_seconds: Duration in seconds\n",
    "    \n",
    "    Returns:\n",
    "        imu: (N, 6) zero-filled IMU samples\n",
    "    \"\"\"\n",
    "    n_samples = int(duration_seconds * FS_IMU_CONST)\n",
    "    return np.zeros((n_samples, 6), dtype=np.float32)\n",
    "\n",
    "print(\"✓ Data loading utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Main Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(data_source, subject_id, trial, movement, noise, sound,\n",
    "                  audio_file, imu_file, modality, threshold_override, use_refinement):\n",
    "    \"\"\"\n",
    "    Main prediction function called by Gradio interface.\n",
    "    \"\"\"\n",
    "    if MODELS is None:\n",
    "        return None, None, {\"Error\": \"Models not loaded\"}, pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        # Map modality to model key\n",
    "        modality_map = {\n",
    "            \"IMU-only\": \"imu\",\n",
    "            \"Audio-only\": \"audio\",\n",
    "            \"Multimodal\": \"multimodal\"\n",
    "        }\n",
    "        model_key = modality_map[modality]\n",
    "        \n",
    "        # Load data based on source\n",
    "        if data_source == \"Dataset Selector\":\n",
    "            if not data_folder:\n",
    "                return None, None, {\"Error\": \"Dataset not found\"}, pd.DataFrame()\n",
    "            \n",
    "            # Convert dropdown values to Enum\n",
    "            trial_enum = Trial(trial)\n",
    "            mov_enum = Movement(movement.lower())\n",
    "            noise_enum = Noise(noise.lower().replace(' ', '_'))\n",
    "            sound_enum = Sound(sound.lower().replace(' ', '_'))\n",
    "            \n",
    "            audio, imu, ground_truth = load_dataset_recording(\n",
    "                subject_id, trial_enum, mov_enum, noise_enum, sound_enum\n",
    "            )\n",
    "        else:  # Upload Files\n",
    "            # Check which files are required based on modality\n",
    "            audio = None\n",
    "            imu = None\n",
    "            \n",
    "            if model_key in ['audio', 'multimodal']:\n",
    "                if audio_file is None:\n",
    "                    return None, None, {\"Error\": f\"Please upload audio file for {modality} model\"}, pd.DataFrame()\n",
    "                audio = load_uploaded_audio(audio_file)\n",
    "            \n",
    "            if model_key in ['imu', 'multimodal']:\n",
    "                if imu_file is None:\n",
    "                    return None, None, {\"Error\": f\"Please upload IMU file for {modality} model\"}, pd.DataFrame()\n",
    "                imu = load_uploaded_imu(imu_file)\n",
    "            \n",
    "            # Create dummy data for missing modality if not required\n",
    "            if audio is None and imu is not None:\n",
    "                # IMU-only model: create dummy audio matching IMU duration\n",
    "                duration = len(imu) / FS_IMU_CONST\n",
    "                audio = create_dummy_audio(duration)\n",
    "            elif imu is None and audio is not None:\n",
    "                # Audio-only model: create dummy IMU matching audio duration\n",
    "                duration = len(audio) / FS_AUDIO_CONST\n",
    "                imu = create_dummy_imu(duration)\n",
    "            \n",
    "            ground_truth = None\n",
    "        \n",
    "        model_data = MODELS[model_key]\n",
    "        \n",
    "        # Override threshold if specified (0.0 means use optimal)\n",
    "        threshold = model_data['threshold'] if threshold_override == 0.0 else threshold_override\n",
    "        \n",
    "        # Run prediction - now returns all_windows as well\n",
    "        raw_predictions, all_probs, window_times, all_windows = sliding_window_predict(\n",
    "            audio, imu, model_data, modality=model_key, threshold=threshold\n",
    "        )\n",
    "        \n",
    "        if use_refinement:\n",
    "            # Merge into coarse candidate segments with a wider gap so the refiner\n",
    "            # can find and split bouts that would otherwise be pre-split here.\n",
    "            candidate_segments = merge_detections(raw_predictions, gap_threshold=0.5)\n",
    "            # Refine: hysteresis peak extraction → dedup at 0.23s → bout split at 0.55s\n",
    "            predictions = refine_cough_events(audio, candidate_segments)\n",
    "        else:\n",
    "            # Classic gap-based merging only (original behaviour)\n",
    "            predictions = merge_detections(raw_predictions, gap_threshold=0.3)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = compute_event_metrics(predictions, ground_truth)\n",
    "        \n",
    "        # Add threshold info to metrics\n",
    "        metrics['Threshold_Used'] = float(threshold)\n",
    "        metrics['Is_Optimal_Threshold'] = (threshold_override == 0.0)\n",
    "        metrics['Cough_E_Refinement'] = bool(use_refinement)\n",
    "        \n",
    "        # Create interactive Plotly visualization\n",
    "        fig = plot_predictions_plotly(\n",
    "            audio, imu, predictions, ground_truth, \n",
    "            all_windows, threshold, window_times, all_probs\n",
    "        )\n",
    "        \n",
    "        # Create events table\n",
    "        if predictions:\n",
    "            events_df = pd.DataFrame([\n",
    "                {'Start (s)': f'{s:.2f}', 'End (s)': f'{e:.2f}', 'Confidence': f'{p:.3f}'}\n",
    "                for s, e, p in predictions\n",
    "            ])\n",
    "        else:\n",
    "            events_df = pd.DataFrame({'Message': ['No coughs detected']})\n",
    "        \n",
    "        # Prepare audio for playback (only if real audio data exists)\n",
    "        audio_playback = None\n",
    "        if data_source == \"Dataset Selector\" or (data_source == \"Upload Files\" and audio_file is not None):\n",
    "            audio_playback = (FS_AUDIO_CONST, audio)\n",
    "        \n",
    "        return fig, audio_playback, metrics, events_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_msg = f\"Error: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        return None, None, {\"Error\": error_msg}, pd.DataFrame()\n",
    "\n",
    "print(\"✓ Main prediction function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset parameters for dropdowns\n",
    "if data_folder:\n",
    "    subject_ids = [d for d in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, d))]\n",
    "    subject_ids = sorted(subject_ids)\n",
    "else:\n",
    "    subject_ids = []\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"Interactive Cough Detection Model Tester\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Interactive Cough Detection Model Tester\n",
    "        \n",
    "        Test XGBoost cough detection models on multimodal biosignals (audio + IMU).\n",
    "        \n",
    "        **Instructions:**\n",
    "        1. Choose data source: Dataset recordings or upload your own files\n",
    "        2. Select model: IMU-only, Audio-only, or Multimodal\n",
    "        3. Upload only the required file(s) based on your selected model\n",
    "        4. Adjust threshold if needed (0 = use optimal from training)\n",
    "        5. Click \"Run Prediction\" to see results and listen to the audio\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            # Data source selector\n",
    "            data_source = gr.Radio(\n",
    "                choices=[\"Dataset Selector\", \"Upload Files\"],\n",
    "                value=\"Dataset Selector\" if data_folder else \"Upload Files\",\n",
    "                label=\"Data Source\"\n",
    "            )\n",
    "            \n",
    "            # Dataset selector (visible when Dataset Selector is chosen)\n",
    "            with gr.Group(visible=(data_folder is not None)) as dataset_group:\n",
    "                gr.Markdown(\"### Dataset Recording\")\n",
    "                subject_dropdown = gr.Dropdown(\n",
    "                    choices=subject_ids,\n",
    "                    value=subject_ids[0] if subject_ids else None,\n",
    "                    label=\"Subject ID\"\n",
    "                )\n",
    "                trial_dropdown = gr.Dropdown(\n",
    "                    choices=[\"1\", \"2\", \"3\"],\n",
    "                    value=\"1\",\n",
    "                    label=\"Trial\"\n",
    "                )\n",
    "                movement_dropdown = gr.Dropdown(\n",
    "                    choices=[\"Sit\", \"Walk\"],\n",
    "                    value=\"Sit\",\n",
    "                    label=\"Movement\"\n",
    "                )\n",
    "                noise_dropdown = gr.Dropdown(\n",
    "                    choices=[\"Nothing\", \"Music\", \"Someone else cough\", \"Traffic\"],\n",
    "                    value=\"Nothing\",\n",
    "                    label=\"Background Noise\"\n",
    "                )\n",
    "                sound_dropdown = gr.Dropdown(\n",
    "                    choices=[\"Cough\", \"Laugh\", \"Deep breathing\", \"Throat clearing\"],\n",
    "                    value=\"Cough\",\n",
    "                    label=\"Sound Type\"\n",
    "                )\n",
    "            \n",
    "            # File upload (visible when Upload Files is chosen)\n",
    "            with gr.Group(visible=(data_folder is None)) as upload_group:\n",
    "                gr.Markdown(\"### Upload Files\")\n",
    "                \n",
    "                # Dynamic hint about required files\n",
    "                file_requirements_hint = gr.Markdown(\n",
    "                    \"**Required files:** Audio + IMU (for Multimodal model)\"\n",
    "                )\n",
    "                \n",
    "                audio_upload = gr.File(\n",
    "                    label=\"Audio\",\n",
    "                )\n",
    "                imu_upload = gr.File(\n",
    "                    label=\"IMU CSV (100 Hz)\",\n",
    "                    file_types=[\".csv\"]\n",
    "                )\n",
    "                gr.Markdown(\n",
    "                    \"*CSV must contain: Accel x, Accel y, Accel z, Gyro Y, Gyro P, Gyro R*\"\n",
    "                )\n",
    "            \n",
    "            # Toggle visibility based on data source\n",
    "            def toggle_data_source(choice):\n",
    "                if choice == \"Dataset Selector\":\n",
    "                    return gr.update(visible=True), gr.update(visible=False)\n",
    "                else:\n",
    "                    return gr.update(visible=False), gr.update(visible=True)\n",
    "            \n",
    "            data_source.change(\n",
    "                toggle_data_source,\n",
    "                inputs=[data_source],\n",
    "                outputs=[dataset_group, upload_group]\n",
    "            )\n",
    "            \n",
    "            # Model selection\n",
    "            gr.Markdown(\"### Model Settings\")\n",
    "            modality_radio = gr.Radio(\n",
    "                choices=[\"IMU-only\", \"Audio-only\", \"Multimodal\"],\n",
    "                value=\"Multimodal\",\n",
    "                label=\"Model\"\n",
    "            )\n",
    "            \n",
    "            # Display optimal threshold for selected model\n",
    "            if MODELS is not None:\n",
    "                optimal_thresh_display = gr.Markdown(\n",
    "                    f\"**Optimal Threshold:** {MODELS['multimodal']['threshold']:.3f}\"\n",
    "                )\n",
    "            else:\n",
    "                optimal_thresh_display = gr.Markdown(\"**Optimal Threshold:** Models not loaded\")\n",
    "            \n",
    "            # Update optimal threshold display and file requirements when model changes\n",
    "            def update_optimal_threshold(modality):\n",
    "                if MODELS is None:\n",
    "                    return \"**Optimal Threshold:** Models not loaded\"\n",
    "                model_key = modality.lower().replace('-only', '')\n",
    "                thresh = MODELS[model_key]['threshold']\n",
    "                return f\"**Optimal Threshold:** {thresh:.3f}\"\n",
    "            \n",
    "            def update_file_requirements(modality):\n",
    "                if modality == \"IMU-only\":\n",
    "                    return \"**Required files:** IMU only (audio not needed)\"\n",
    "                elif modality == \"Audio-only\":\n",
    "                    return \"**Required files:** Audio only (IMU not needed)\"\n",
    "                else:  # Multimodal\n",
    "                    return \"**Required files:** Both Audio + IMU\"\n",
    "            \n",
    "            modality_radio.change(\n",
    "                update_optimal_threshold,\n",
    "                inputs=[modality_radio],\n",
    "                outputs=[optimal_thresh_display]\n",
    "            )\n",
    "            \n",
    "            modality_radio.change(\n",
    "                update_file_requirements,\n",
    "                inputs=[modality_radio],\n",
    "                outputs=[file_requirements_hint]\n",
    "            )\n",
    "            \n",
    "            threshold_slider = gr.Slider(\n",
    "                minimum=0.0,\n",
    "                maximum=1.0,\n",
    "                value=0.0,\n",
    "                step=0.05,\n",
    "                label=\"Threshold Override\",\n",
    "                info=\"Set to 0.0 to use optimal threshold above, or override with custom value\"\n",
    "            )\n",
    "            \n",
    "            refinement_checkbox = gr.Checkbox(\n",
    "                value=True,\n",
    "                label=\"Use Cough-E Refinement\",\n",
    "                info=(\n",
    "                    \"Post-process merged detections with peak-level analysis: \"\n",
    "                    \"hysteresis on signal power, dedup peaks <0.23 s apart, \"\n",
    "                    \"split cough bouts at 0.55 s. Improves per-cough counting \"\n",
    "                    \"accuracy, especially for rapid or sequential coughs. \"\n",
    "                    \"Uncheck to use classic gap-based merging only.\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Run button\n",
    "            run_btn = gr.Button(\n",
    "                \"Run Prediction\",\n",
    "                variant=\"primary\",\n",
    "                size=\"lg\"\n",
    "            )\n",
    "        \n",
    "        with gr.Column(scale=2):\n",
    "            # Outputs\n",
    "            gr.Markdown(\"### Results\")\n",
    "            \n",
    "            # Audio playback widget\n",
    "            audio_output = gr.Audio(\n",
    "                label=\"Audio Playback\",\n",
    "                type=\"numpy\",\n",
    "                interactive=False\n",
    "            )\n",
    "            \n",
    "            plot_output = gr.Plot(label=\"Waveform with Detections\")\n",
    "            metrics_output = gr.JSON(label=\"Metrics\")\n",
    "            events_output = gr.Dataframe(label=\"Detected Events\")\n",
    "    \n",
    "    # Connect button to prediction function\n",
    "    run_btn.click(\n",
    "        run_prediction,\n",
    "        inputs=[\n",
    "            data_source, subject_dropdown, trial_dropdown, movement_dropdown,\n",
    "            noise_dropdown, sound_dropdown, audio_upload, imu_upload,\n",
    "            modality_radio, threshold_slider, refinement_checkbox\n",
    "        ],\n",
    "        outputs=[plot_output, audio_output, metrics_output, events_output]\n",
    "    )\n",
    "\n",
    "print(\"✓ Gradio interface created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Launch Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_IN_KAGGLE = os.environ.get('KAGGLE_URL_BASE') is not None\n",
    "\n",
    "# Launch Gradio app\n",
    "if MODELS is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Launching Interactive Cough Detection Model Tester...\")\n",
    "    print(\"=\"*70)\n",
    "    demo.launch(share=IS_IN_KAGGLE, inline=False, debug=True)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Cannot launch: Models not loaded\")\n",
    "    print(\"Please run Model_Training_XGBoost.ipynb first to train models\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Usage Examples\n",
    "\n",
    "### Example 1: Test on Dataset Recording\n",
    "\n",
    "1. Select \"Dataset Selector\" as data source\n",
    "2. Choose Subject: `14287`, Trial: `1`, Movement: `Sit`, Noise: `Nothing`, Sound: `Cough`\n",
    "3. Select Model: `Multimodal`\n",
    "4. Keep threshold at `0.0` (auto-optimal)\n",
    "5. Click \"Run Prediction\"\n",
    "\n",
    "**Expected output:**\n",
    "- **Audio Playback**: Interactive player to listen to the recording\n",
    "- **Waveform plot**: Red prediction spans and green ground truth spans\n",
    "- **Metrics**: TP/FP/FN counts, Sensitivity ~0.9+, Precision ~0.8+\n",
    "- **Events table**: Detected cough times with confidence scores\n",
    "\n",
    "**Tip**: Use the audio player to correlate what you hear with the visual predictions!\n",
    "\n",
    "### Example 2: Compare Models\n",
    "\n",
    "Run the same recording through all three models:\n",
    "- IMU-only: Lower sensitivity, may miss some coughs\n",
    "- Audio-only: Good performance on coughs\n",
    "- Multimodal: Best overall performance\n",
    "\n",
    "Listen to the audio while comparing predictions to understand how each modality performs.\n",
    "\n",
    "### Example 3: Threshold Adjustment\n",
    "\n",
    "1. Run prediction with threshold `0.0` (optimal)\n",
    "2. Increase threshold to `0.7` - fewer detections, higher precision\n",
    "3. Decrease threshold to `0.3` - more detections, lower precision\n",
    "\n",
    "Play the audio to verify which threshold setting matches your perception of coughs.\n",
    "\n",
    "### Example 4: Test on Non-Cough Sounds\n",
    "\n",
    "1. Select Sound: `Laugh` or `Throat clearing`\n",
    "2. Model should show low/no detections (good specificity)\n",
    "3. No ground truth will be shown (only available for coughs)\n",
    "4. Listen to understand what non-cough sounds are present\n",
    "\n",
    "### Example 5: Upload Custom Files (Multimodal)\n",
    "\n",
    "1. Select \"Upload Files\" as data source\n",
    "2. Select Model: `Multimodal`\n",
    "3. Upload both WAV audio and CSV IMU files\n",
    "4. Run prediction (no ground truth comparison available)\n",
    "5. Use audio playback to verify predictions on your own data\n",
    "\n",
    "### Example 6: Upload Audio Only\n",
    "\n",
    "1. Select \"Upload Files\" as data source\n",
    "2. Select Model: `Audio-only`\n",
    "3. Upload only the WAV audio file (no IMU needed)\n",
    "4. Notice the hint says \"Required files: Audio only (IMU not needed)\"\n",
    "5. Run prediction - the system will automatically create dummy IMU data internally\n",
    "6. Listen to the audio to verify predictions\n",
    "\n",
    "### Example 7: Upload IMU Only\n",
    "\n",
    "1. Select \"Upload Files\" as data source\n",
    "2. Select Model: `IMU-only`\n",
    "3. Upload only the CSV IMU file (no audio needed)\n",
    "4. Notice the hint says \"Required files: IMU only (audio not needed)\"\n",
    "5. Run prediction - the system will automatically create dummy audio data internally\n",
    "6. No audio playback will be available (since no real audio was provided)\n",
    "\n",
    "## Performance Notes\n",
    "\n",
    "- **Processing time**: ~2-5 seconds for 10-second recording (depends on hardware)\n",
    "- **Window size**: 0.4 seconds (fixed, from training)\n",
    "- **Hop size**: 0.05 seconds (50ms overlap between windows)\n",
    "- **Audio playback**: Full recording available for playback when audio data is provided\n",
    "- **Multimodal model**: Best performance but requires both audio and IMU\n",
    "- **IMU-only**: Useful for privacy-preserving scenarios (no audio required)\n",
    "- **Audio-only**: Strong baseline, works well in quiet environments (no IMU required)\n",
    "- **File flexibility**: Upload only the required file(s) based on your selected model\n",
    "\n",
    "## Audio Playback Tips\n",
    "\n",
    "- **Playback speed**: Use browser controls to slow down audio and identify coughs more easily\n",
    "- **Loop sections**: Replay specific parts to understand false positives/negatives\n",
    "- **Volume**: Adjust volume to hear quiet coughs that might be missed\n",
    "- **Correlation**: Compare what you hear with red (predicted) and green (ground truth) spans on the waveform\n",
    "\n",
    "## Limitations\n",
    "\n",
    "1. **Dataset bias**: Models trained on 15 subjects, may not generalize to all populations\n",
    "2. **Microphone dependency**: Audio features tuned to specific hardware\n",
    "3. **Fixed window**: 0.4s windows may miss very long/short coughs\n",
    "4. **Threshold sensitivity**: Performance varies with threshold choice\n",
    "5. **No real-time processing**: Batch processing only (not streaming)\n",
    "6. **Dummy data for single-modality uploads**: When using Audio-only or IMU-only models with uploaded files, the system creates zero-filled placeholder data for the missing modality - this works for prediction but means visualizations will show flat lines for the unused modality\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Test on different subjects to assess generalization\n",
    "- Use audio playback to understand model errors (listen to false positives/negatives)\n",
    "- Experiment with threshold values for your use case\n",
    "- Compare model performance across different noise conditions\n",
    "- Analyze false positives/negatives to understand model weaknesses\n",
    "- Try uploading single-modality files to test Audio-only or IMU-only models\n",
    "- Consider deploying to edge device for real-time monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge-ai-cough-count",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
