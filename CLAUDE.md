# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a research repository for edge-AI cough counting using multimodal biosignals (acoustic and kinematic sensors). The repository provides tools for working with a public dataset of ~4 hours of biosignal data containing 4,300 annotated cough events, along with various non-cough sounds and motion scenarios.

Dataset available at: https://zenodo.org/record/7562332

## Development Environment

**Python backend** — requires Python 3.10, managed via `pyproject.toml`:

```bash
uv sync          # recommended
pip install -r requirements.txt  # alternative
```

**Dataset collector frontend** (`dataset-collector/`) — React + TypeScript + Vite app using pnpm:

```bash
cd dataset-collector
pnpm install
pnpm dev      # dev server on http://localhost:5173
pnpm build    # production build to dist/
pnpm lint     # ESLint check
```

**FastAPI inference server** (`server/`):

```bash
uvicorn server.main:app --reload   # from repo root
```

The server expects a trained model at `notebooks/models/xgb_audio.pkl`. Train it first via `notebooks/Model_Training_XGBoost.ipynb`.

## Repository Architecture

This repo has three loosely-coupled components:

```
edge-ai-cough-count/
├── src/                   # Python ML library
│   ├── helpers.py         # Dataset I/O + signal processing
│   ├── dataset_gen.py     # Window extraction + augmentation
│   ├── features.py        # Feature extraction for XGBoost
│   └── predict.py         # Shared sliding-window inference pipeline
├── server/                # FastAPI inference server
│   └── main.py            # POST /api/predict endpoint
├── dataset-collector/     # React PWA for recording new samples
│   └── src/
│       ├── App.tsx        # State machine: idle→recording→predicting→labeling
│       ├── types.ts       # Shared types (AppState, PredictionResult, etc.)
│       ├── components/    # PermissionScreen, RecordingScreen, PredictionScreen, LabelingScreen
│       ├── hooks/         # useAudioRecorder.ts
│       └── utils/         # buildZip.ts (packages recording + ground_truth.json for download)
├── notebooks/             # Jupyter notebooks for ML pipeline
│   └── models/            # Saved model .pkl files (git-ignored, generated by training)
├── public_dataset/        # Downloaded from Zenodo (not in git)
└── sample-collected/      # Samples recorded via dataset-collector app
```

### Core Architecture: Python ML Library (`src/`)

**`src/helpers.py`** — Core utilities:
- Enums for dataset hierarchy: `Trial`, `Movement`, `Noise`, `Sound`, `IMU_Signal`
- `load_audio()`, `load_annotation()`, `load_imu()` — dataset I/O
- `IMU` class — 6-channel container (accel x,y,z + gyro Y,P,R) with normalize/plot methods
- `segment_cough()` — hysteresis-based segmentation using signal power
- `delineate_imu()` — peak/valley detection via derivatives
- Constants: `FS_AUDIO = 16000`, `FS_IMU = 100`

**`src/dataset_gen.py`** — Window extraction:
- `get_cough_windows()` — fixed-length windows around labeled coughs with random temporal shifts (augmentation)
- `get_non_cough_windows()` — random non-cough segments from other sound types
- `get_samples_for_subject()` — balanced cough/non-cough dataset for a single subject
- Output shapes: audio `(N, window_len*16000, 2)`, IMU `(N, window_len*100, 6)`, labels `(N,)`

**`src/features.py`** — Feature extraction for classical ML:
- `extract_audio_features()` — 65 features (13 MFCCs, spectral centroid/bandwidth/rolloff/contrast/flatness, ZCR, RMS, envelope)
- `extract_imu_features()` — 40 features (8 derived signals × 5 stats: line length, ZCR, kurtosis, crest factor, RMS)

**`src/predict.py`** — Shared inference pipeline used by both server and notebooks:
- `sliding_window_predict()` — applies model over continuous recording with 0.4s windows / 0.05s hop
- `merge_detections()` — merges consecutive window detections within 0.3s gap
- `create_dummy_imu()` / `create_dummy_audio()` — zero-filled placeholders for single-modality inference

### FastAPI Inference Server (`server/main.py`)

Serves the dataset-collector frontend. Single endpoint:
- `GET /api/health` — health check
- `POST /api/predict` — accepts multipart audio upload, returns `{cough_count, start_times, end_times, window_times, probabilities}`

Audio is peak-normalized on load to match training preprocessing: `y = (y - mean) / (max_abs + 1e-17)`. Loads `notebooks/models/xgb_audio.pkl` at startup (audio-only model, no IMU required from frontend). Uses `modality='audio'` with dummy IMU.

### Dataset Collector Frontend (`dataset-collector/`)

React PWA that records audio via `MediaRecorder`, sends it to `/api/predict`, shows cough detection results, then packages a ZIP for download containing:
- `outward_facing_mic.{webm,mp4,ogg}` — raw audio
- `ground_truth.json` — predicted `{start_times, end_times}` (used as pseudo-ground-truth for new samples)
- `metadata.json` — subject ID, sound type, movement, noise, trial, device info

The app uses a discriminated union state machine in `App.tsx`: `idle → requesting-permissions → ready → recording → stopped → predicting → results → labeling`. The Vite dev server proxies `/api/*` to the backend.

### Saved Model Format

Each `.pkl` in `notebooks/models/` contains:
```python
{
    'model': XGBClassifier,    # trained binary classifier
    'scaler': StandardScaler,  # fitted on training features
    'threshold': float         # optimal threshold from ROC (Youden's J)
}
```

Three variants: `xgb_audio.pkl` (65 features), `xgb_imu.pkl` (40 features), `xgb_multimodal.pkl` (105 features).

## ML Training Workflow

### Classical ML (Primary Approach)

1. **Segment**: `src/dataset_gen.py` → 0.4s windows, `aug_factor=2` augmentation
2. **Extract features**: `src/features.py` → 65 audio + 40 IMU = 105 multimodal features
3. **Train**: `notebooks/Model_Training_XGBoost.ipynb` — subject-wise cross-validation, SMOTE on training splits, RFECV feature selection, XGBoost
4. **Evaluate**: `notebooks/Compute_Success_Metrics.ipynb` — event-based metrics via `timescoring`

Key parameters:
- **Window**: 0.4s (6400 audio samples, 40 IMU samples)
- **Cross-validation**: Leave-n-Subjects-Out (never mix subjects across train/val)
- **SMOTE**: Apply only on training folds, not validation
- **Target**: ROC-AUC ~0.96 for multimodal

Event evaluation parameters (must match across notebooks):
```python
toleranceStart = 0.25, toleranceEnd = 0.25, minOverlap = 0.1, maxEventDuration = 0.6
```

### Dataset Structure

```
public_dataset/
└── {subject_id}/
    └── trial_{1,2,3}/
        └── mov_{sit,walk}/
            └── background_noise_{music,nothing,someone_else_cough,traffic}/
                └── {cough,laugh,deep_breathing,throat_clearing}/
                    ├── outward_facing_mic.wav
                    ├── body_facing_mic.wav
                    ├── imu.csv
                    └── ground_truth.json   # only for cough recordings
```

`ground_truth.json` format: `{"start_times": [...], "end_times": [...]}` (seconds from recording start).

## Additional Documentation

- `model-training-documentation.md` — comprehensive guide to reproducing the XGBoost pipeline with expected performance targets
- `interactive-testing-documentation.md` — guide to the Gradio-based interactive model testing notebook (`notebooks/Interactive_Model_Testing.ipynb`)
